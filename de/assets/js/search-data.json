{"0": {
    "doc": "Not found",
    "title": "404 Not Found",
    "content": "We cannot find the page you are looking for! . Click here to go back home . ",
    "url": "/404.html#404-not-found",
    
    "relUrl": "/404.html#404-not-found"
  },"1": {
    "doc": "Not found",
    "title": "Not found",
    "content": " ",
    "url": "/404.html",
    
    "relUrl": "/404.html"
  },"2": {
    "doc": "Willkommen!",
    "title": "Willkommen!",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  },"3": {
    "doc": "Einführung",
    "title": "Einführung",
    "content": "Elastic Cloud Enterprise by German Edge Cloud (ECE) ist eine von GEC gehostete und betriebene Cloud-Plattform, auf der Sie Ihre Elastic-Workloads in Form sogenannter Deployments betreiben können. Ein Deployment ist ein isoliertes Elastic Cluster, das mit ECE verwaltet wird. Es besteht aus mehreren Komponenten des Elastic Stacks. Für jedes Deployment können Sie folgendes individuell festlegen: . | Welche der unten aufgeführten Elastic Stack Komponenten verwendet werden sollen | Die Größe der jeweiligen Komponenten | Die Anzahl der Availability Zones (1 bis 3) über die sie verteilt werden sollen | Welche der von GEC bereitgestellten Versionen für den Elastic Stack verwendet werden soll | . Jedes Deployment bekommt eigene, aus dem Internet erreichbare URLs für Elasticsearch, Kibana, Fleet und Enterprise Search. ",
    "url": "/ece/intro",
    
    "relUrl": "/ece/intro"
  },"4": {
    "doc": "Einführung",
    "title": "Komponenten des Elastic Stacks",
    "content": "ECE enthält alle aktuell von Elastic angebotenen Server-Komponenten: . | Elasticsearch in den 4 Tiers Hot, Warm, Cold und Frozen sowie Master Nodes und Coordinating/Ingest Nodes | Kibana | Machine Learning | Integration Server (Fleet und APM) | Enterprise Search | . Die Definition der Elasticsearch Data Tier finden Sie hier: https://www.elastic.co/guide/en/elasticsearch/reference/current/data-tiers.html. ",
    "url": "/ece/intro#komponenten-des-elastic-stacks",
    
    "relUrl": "/ece/intro#komponenten-des-elastic-stacks"
  },"5": {
    "doc": "Einführung",
    "title": "Funktionsumfang der Komponenten",
    "content": "Alle ECE Deployments sind mit der Elastic Enterprise Lizenz ausgestattet, d.h. alle Komponenten enthalten alle Funktionalitäten der höchstwertigen Elastic-Lizenz. Dazu gehören Machine Learning, durchsuchbare Snapshots im Frozen Tier (eine sehr kostengünstige Variante für die Speicherung selten abgerufener Daten), Cross-Cluster-Search, alle SIEM (Security Information and Event Management) Funktionen des Elastic Stacks, Watcher und Alerting, der neue AI Assistant u.v.m. Einen Überblick über die Komponenten finden Sie hier https://www.elastic.co/de/pricing/. Eine detaillierte Auflistung erhalten Sie hier: https://www.elastic.co/de/subscriptions. ",
    "url": "/ece/intro#funktionsumfang-der-komponenten",
    
    "relUrl": "/ece/intro#funktionsumfang-der-komponenten"
  },"6": {
    "doc": "Einführung",
    "title": "Berechnung der Größe des Deployments und Preismodell",
    "content": "Die Größe des Deployments wird in GB RAM gemessen. Jeder Elastic Komponente wird nach einem bestimmten Schlüssel zum RAM entsprechend CPU und Storage zugeordnet. Dieser Zuordnungsschlüssel kann pro Komponente verschieden sein. Für die meisten Anwendungsfälle ist der Arbeitsspeicher der bestimmende Faktor für die Performance eines Elastic Workloads. Da die Zuordnung nach festen Schlüsseln erfolgt, reicht ein Parameter (GB RAM) aus, um die anderen Parameter (CPU, Storage) zu berechnen. Dadurch wird auch das Preismodell sehr transparent und leicht verständlich. Sie müssen lediglich den Arbeitsspeicher des Deployments kennen, um Ihre Kosten zu berechnen. Sie zahlen für die genutzten GB RAM Ihres Deployments. Die Erfassung der Nutzung und die Abrechnung erfolgen stundengenau. Zusätzlich fallen Kosten für den genutzten Snapshot-Storage in unserem Object Storage Cluster an. Der (ein- und ausgehende) Datenverkehr ist kostenfrei. ",
    "url": "/ece/intro#berechnung-der-gr%C3%B6%C3%9Fe-des-deployments-und-preismodell",
    
    "relUrl": "/ece/intro#berechnung-der-größe-des-deployments-und-preismodell"
  },"7": {
    "doc": "Einführung",
    "title": "Autoscaling Feature",
    "content": "Die Komponenten Elasticsearch und Machine Learning verfügen über ein sogenanntes Autoscaling. Dieses Feature haben Sie bei einem selbst gehosteten Elastic Stack in dieser Form nicht oder müssten es selbst entwickeln. Mit Autoscaling überwacht ECE den genutzten Festplattenspeicher der Elasticsearch-Komponenten und fügt bei Überschreiten eines Schwellenwertes automatisch weitere Ressourcen hinzu. Dadurch müssen Sie auch bei Wachstum Ihrer Datenmenge keine Ausfälle wegen voller Festplatten o.ä. befürchten. Machine Learning Workloads werden in der notwendigen Größe nur dann gestartet, wenn sie auch benötigt werden. Die Ressourcen in Ihrem Deployment und somit auch Ihre Kosten wachsen mit Ihrer Datenmenge. Dadurch wird eine effiziente Ausnutzung der Ressourcen und eine kosteneffiziente Abbildung Ihrer Workloads auf der ECE Plattform möglich. Für jede Komponente mit Autoscaling lassen sich auch Obergrenzen definieren bis zu denen automatisch hochskaliert wird. Somit gerät z.B. bei Fehlkonfigurationen Ihr Rechnungsbetrag nicht außer Kontrolle. ",
    "url": "/ece/intro#autoscaling-feature",
    
    "relUrl": "/ece/intro#autoscaling-feature"
  },"8": {
    "doc": "Einführung",
    "title": "Ihre Vorteile durch ECE von GEC",
    "content": "Mit der ECE-Lösung von GEC erhalten Sie alle Funktionalitäten der Software von Elastic, dem Marktführer bei Suche, SIEM und Observability, inklusive Enterprise-Lizenz. GEC hostet ausschließlich in hochsicheren deutschen Rechenzentren und leistet alle Betriebsaufgaben von Deutschland aus. Sie erhalten alles aus einer Hand von einem deutschen Vertragspartner. Im Gegensatz zu einem “klassischen” Deployment des Elastic Stacks in einem Cluster mit virtuellen Maschinen, die von der jeweiligen Cloud-Plattform auf vorgegebene Größen beschränkt sind, werden bei ECE Docker-Container verwendet. Diese können bei Bedarf automatisch skalieren. Durch das feingranulare Autoscaling können Sie die Ressourcen effizient ausnutzen und die Workloads kosteneffizient betreiben. ",
    "url": "/ece/intro#ihre-vorteile-durch-ece-von-gec",
    
    "relUrl": "/ece/intro#ihre-vorteile-durch-ece-von-gec"
  },"9": {
    "doc": "Einführung",
    "title": "Leistungen der GEC",
    "content": ". | GEC stellt in seinen sicheren und zertifizierten Rechenzentren in Deutschland die Cloud-Umgebung bereit, auf denen Ihre Elastic Workloads laufen. | GEC gibt Ihnen die Möglichkeit, zwecks hoher Verfügbarkeit Ihre Workloads über 3 Verfügbarkeitszonen (Availability Zones) zu verteilen. | GEC kümmert sich darum, dass genügend Ressourcen für die Skalierung Ihres Deployments zur Verfügung stehen. | GEC überwacht die ECE-Umgebung und sorgt dafür, dass sie verfügbar ist (99,85 %). Auch außerhalb der Bürozeiten wird automatisch eine Rufbereitschaft alarmiert, um eventuelle Probleme an der ECE-Plattform möglichst schnell zu beheben. | GEC sorgt für regelmäßige Updates und Security-Patches der Server. | GEC stellt neue Versionen des Elastic Stacks zur Installation bereit, meist nur wenige Werktage nachdem diese von Elastic veröffentlicht wurden. Wir führen jedoch keine automatischen oder unaufgeforderten Versions-Upgrades Ihres Deployments durch (ausgenommen EOL-Versionen). | GEC stellt aus dem Internet erreichbare URLs für alle Deployments inklusive TLS-Zertifikat für die verschlüsselte Kommunikation zur Verfügung (kundeneigene Zertifikate sind leider nicht möglich). | GEC stellt Backup- und Snapshot-Storage im redundanten GEC Object Storage Cluster zur Verfügung. Für jedes Deployment wird standardmäßig eine (von Ihnen änderbare) Snapshot Policy aktiviert, über die automatisch in regelmäßigen Abständen eine Datensicherung Ihrer Elasticsearch-Daten in den Object Storage erfolgt. Dadurch sind Sie sehr gut gegen Datenverlust geschützt. | All Ihre Deployments in der ECE-Cloud von GEC sind mit der Elastic Enterprise Lizenz ausgestattet. Sie müssen keine Lizenzen bei Elastic einkaufen, Sie erhalten von GEC alles aus einer Hand. | . ",
    "url": "/ece/intro#leistungen-der-gec",
    
    "relUrl": "/ece/intro#leistungen-der-gec"
  },"10": {
    "doc": "Einführung",
    "title": "Ihre Verantwortlichkeit als Kunde",
    "content": "Als Kunde sind Sie für alles verantwortlich was innerhalb Ihres Deployments passiert, insbesondere: . | Das Einsammeln von Daten aus Client-Systemen und das Laden dieser Daten in Elasticsearch | Die Konfiguration von Lifecycle Policies | Das Anlegen von Nutzern und die Vergabe von Berechtigungen | Die Überwachung der Shard-Gesundheit und bei Bedarf deren Reparatur (s. hierzu die Dokumentation von elastic.co) | Konfiguration von Alerts - sofern gewünscht | Wiederherstellung von Snapshots bei Bedarf | Überwachung der Performance Ihres Deployments, sofern Sie dafür spezielle Anforderungen haben. Welche Performance für Ihren Anwendungsfall akzeptabel ist, können wir nicht wissen. | . Weitere Bestimmungen entnehmen Sie den Allgemeinen Geschäftsbedingungen der GEC. ",
    "url": "/ece/intro#ihre-verantwortlichkeit-als-kunde",
    
    "relUrl": "/ece/intro#ihre-verantwortlichkeit-als-kunde"
  },"11": {
    "doc": "Einführung",
    "title": "Service Description",
    "content": "Für detailliertere Informationen kontaktieren Sie bitte unseren Vertrieb. ",
    "url": "/ece/intro#service-description",
    
    "relUrl": "/ece/intro#service-description"
  },"12": {
    "doc": "Einführung",
    "title": "Auszuführende Schritte nach dem ECE-Deployment",
    "content": "Wir empfehlen Ihnen, folgende Schritte durchzuführen, um die Sicherheit Ihres ECE Deployments zu erhöhen und Daten in Elastic zu laden: . Erhöhung der Sicherheit . | Legen Sie in Kibana die benötigten Spaces, Rollen und Nutzer an. Weitere Informationen dazu finden Sie hier: https://www.elastic.co/guide/en/kibana/current/tutorial-secure-access-to-kibana.html. | Verwenden Sie nicht den elastic Superuser für die tägliche Arbeit in Kibana oder um Daten anzuliefern. | Sofern gewünscht, konfigurieren Sie Single Sign-On mittels SAML, LDAP, Active Directory, OpenID Connect oder Kerberos sowie dynamische role mappings, wenn Sie Role-based oder Attribute-based Access Control benötigen. Für die Einstellung der Parameter in Ihrem Deployment wenden Sie sich vorläufig an unseren Support. | Falls gewünscht, lassen Sie durch unseren Support traffic filter konfigurieren. Damit können Sie unerwünschte Zugriffe auf Ihr Deployment verhindern. | Legen Sie ggf. (falls Sie nicht den Agent verwenden, s.u.) Service Accounts für die Datenanlieferung an und erzeugen Sie dafür API Keys. Weitere Informationen finden Sie hier: Grant access using API keys. | . Laden von Daten in Elastic . | Laden Sie Daten in Ihren Cluster. Wir empfehlen hierfür den Elastic Agent in Zusammenarbeit mit dem Fleet Server. Eine detaillierte Anleitung finden Sie hier: Laden von Daten in ECE. | Um Verbindungsprobleme zu ihrem Cluster zu vermeiden, stellen sie sicher, Sniffingin den verwendeten Elasticsearch Clients zu deaktivieren (z.B. fluentd). Elastic Agent, Filebeat etc. verhalten sich automatisch richtig. | Falls Sie Daten aus Ihrem bestehenden Elastic Cluster in Ihr neues Deployment migrieren möchten, finden Sie einige Möglichkeiten in der Elastic Dokumentation: https://www.elastic.co/guide/en/cloud-enterprise/current/ece-migrating-data.html. | Prüfen Sie in Kibana Ihre Index Templates und Lifecycle Policies, damit Sie Ihre Daten den von Ihnen gewünschten Tiers Hot, Warm, Cold, Frozen (und Delete) zuweisen können. Die von Elastic automatisch angelegten Lifecycle Policies haben häufig nur ein Hot Tier ohne Ablaufdatum konfiguriert. Weitere Informationen finden Sie hier: https://www.elastic.co/guide/en/elasticsearch/reference/current/index-lifecycle-management.html. Dies gilt insbesondere für die standardmäßig deployten Lifecycle Policies logs und metrics, die bei Verwendung des Elastic Agents die relevantesten Policies sind. | . Ein Beispiel für die Konfiguration einer Policy, bei der die Daten nach 70 Tagen gelöscht werden, finden Sie hier: Aktualisieren der Standard Lifecycle Policies. ",
    "url": "/ece/intro#auszuf%C3%BChrende-schritte-nach-dem-ece-deployment",
    
    "relUrl": "/ece/intro#auszuführende-schritte-nach-dem-ece-deployment"
  },"13": {
    "doc": "Einführung",
    "title": "Support-Leistungen von GEC",
    "content": "Bei Support-Anfragen können Sie sich an unseren Support wenden, der werktags von 8-18 Uhr erreichbar ist. Der Support von GEC kann Sie bei allen Themen unterstützen, die im Rahmen des ECE-Dienstes in der Verantwortung von GEC liegen: . | Verfügbarkeit und Erreichbarkeit der Plattform | Fragen zur Verwendung von ECE | Fehler in der Elastic Software (wir würden diese entsprechend an Elastic weiterleiten) | . Während wir daran arbeiten, Ihnen in Zukunft eine Self-Service-Oberfläche für die Verwaltung Ihres ECE-Deployments zur Verfügung zu stellen, müssen Sie vorläufig folgende Einstellungen an Ihrem Deployment über unseren Support beauftragen. Während dieser Zeit sind die Leistungen auch im Support-Umfang enthalten: . | Erweiterte Konfiguration des Deployments (d.h. Anpassungen an der elasticsearch.yml bzw. kibana.yml) | Einträge im Elasticsearch Keystore | Upgrade der Elastic-Version Ihres Deployments | Konfiguration von IP-Filtern für Ihr Deployment | Konfiguration von Vertrauensbeziehungen zwischen Deployments, damit Sie Cross-Cluster Search bzw. Cross-Cluster Replication verwenden können | . Nicht durch den Support der GEC abgedeckt sind: . | Generelle Beratung zur Elastic Software oder deren Verwendung in Ihrem Deployment | . Ggf. können wir auf Anfrage einige der nicht abgedeckten Leistungen als vergütete Consulting-Leistung anbieten. Bitte wenden Sie sich hierzu an den Vertrieb. Ebenso können wir keine 24/7-Rufbereitschaft für Kunden anbieten. Dies ist auch nicht nötig, da die ordnungsgemäße Funktionalität der ECE-Plattform von einem automatisierten Monitoring überwacht wird. Wird eine Einschränkung des Service festgestellt, wird unsere interne Rufbereitschaft automatisch alarmiert und beginnt mit der Problembehebung. ",
    "url": "/ece/intro#support-leistungen-von-gec",
    
    "relUrl": "/ece/intro#support-leistungen-von-gec"
  },"14": {
    "doc": "Laden von Daten in Elastic",
    "title": "Laden von Daten in Elastic",
    "content": "Nachdem Ihr Deployment eingerichtet wurde, können Sie mit Elastic Agent und Fleet Daten aus Ihren Logdaten-Quellen (Log Sources) an Ihr Deployment senden. Dieses Dokument beschreibt die nötigen Schritte. ",
    "url": "/ece/shipdata/",
    
    "relUrl": "/ece/shipdata/"
  },"15": {
    "doc": "Laden von Daten in Elastic",
    "title": "Begriffsklärung",
    "content": "Bevor wir beginnen, möchten wir einige Begriffe erklären, die in diesem Dokument verwendet werden. | Begriff | Definition | . | Elastic Agent | Komponente für die Sammlung von Log- und Metrikdaten, die Client-seitig auf den Logdaten-Quellen installiert wird. Es handelt sich um die einzige Komponente, die installiert werden muss, da sie alle Beats mitbringt und diese mit den zugehörigen Konfigurationen auf dem Client (Klienten) verwaltet. | . | Fleet Server | Komponente im Elastic-Deployment, mit der die Agenten verwaltet werden. Alle Einstellungen in Fleet werden in Kibana vorgenommen. | . | Integration | In diesem Zusammenhang handelt es sich um eine (meist durch Elastic selbst) vorkonfigurierte Zusammenstellung von Agenteneinstellungen, Ingest-Pipelines und Kibana-Dashboards für einen bestimmten Logdaten-Quellen-Typ (z. B. Tomcat, Nginx, MySQL, Cisco Komponenten, …). Für die gängigsten Logdaten-Quellen sind Integrationen verfügbar (siehe: https://www.elastic.co/de/integrations/data-integrations). | . | Agent Policy | Eine Sammlung von Einstellungen und Integrationen in Form einer Richtlinie (Policy), die dem Agenten und den darunter liegenden Beats mitteilt, welche Daten wo gesammelt werden sollen. Sie benötigen für jeden Quellentyp eine separate Agentenrichtlinie (Agent Policy). Wenn beispielsweise alle Ihre MySQL-Datenbanken ihre Logdateien im selben lokalen Pfad speichern (was sie ohnehin tun sollten), reicht eine Agentenrichtlinie für MySQL-Datenbanken aus. Eine Agentenrichtlinie kann mehrere Integrationen enthalten. Somit kann ein Agent Logdateien von mehreren Anwendungen auf derselben Quelle sammeln. Sie können nur eine Richtlinie pro Agent haben. Angenommen, Sie haben auf einigen Hosts nur Apache httpd und auf anderen Hosts httpd und Tomcat installiert, dann benötigen Sie mindestens zwei Agentenrichtlinien: eine für httpd und eine für httpd+tomcat. | . | Enrollment Token | Wird vom Agenten beim Start auf dem Client benötigt. Das Token erfüllt zwei Zwecke: Erstens, ermöglicht es die Erstauthentifizierung des Agenten gegenüber den Fleet- und Elastic-Servern. Zweitens, verweist es auf genau eine Agentenrichtlinie, sodass der Fleet-Server dem Agenten mitteilen kann, von welchen Quellen er Daten sammeln soll. Jede Agentenrichtlinie verfügt über mindestens ein Registrierungstoken. Registrierungstokens sind von Natur aus sensible Daten und sollten daher auf sichere Weise gespeichert werden. | . ",
    "url": "/ece/shipdata/#begriffskl%C3%A4rung",
    
    "relUrl": "/ece/shipdata/#begriffsklärung"
  },"16": {
    "doc": "Laden von Daten in Elastic",
    "title": "Erstellen der Agentenrichtlinie",
    "content": "Da die Agentenrichtlinie für den Agenten definiert, welche Daten erfasst werden sollen, muss diese als erstes definiert werden. Definieren Sie am besten eine Agentenrichtlinie pro Quellentyp. Gehen Sie in Kibana zu Fleet → Agent Policies. Erstellen Sie eine neue Agentenrichtlinie und geben Sie ihr einen aussagekräftigen Namen (meist ist es nicht nötig, die Advanced Optionen zu ändern). Klicken Sie dann auf Ihre neue Integration. Sie werden sehen, dass bereits eine Integration, die Systemintegration, vorkonfiguriert ist. Diese sammelt die System-Logs und Metriken vom Client-Computer. Sie müssen sich keine Sorgen wegen des Betriebssystems machen. Egal ob Linux, Windows oder MacOS, der Agent findet das selbst heraus und konfiguriert sich korrekt. Fügen Sie je nach Art der Quelle, von der Sie Daten sammeln möchten, weitere Integrationen hinzu, wie z.B. Apache Produkte: . Durch einen Klick auf die entsprechende Kachel erhalten Sie nicht nur einen Überblick darüber, was die Integration beinhaltet. Sie können, nachdem Sie “Add Integration” ausgewählt haben, auf der nachfolgenden Bildschirmseite auch die Einstellungen anpassen. Der Inhalt der Seite ist vom Typ der Integration abhängig. Wenn Sie Ihre MySQL-Logdaten beispielsweise nicht im Standardpfad speichern, können Sie dies hier ändern. Es wird wahrscheinlich ungefähr wie in dem folgenden Bild aussehen: . ",
    "url": "/ece/shipdata/#erstellen-der-agentenrichtlinie",
    
    "relUrl": "/ece/shipdata/#erstellen-der-agentenrichtlinie"
  },"17": {
    "doc": "Laden von Daten in Elastic",
    "title": "Abrufen des Enrollment Tokens",
    "content": "Für jede Agentenrichtlinie wird automatisch ein Enrollment Token (Registrierungstoken) mit dem Namen Default erstellt. Dieses können Sie jederzeit in Kibana abrufen. ",
    "url": "/ece/shipdata/#abrufen-des-enrollment-tokens",
    
    "relUrl": "/ece/shipdata/#abrufen-des-enrollment-tokens"
  },"18": {
    "doc": "Laden von Daten in Elastic",
    "title": "Installieren des Agenten auf dem Content-Host",
    "content": "In Kibana gibt es einen Wizard, der Sie bei der Installation des Agenten auf den Clients unterstützt. Für den ersten Rollout dieser Art starten Sie die Installation in Kibana → Fleet → Agents → Add Agent. Bei mehreren Agenten mit der selben Agentenrichtlinie können Sie die Installation auch automatisieren, da die Installationsbefehle und -parameter identisch sind. Wählen Sie die richtige Agentenrichtlinie für den Host aus, auf dem Sie die Installation durchführen möchten. Lassen Sie Enroll in Fleet aktiviert. Kibana präsentiert Ihnen dann verschiedene Optionen für die Befehle, die auf dem Client-Computer auszuführen sind: . Der URL-Parameter ist automatisch auf das Deployment eingestellt, in dem Sie sich gerade befinden, und das Enrollment-Token verweist auf die Agentenrichtlinie. Sobald diese Befehle auf dem Client-Computer ausgeführt werden, registriert sich der Agent beim Fleet-Server und erscheint in Kibana in der Agentenliste. Alle nachfolgenden Installationen von Agenten, die dieselbe Agentenrichtlinie verwenden, verfügen über denselben Befehls- und Parametersatz. Sie müssen den Assistenten nicht erneut durchlaufen. Sie können somit diesen Installationsteil im Tool Ihrer Wahl automatisieren. ",
    "url": "/ece/shipdata/#installieren-des-agenten-auf-dem-content-host",
    
    "relUrl": "/ece/shipdata/#installieren-des-agenten-auf-dem-content-host"
  },"19": {
    "doc": "Laden von Daten in Elastic",
    "title": "Auszuführende Schritte nach der Installation",
    "content": "Überprüfen Sie unbedingt die Lifecycle Policies (ILM), einschließlich aller vom System bereitgestellten Richtlinien wie logs und metrics. Standardmäßig sind diese mit einer unbegrenzten Lebensdauer versehen. Wir empfehlen, das zu ändern. Entdecken Sie die ansprechenden neuen Dashboards, die mit jeder zusätzlichen Integration geliefert werden. Machen Sie sich mit dem Fleet-Menü in Kibana vertraut. Suchen Sie nach möglichen Aktualisierungen für die Agentenversionen oder Integrationen. Sie können all dies im Fleet-Menü in Kibana erledigen, dafür müssen Sie sich nicht auf den Client-Computern einloggen. ",
    "url": "/ece/shipdata/#auszuf%C3%BChrende-schritte-nach-der-installation",
    
    "relUrl": "/ece/shipdata/#auszuführende-schritte-nach-der-installation"
  },"20": {
    "doc": "Anpassen von Lifecycle Policies",
    "title": "Anpassen von Lifecycle Policies",
    "content": "Dieses Dokument beschreibt, wie Sie die (Standard) Index Lifecycle Policies (ILM) anpassen können. | Begriffsklärung | Auffinden der zugeordneten ILM-Richtlinie | Erstellen einer neuen ILM Richtlinie . | Verwenden der Benutzeroberfläche (UI) . | Hot Phase | Warm Phase | Cold Phase | Frozen Phase | Delete Phase | . | Verwenden von Dev Tools | . | Zuweisen der Richtlinie zu einem Index | Rollover des Data Streams | . ",
    "url": "/ece/updateilm/",
    
    "relUrl": "/ece/updateilm/"
  },"21": {
    "doc": "Anpassen von Lifecycle Policies",
    "title": "Begriffsklärung",
    "content": "Bevor wir beginnen, möchten wir einige Begriffe erklären, die in diesem Dokument verwendet werden. | Begriff | Definition | . | ILM | Abkürzung für Index Lifecycle Management. Bestimmt den Lebenszyklus Ihrer Daten sowie deren Zuordnung zu den verschiedenen Phasen. Wird in Richtlinien definiert (“Index Lifecycle Policies”). | . | Data stream | Der Datenstrom ist eine Sammlung einzelner Indizes. Man könnte es fast als kleines DNS (Domain Name System) für das Routing von Anfragen an Indizes betrachten. Ein Datenstrom kann mehrere Indizes enthalten. | . | Rollover | Prozess der (automatischen) Erstellung eines neuen Indexes innerhalb eines Datenstroms in der Hot Phase, sodass ein einzelner Index nicht unbegrenzt wächst. Er ist in der Hot Phase der ILM-Richtlinie definiert. | . ",
    "url": "/ece/updateilm/#begriffskl%C3%A4rung",
    
    "relUrl": "/ece/updateilm/#begriffsklärung"
  },"22": {
    "doc": "Anpassen von Lifecycle Policies",
    "title": "Auffinden der zugeordneten ILM-Richtlinie",
    "content": "Bei Verwendung des Elastic Agents und Fleet gelten Standardrichtlinien für die Datenströme. Diese Richtlinien sind einfach gehalten und beinhalten nur wenige Phasen, oft sogar nur die Hot Phase. Die auf die Datenströme angewendete Richtlinie finden Sie in Kibana. | Öffnen Sie Kibana. | Wählen Sie im Menü Stack Management aus. | Öffnen Sie Index Management und gehen Sie zu Data Streams. | Im Popup rechts sehen Sie die zugeordnete ILM Richtlinie. | . ",
    "url": "/ece/updateilm/#auffinden-der-zugeordneten-ilm-richtlinie",
    
    "relUrl": "/ece/updateilm/#auffinden-der-zugeordneten-ilm-richtlinie"
  },"23": {
    "doc": "Anpassen von Lifecycle Policies",
    "title": "Erstellen einer neuen ILM Richtlinie",
    "content": "Verwenden der Benutzeroberfläche (UI) . | Um eine neue ILM-Richtlinie zu erstellen, wählen Sie Stack Management. | Wählen Sie Index Lifecycle Management und klicken Sie auf Create Policy. | Geben Sie einen neuen Namen für die Richtlinie ein. | . Hot Phase . In der Hot Phase werden alle Daten gespeichert, die neu indiziert werden und auf die ständig zugegriffen wird. Sie ist immer die erste Phase und daher verpflichtend. In unserem Beispiel haben wir den Schalter “Use recommended default” deaktiviert, um das maximale Alter der Daten bis zum Rollover vom Standardwert von 30 Tagen auf 7 Tage zu verkürzen. Warm Phase . Nachdem 7 Tage lang Daten gesammelt wurden oder der Index eine Größe von 50 GB erreicht hat, sollen die Indizes unmittelbar in die zweite Phase, die sogenannte Warm Phase, übergehen. In dieser Stufe wird die Priorität der Indizes reduziert und auf “schreibgeschützt” gesetzt. Cold Phase . Nach 7 Tagen in der Warm Phase gehen die Daten in die nächste Phase über, die sogenannte Cold Phase. In dieser Phase können durchsuchbare Snapshots verwendet werden, um den Platzbedarf auf den Elastic Instanzen zu verringern. Die Snapshots sind im Object Storage gespeichert, der selbst schon hochverfügbar ausgelegt ist. Daher können wir die Anzahl der Replikate und die Priorität auf Null setzen. Frozen Phase . In dieser Phase werden die Daten von den Elastic Knoten entfernt und nur ein kleiner Cache verbleibt hier. Wenn auf die Daten zugegriffen wird, werden diese aus dem Snapshot geholt und bereitgestellt. Delete Phase . In dieser Phase werden die Indizes entfernt, die ein bestimmtes Alter erreicht haben. Verwenden von Dev Tools . Alternativ kann auch in den Dev Tools von Kibana eine neue ILM-Richtlinie erstellt werden. Die Konsole finden Sie in Kibana unter Management → Dev Tools. PUT _ilm/policy/&lt;add-some-name-here&gt; { \"policy\": { \"phases\": { \"hot\": { \"min_age\": \"0ms\", \"actions\": { \"rollover\": { \"max_primary_shard_size\": \"50gb\", \"max_age\": \"7d\" }, \"set_priority\": { \"priority\": 100 } } }, \"warm\": { \"min_age\": \"0d\", \"actions\": { \"set_priority\": { \"priority\": 50 }, \"readonly\": {} } }, \"cold\": { \"min_age\": \"7d\", \"actions\": { \"readonly\": {}, \"searchable_snapshot\": { \"snapshot_repository\": \"found-snapshots\", \"force_merge_index\": true }, \"set_priority\": { \"priority\": 0 }, \"allocate\": { \"number_of_replicas\": 0 } } }, \"frozen\": { \"min_age\": \"14d\", \"actions\": { \"searchable_snapshot\": { \"snapshot_repository\": \"found-snapshots\", \"force_merge_index\": true } } }, \"delete\": { \"min_age\": \"70d\", \"actions\": { \"delete\": { \"delete_searchable_snapshot\": true } } } } } } . ",
    "url": "/ece/updateilm/#erstellen-einer-neuen-ilm-richtlinie",
    
    "relUrl": "/ece/updateilm/#erstellen-einer-neuen-ilm-richtlinie"
  },"24": {
    "doc": "Anpassen von Lifecycle Policies",
    "title": "Zuweisen der Richtlinie zu einem Index",
    "content": "Die neu erstellte Richtlinie kann dann über das Menü Index Lifecycle Policies z.B. einem Index Template zugewiesen werden. ",
    "url": "/ece/updateilm/#zuweisen-der-richtlinie-zu-einem-index",
    
    "relUrl": "/ece/updateilm/#zuweisen-der-richtlinie-zu-einem-index"
  },"25": {
    "doc": "Anpassen von Lifecycle Policies",
    "title": "Rollover des Data Streams",
    "content": "Damit die neue ILM-Richtlinie sofort benutzt wird, muss ein Rollover für den Datenstrom angestoßen werden. Dies kann auch in den Dev Tools von Kibana erfolgen, hier am Beispiel für den Datenstrom auditbeat-8.5.0: . POST auditbeat-8.5.0/_rollover . ",
    "url": "/ece/updateilm/#rollover-des-data-streams",
    
    "relUrl": "/ece/updateilm/#rollover-des-data-streams"
  },"26": {
    "doc": "Produkt Übersicht",
    "title": "Produkt Übersicht",
    "content": "Die ONCITE Open Edition bietet eine benutzerfreundliche virtuelle Maschine (VM) und eine Container-Plattform auf Open-Source-Basis. Sie ist skalierbar von der Größe einer Edge bis zur Größe eines Rechenzentrums und bietet volle Datenhoheit sowie Echtzeitfähigkeiten. ",
    "url": "/edge/productoverview/",
    
    "relUrl": "/edge/productoverview/"
  },"27": {
    "doc": "Produkt Übersicht",
    "title": "Architektur",
    "content": "Um virtuelle Maschinen auf der Edge bereitzustellen, verwenden wir OpenStack. Der bereitgestellte Speicher wird von CEPH verwaltet. Um die virtuellen Maschinen zu erstellen und zu verwalten, kann der Kunde das OperationsCenter verwenden, das über LDAP für Single Sign-On verbunden ist. Administratoren können das OpenStack Horizon verwenden, das nicht mit LDAP verbunden ist. ",
    "url": "/edge/productoverview/#architektur",
    
    "relUrl": "/edge/productoverview/#architektur"
  },"28": {
    "doc": "Operations Center",
    "title": "Operations Center",
    "content": " ",
    "url": "/edge/operationscenter/",
    
    "relUrl": "/edge/operationscenter/"
  },"29": {
    "doc": "VPN as a Service",
    "title": "VPN as a Service",
    "content": " ",
    "url": "/edge/operationscenter/vpnaas/",
    
    "relUrl": "/edge/operationscenter/vpnaas/"
  },"30": {
    "doc": "VPN as a Service",
    "title": "Übersicht",
    "content": "Dieser Abschnitt veranschaulicht, wie die Nutzer mit dem Kundenprojekte von OpenStack über das OpenStack-Projekt-VPN-Gateway kommunizieren. Das OpenStack-Projekt-VPN-Gateway verwendet eine einzige öffentliche IP-Adresse. Es authentifiziert und leitet den Datenverkehr über verschiedene Ports zu den jeweiligen VPN-Servern weiter. Typischerweise hat jedes Projekt seinen eigenen VPN-Server. In dieser Konfiguration hat jedes VPN-Gateway eine einzige öffentliche IP-Adresse, die für alle VPN-Server gültig ist. ",
    "url": "/edge/operationscenter/vpnaas/#%C3%BCbersicht",
    
    "relUrl": "/edge/operationscenter/vpnaas/#übersicht"
  },"31": {
    "doc": "VPN as a Service",
    "title": "Zweck",
    "content": "GEC bietet eine VPNaaS-Lösung an, die es dem Kunden ermöglicht, ihre Anwendungen und Systeme mit den Systemen von Projektpartner zu integrieren. Externer Partner können Einzelpersonen sein, die mit ihren Computern oder Kommunikationssystemen auf das Projekt zugreifen. ",
    "url": "/edge/operationscenter/vpnaas/#zweck",
    
    "relUrl": "/edge/operationscenter/vpnaas/#zweck"
  },"32": {
    "doc": "VPN as a Service",
    "title": "Anforderungen",
    "content": "Die VPNaaS-Lösung hat folgende Anforderungen: . | Das Gateway benötigt eine öffentlich erreichbare IP welche mit der Floating-IP verbunden ist. | . ",
    "url": "/edge/operationscenter/vpnaas/#anforderungen",
    
    "relUrl": "/edge/operationscenter/vpnaas/#anforderungen"
  },"33": {
    "doc": "VPN as a Service",
    "title": "Einschränkungen",
    "content": "Die VPNaaS-Lösung unterliegt den folgenden Einschränkungen: . | Die Anzahl der VPN-Server wird durch den vordefinierten Portbereich im Gateway begrenzt. | Das Netzwerk unterstützt bis zu 250 Benutzer. | Konfigurationsänderungen am Gateway und Server sind nach der Erstellung nicht möglich; sie müssen neu erstellt werden. | Nach der Neuerstellung werden alle OVPN-Konfigurationen ungültig. | . ",
    "url": "/edge/operationscenter/vpnaas/#einschr%C3%A4nkungen",
    
    "relUrl": "/edge/operationscenter/vpnaas/#einschränkungen"
  },"34": {
    "doc": "VPN as a Service",
    "title": "Komponenten und Kommunikationsfluss",
    "content": "Dieser Abschnitt umreißt den Kommunikationsfluss zwischen dem Operations Center und den Kunden-VPN-Servern. Der Kunde initiiert eine Anfrage, und das VPN-Gateway authentifiziert und leitet die Anfragen an den jeweiligen VPN-Server weiter. VPN Gateway . Das VPN-Gateway dient als zentrale Managementzentrale für GEC. Es verwaltet VPN-Server und die Externe Verbindungen. Mit nur einer öffentlichen IP-Adresse stellen IPtable-Regeln sicher, dass Verbindungen zum richtigen VPN-Server geleitet werden. Diese Einrichtung erfordert ein Wide Area Network (WAN) und ein mit dem VPN-Gateway verbundenes VPN-Transfernetzwerk. Das VPN-Gateway verwaltet ausschließlich VPN-Server und Proxy-Verbindungen zu und von diesen Servern, um sicherzustellen, dass der Datenverkehr sein beabsichtigtes Ziel erreicht. VPN Server . Der VPN-Server verwendet OpenVPN, um VPN-Verbindungen mit dem Kundennetzwerk herzustellen. Diese Netzwerkverbindung arbeitet in einem eigenen Virtual Routing and Forwarding (VRF), um die Isolation zu verbessern, ohne zusätzliche Ports freizugeben. Externe Verbindungen werden über das VPN-Gateway geroutet. Der API-Server für den VPN-Server verwaltet den auf der Maschine laufenden OpenVPN-Server. ",
    "url": "/edge/operationscenter/vpnaas/#komponenten-und-kommunikationsfluss",
    
    "relUrl": "/edge/operationscenter/vpnaas/#komponenten-und-kommunikationsfluss"
  },"35": {
    "doc": "VPN as a Service",
    "title": "Sicherheit",
    "content": "WAN . | Nur die für die bestehende VPN-Server erforderlichen Ports sind geöffnet. | SSH ist deaktiviert. | Virtual Routing and Forwarding (VRF)-Trennung ist verfügbar. | Sicherheitsgruppen erlauben nur die VPN-Server-Portbereiche. | Firewall-Regeln sind vorhanden und werden bei Bedarf angepasst. | . VPNaaS . | Separate VRF wird für WAN verwendet. | Firewall-Regeln sind vorhanden und werden bei Bedarf angepasst. | SSH- und API-Zugriff werden bereitgestellt. | SSH-Schlüsselauthentifizierung wird verwendet. | . ",
    "url": "/edge/operationscenter/vpnaas/#sicherheit",
    
    "relUrl": "/edge/operationscenter/vpnaas/#sicherheit"
  },"36": {
    "doc": "VPN as a Service",
    "title": "Server- und Benutzerverwaltung",
    "content": "Serverstatus . | Klicken Sie auf die VPN-Ressource. | Überprüfen Sie den Status unter Eigenschaften. Die Konfigurationsbearbeitung ist nur möglich, wenn der Status aktiv ist. | . Benutzer hinzufügen . Hinweis: Der Projektinhaber wird automatisch für den Remotezugriff hinzugefügt und kann nicht entfernt werden. Voraussetzungen: Sie können nur Benutzer auswählen, die zuvor dem Projekt hinzugefügt wurden. | Gehen Sie zur VPN-Konfiguration. | Wählen Sie den gewünschten Benutzer aus, um ihn als VPN-Benutzer hinzuzufügen. | Speichern Sie die Auswahl. Der Benutzer wird sofort hinzugefügt. | . OVPN-Konfiguration und Passphrase erhalten . Hinweis: Sie können die OVPN-Konfiguration nur für sich selbst oder als Projektinhaber für externe Benutzer herunterladen und eine Passphrase anzeigen. Sie können die OVPN-Konfiguration oder Passphrasen für andere Benutzer nicht herunterladen. | Gehen Sie zur VPN-Konfiguration. | Klicken Sie auf den Welt-Icon-Link und wählen Sie zwischen Download und Passphrase. | . Benutzer löschen . | Klicken Sie auf das Löschsymbol neben dem Benutzer, den Sie löschen möchten. | Klicken Sie auf Speichern. | . ",
    "url": "/edge/operationscenter/vpnaas/#server--und-benutzerverwaltung",
    
    "relUrl": "/edge/operationscenter/vpnaas/#server--und-benutzerverwaltung"
  },"37": {
    "doc": "Openstack",
    "title": "Openstack",
    "content": "Openstack ist die Software die genutzt wird für den Infrastructure as a Service Layer. Alle VMs werden im Openstack erstellt, Openstack ist direkt mit CEPH verbunden worüber der Storage angeboten wird. ",
    "url": "/edge/openstack/",
    
    "relUrl": "/edge/openstack/"
  },"38": {
    "doc": "Openstack",
    "title": "Openstack - Horizon",
    "content": "Das Dashboard für Openstack ist Horizon. Horizon Login . Alle Administratoren erhalten bei der Übergabe einen Login. Horizon Instances . In der Instance Übersicht kann man die VMs ansehen die im ausgewählten Projekt laufen. Horizon Network . Im reiter Network kann man die Netzweke im aktuell Projekt sehen und die Provider Netzwerke. Die Provider Netzwerke sind die Netzwerke welche die VMs zum Kunden Netzwerk verbinden. Desweiteren kann man sich die Netzwerk Topologie ansehen, wie die VMs verbunden sind und wie die Netzwerke verbunden sind. Horizon New Network . Um ein neues Netzwerk anzulegen muss im Reiter “Network” auf “Create Network” geklickt werden. Dann kann man den Namen auswählen. Und man muss ein Subnet erstellen. Um mit dem Netzwerk zu arbeiten oder das Netzwerk nach aussen zu verbinden muss ein Router erstellt werden. Der Router muss dann mit einem Netzwerk verbunden werden. Das passiert durch die Erstellung eines Interfaces im Router, welches den Router dann mit einem Netzwerk verbindet. Um zwei Netzwerke zu verbinden braucht der Router ein Interface in beiden Netzwerken . ",
    "url": "/edge/openstack/#openstack---horizon",
    
    "relUrl": "/edge/openstack/#openstack---horizon"
  },"39": {
    "doc": "VM Erstellung",
    "title": "Erstellung einer VM",
    "content": "Zum erstellen einer neue Virtuele Machine im Openstack muss man sich im Openstack Dashboard einloggen. Diesen Zugriff haben nur Administratoren. Nach dem einloggen, können neue VMs im Reiter Instances erstellt werden. Man klickt auf “Launch Instance” um mit einen Wizard durch die Erstellung der VM durch geführt zu werden. ",
    "url": "/edge/openstack/create_vm/#erstellung-einer-vm",
    
    "relUrl": "/edge/openstack/create_vm/#erstellung-einer-vm"
  },"40": {
    "doc": "VM Erstellung",
    "title": "Details",
    "content": "Hier kann der Name ausgesucht werden, welche die VM erhält. Desweiteren kann die Anzahl der VMs geändert werden, der standard ist eine. ",
    "url": "/edge/openstack/create_vm/#details",
    
    "relUrl": "/edge/openstack/create_vm/#details"
  },"41": {
    "doc": "VM Erstellung",
    "title": "Source",
    "content": "Hier kann ausgewählt werden ob die Instance aus einem existierenden Volume erstellt werden soll. Oder ob die VM aus einem Image erstellt werden soll. Desweiteren kann eingestellt werden ob für die VM ein neues Volume erstellt werden soll und ob dieses Volume nach der Löschung der VM mit gelöscht werden soll. ",
    "url": "/edge/openstack/create_vm/#source",
    
    "relUrl": "/edge/openstack/create_vm/#source"
  },"42": {
    "doc": "VM Erstellung",
    "title": "Flavour",
    "content": "Hier kann eingestellt werden welche größe die VM haben soll und ob diese gegebenfalls mit einer Grafikkarte oder TSN Karte ausgestattet werden soll. ",
    "url": "/edge/openstack/create_vm/#flavour",
    
    "relUrl": "/edge/openstack/create_vm/#flavour"
  },"43": {
    "doc": "VM Erstellung",
    "title": "Network",
    "content": "Hier kann das Netzwerk ausgewählt werden mit welcher die VM bei der Erstellung verbunden ist. ",
    "url": "/edge/openstack/create_vm/#network",
    
    "relUrl": "/edge/openstack/create_vm/#network"
  },"44": {
    "doc": "VM Erstellung",
    "title": "Key Pair",
    "content": "Hier kann der SSH Key ausgewählt werden, mit welchen man sich über SSH verbinden kann. Desweiteren können hier neue SSH keys importiert werden. ",
    "url": "/edge/openstack/create_vm/#key-pair",
    
    "relUrl": "/edge/openstack/create_vm/#key-pair"
  },"45": {
    "doc": "VM Erstellung",
    "title": "VM Erstellung",
    "content": " ",
    "url": "/edge/openstack/create_vm/",
    
    "relUrl": "/edge/openstack/create_vm/"
  },"46": {
    "doc": "FAQ",
    "title": "FAQ",
    "content": " ",
    "url": "/edge/faq/",
    
    "relUrl": "/edge/faq/"
  },"47": {
    "doc": "FAQ",
    "title": "Openstack",
    "content": "Wie werden User hinzugefügt? . Aktuell nur auf Anfrage per Jira Helpdesk Ticket da jeder Openstack User auch Admin ist. Wie kann man VM’s löschen von anderen Usern/Projekten? Im Horizon dashboard kann man über den Adminbereich auf alle VMs zugreifen und diese löschen. Sonst ist der Zugriff auch über die Openstack CLI möglich . Wie kann die Plattengröße einer bestehenden VM erweitert werden? . Dies ist in der aktuellen Openstack Version nicht möglich. Welche Subnetze gibt es . Die folgenden Subnetze sind immer vorhanden: . | public | shared | private/internal | . Auf Anfrage kann ein viertes Netzwerk, Labor, für eine direkte Verbindung ohne die Barracuda Firewall erstellt werden. Wie erfolgt die Trennung der Subnetze: . Subnetze werden über OpenVSwitch virtualisiert zur garantierten Trennung können Securityregeln angelegt werden. Diese Regeln sind durch IPFilter Regeln in OpenStack Neutron dar. Die Einhaltung von Zonen wird im Operations Center über Sicherheitsregeln und Sicherheitsgruppen verwaltet. ",
    "url": "/edge/faq/#openstack",
    
    "relUrl": "/edge/faq/#openstack"
  },"48": {
    "doc": "FAQ",
    "title": "Operations Center",
    "content": "Wie werden User hinzugefügt im Operations Center? . Der Zugriff auf das Operations Center wird über SSO realisiert. Nachdem Benutzer sich angemeldet haben können sie durch Projekteigentümer zu einem Projekt hinzugefügt werden. Jeder Nutzer kann eigene Projekte anlegen. Zusätzlich können im Operations Center externe Benutzer angelegt werden. Siehe Rolle Externe Benutzer . Gruppen vs Rollen Unterschied von Rollen und Gruppen . Das Operations Center hat aktuell 4 Benutzergruppen: . | Externe Benutzer: Externe Benutzer werden im Operations Center verwaltet. Sie können Zugriff per VPN auf VMs erhalten. Zusätzlich kann der öffentliche SSH Schlüssel zu VMS hinzugefügt werden. | Benutzer (SSO): Benutzer können Projekte anlegen und Verwalten. Auf Projektebene gibt es drei Rollen | . | Owner: Vollzugriff auf das Projekt. | . | . | Member: Vollen Zugriff auf die Projektresourcen ohne Benutzerverwaltung. | . | . | Viewer: Nur lesender Zugriff. Der Gast kann keine Passwörter für VMs einsehen. | . | Admin (SSO)*können externe Benutzer verwalten und haben Zugriff auf die Ressourcenübersicht und Globale Nachrichten Funktion. | Global Admin (SSO)*: Haben zusätzlichen Zugriff auf alle Projekte und können das VPNaaS für die Projekte aktivieren VPN können die normalen Admins auch anlegen | . *Anfrage an GEC notwendig nachdem der Benutzer sich das erste mal am Operations Center angemeldet hat. Admin Rechte nur auf einem Projekt? Wie? . Benutzer haben automatisch vollen Zugriff auf Projekte, die sie anlegen. Andere Projektbesitzer können das Recht weitergeben. Wie kann ich den Besitzer von einem Projekt ändern? . Jeder Projektbesitzer und Admin mit Zugriff auf das Projekt kann die Berechtigungen ändern. Was passiert wenn ein User das Unternehmen verlässt? . Durch die Integration von SSO verliert der Benutzer automatisch den Zugriff. Es ist ggfls. notwendig seine Projekte neu zuzuweisen. Siehe vorheriger Punkt. VPN Zugänge müssen manuell gelöscht werden. Können Vorgaben gemacht werden wie ein Projekt heißen soll? . Beim Anlegen. Eine Änderung ist nicht vorgesehen. Wie wird ein VPN angelegt? . Globale Administratoren können die Funktion für einzelne Projekte aktivieren. Projektbesitzer können weiter Benutzer die dem Projekt hinzugefügt sind freischalten. Benutzer brauchen Zugriff auf das Projekt um ihre eigene Konfiguration herunterzuladen. Externe Benutzer . Externe Benutzer können auch für VPN freigeschaltet werden. Die VPN Konfiguration für externe Benutzer muss durch einen Projektbesitzer Benutzer heruntergeladen und zur Verfügung gestellt werden. Wie werden Projektefreigaben angelegt bzw. erstellt? . Auf der Startseite jedes Projektes über die Mitgliederliste. Wie werden die Flavor verwaltet? . Flavor werden in Openstack verwaltet. Jedes “public” Flavor steht im OC zur Konfiguration bereit. Falls ein “public” Flavor nicht mehr im Operations Center beim Erstellen einer VM auswählbar sein soll, dann können die Metadaten des Flavors entsprechend geändert werden. Dazu in Horizon das Flavor auswählen, auf “Aktualisiere Metadaten” klicken und im Feld “Custom” visibility eintragen und mit “+” hinzufügen. Das Feld taucht jetzt unter “Existing Metadata” auf. Hier den Wert “false” eintragen und die Änderung speichern. Das Flavor kann jetzt nicht mehr beim erstellen einer neuen VM ausgewählt werden. Bestehende VMs die dieses Flavor benutzen werden nicht beeinflusst und können weiterhin im Operations Center verwaltet werden. Flavor können nicht auf einzelne Benutzer beschränkt werden. Dürfen Flavor gelöscht werden . Ja, aber nur wenn sichergestellt ist, dass keine VM das Flavor nutzt. Das Löschen erfolgt über Horizon. Sollte ein Flavor, welches noch in Nutzung ist, gelöscht werden, kann jedes Projekt mit einer betroffenen VM nicht mehr über das Operations Center verwaltet werden. Wo kann man Logs sehen, wenn eine VM angelegt wurde aber sie nicht erscheint? . Über Horizon ist es möglich den Startvorgang der VM zu beobachten und zu wiederholen. Wird die VM in Horizon nicht angelegt, ist aktuell ein Ticket im Helpdesk anzulegen. Wie kann man VMs löschen von anderen Usern/Projekten? . Nur Benutzer und Globale Administratoren mit Zugriff auf Projekte können VMs löschen. Projektliste Operations Center vs Horizon . Die Projekte vom Operations Center sind auch in Horizon sichtbar. Projekte, die in Horizon erzeugt werden sind nicht im Operations Center sichtbar. Wie können VMs gestartet werden? . Der Start von VMs ist über die Schaltfläche “Starten” im Operations Center möglich. Wie kann die Plattengröße einer bestehenden VM erweitert werden im Operations Center? . Es ist nicht möglich. Wie können externe/interne User gelöscht werden? . Interne User werden über das SSO authentifiziert. Werden sie dort deaktiviert, werden sie automatisch gesperrt. Externe Benutzer können in der Übersicht der Externen Benutzer im Operations Center von Administratoren gelöscht werden. Wie kann ich den Router bearbeiten und Konfigurieren? . Das Operations Center konfiguriert die Router entsprechend der Projektvorgaben automatisch. Wie können Routen bearbeitet werden im Operations Center? . Es können im Operations Center keine zusätzlichen Routen eingerichtet werden. Wie kann man einen SSH key einer vorhandenen VM hinzufügen? . Es ist nicht möglich SSH keys nach der Erstellung einer VM über das Operations Center hinzuzufügen. Dies müssen direkt in der VM hinzugefügt werden. Volumegrößen . Das Operations Center erlaubt das Anlegen von Volumes in unterschiedlichen Größen. Die Liste kann auf Anfrage erweitert werden. Kann das Operations Center per API konfiguriert werden . Nein, das ist nicht vorgesehen. Wie kann ich mich bei einer Windows VM anmelden? Der User operation funktioniert nicht mit dem eingegebenen Passwort. Anstelle des Benutzers “operation” muss für Windows VMs der Benutzer des ausgewählten images verwendet werden. Oft Administrator. ",
    "url": "/edge/faq/#operations-center",
    
    "relUrl": "/edge/faq/#operations-center"
  },"49": {
    "doc": "FAQ",
    "title": "General",
    "content": "Zonen . Sicherheitszonen sind in hierarchisch absteigender Reihenfolge strukturiert, beginnend mit der Private / Internal Zone, gefolgt von der Shared Zone und Public Zone. Auf dieser Grundlage können virtuelle Maschinen (VMs) durch die Zuweisung von Floating IPs hierarchisch absteigend kommunizieren. Wenn VMs in sich in der Private / Internal Zonen befinden ist eine Kommunikation mit den Floating IPs in der Shared und Public Zone möglich. Allerdings ist der umgekehrte Weg, also von der Shared Zone zurPrivate / Internal Zonen, nicht gestattet. Analog ist die Kommunikation von der Public Zone in die Shared Zone zur Private / Internal Zonen nicht gestattet. Die Kommunikation folgt dabei stets dem Prinzip von sicher zu unsicher. Diese hierarchische Struktur ermöglicht eine geordnete und sicherheitsbewusste Kommunikation zwischen den verschiedenen Zonen. Des weiteren sind Floating IPs aus der Private / Internal Zonen nur aus dem Instituts Netz erreichbar, Services mit einer Floating IP aus der Public IP können durch die Instituts IT ins Internet bereitgestellt werden. Als spezial Zone kann auf Anfrage eine Lab Zone eingerichtet werden mit direkt Verbindung an den Switchen. Die Zonen werden durch das Operations Center verwaltet. Bei Projekten, die in Openstack angelegt werden, ist der Nutzer selbst verantwortlich. Wie können die Hardware Server runter/hoch gefahren werden? z.B. bei einer kurzfristigen Stromabschaltung. Aktuell nur per Anfrage im Jira Helpdesk . Starten die Server nach Strom automatisch? . Die Server Starten automatisch aber die Edge ist nicht automatisch nutzbar, die bei einem geregelten Herunterfahren die CEPH Replikation deaktiviert wird. Diese muss bei einem Neustart wieder aktiviert werden. Wenn das beigefügte Windows Image ausgewählt ist, ist da bereits eine Lizenz mit dabei. Um was für eine Lizenz handelt es sich? . Es handelt sich um eine Trial Lizenz von Microsoft. Bei Windows VM’s, ist der Kunde für den Erwerb und das Einfügen einer Lizenz verantwortlich. NVLink und NVSwitches . NV Link und NVSwitches verbinden NVidia Grafikkarten zum direkten Austausch von Daten zwischen den Grafikkarten. Aktuell verbauen wir Supermicro GPU Server diese haben keine NVLink oder NVSwitch zwischen den einzelnen GPUs. Zum aktuellen Zeitpunkt werden NVLink und NVSwitches nur in NVidia eigenen Servern verbaut der Marke DGX oder HGX. ",
    "url": "/edge/faq/#general",
    
    "relUrl": "/edge/faq/#general"
  },"50": {
    "doc": "Guided Tour",
    "title": "Guided Tour",
    "content": "Aus dem Browser zum eigenen Stack per Heat . ",
    "url": "/optimist/guided_tour/",
    
    "relUrl": "/optimist/guided_tour/"
  },"51": {
    "doc": "01: Das Dashboard (Horizon)",
    "title": "Schritt 1: Das Dashboard (Horizon)",
    "content": " ",
    "url": "/optimist/guided_tour/step01/#schritt-1-das-dashboard-horizon",
    
    "relUrl": "/optimist/guided_tour/step01/#schritt-1-das-dashboard-horizon"
  },"52": {
    "doc": "01: Das Dashboard (Horizon)",
    "title": "Vorwort",
    "content": "In diesem Schritt für Schritt Tutorial werden wir uns schrittweise der Bedienung von Openstack widmen. Den Anfang macht das Horizon(Dashboard), nach einer kleinen Einführung, wird dann auf die Konsole gewechselt und der Abschluss bildet die Erstellung eigener Heat-Templates. ",
    "url": "/optimist/guided_tour/step01/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step01/#vorwort"
  },"53": {
    "doc": "01: Das Dashboard (Horizon)",
    "title": "Login",
    "content": "Nachdem die Zugangsdaten vorliegen, ist der erste Schritt der Login. WICHTIG: Es gibt keinen Reset-Knopf für das Passwort. Für ein neues Passwort, schreiben Sie uns bitte eine E-Mail an support@gec.io. Hierzu wechseln wir im Browser auf folgende URL: https://optimist.gec.io/ . Im sich öffnenden Fenster wählen wir bei Domain default, und tragen den zugesendeten Benutzer (User-Name) sowie das zugehörige Passwort(Password) ein und klicken auf Connect. Nun öffnet sich das Horizon(Dashboard). ",
    "url": "/optimist/guided_tour/step01/#login",
    
    "relUrl": "/optimist/guided_tour/step01/#login"
  },"54": {
    "doc": "01: Das Dashboard (Horizon)",
    "title": "Passwort ändern",
    "content": "Da aus Sicherheitsgründen empfohlen wird das Passwort nach Erhalt zu ändern, klicken wir im Horizon(Dashboard) dafür rechts oben auf den Benutzernamen(1) und auf Settings(2). Im sich nun öffnenden Fenster sehen wir zuerst Settings, wo unter anderem auch die Sprache umgestellt werden kann. Um das Passwort zu ändern, klicken wir rechts auf Change Password(1). Hier können nun das Passwort geändert werden. Dafür geben wir zunächst unser bisheriges Passwort ein(2), geben dann das neue an(3) und bestätigen es in der neuen Zeile (4). Damit das neue Passwort auch übernommen wird, fehlt noch ein Klick auf Change(5). ",
    "url": "/optimist/guided_tour/step01/#passwort-%C3%A4ndern",
    
    "relUrl": "/optimist/guided_tour/step01/#passwort-ändern"
  },"55": {
    "doc": "01: Das Dashboard (Horizon)",
    "title": "Abschluss",
    "content": "Sie haben Ihre ersten Schritte im Dashboard ausgeführt und Ihr Passwort geändert! . ",
    "url": "/optimist/guided_tour/step01/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step01/#abschluss"
  },"56": {
    "doc": "01: Das Dashboard (Horizon)",
    "title": "01: Das Dashboard (Horizon)",
    "content": " ",
    "url": "/optimist/guided_tour/step01/",
    
    "relUrl": "/optimist/guided_tour/step01/"
  },"57": {
    "doc": "02: SSH-Key per Horizon anlegen",
    "title": "Schritt 2: SSH-Key per Horizon anlegen",
    "content": " ",
    "url": "/optimist/guided_tour/step02/#schritt-2-ssh-key-per-horizon-anlegen",
    
    "relUrl": "/optimist/guided_tour/step02/#schritt-2-ssh-key-per-horizon-anlegen"
  },"58": {
    "doc": "02: SSH-Key per Horizon anlegen",
    "title": "Vorwort",
    "content": "Um im nächsten Schritt einen Stack inkl. einer Instanz zu starten, wird ein SSH Keypair (Schlüsselpaar) benötigt. Für den Fall, dass bereits ein Keypair vorhanden ist und der Umgang damit bekannt ist, kann dieser Schritt übersprungen werden. ",
    "url": "/optimist/guided_tour/step02/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step02/#vorwort"
  },"59": {
    "doc": "02: SSH-Key per Horizon anlegen",
    "title": "Installation",
    "content": "Es gibt verschiedene Wege, um ein Keypair zu erzeugen. Einer der späteren Schritte erklärt den Weg zu einem selbst erstellten Keypair. Hier wird der Schlüssel direkt im Horizon(Dashboard) erstellt, um im nächsten Schritt den Stack zu erstellen. Um nun den Schlüssel zu erstellen, wechseln wir im Horizon(Dashboard) in der Navigation auf Compute → Key Pairs und klicken dort auf Create Key Pair. Im sich öffnenden Fenster kann nun ein Name für den Key vergeben werden, in dem Beispiel wird BeispielKey verwendet, anschließend klicken wir auf Create Key Pair. ",
    "url": "/optimist/guided_tour/step02/#installation",
    
    "relUrl": "/optimist/guided_tour/step02/#installation"
  },"60": {
    "doc": "02: SSH-Key per Horizon anlegen",
    "title": "Abschluss",
    "content": "Wir haben jetzt unser SSH Keypair erstellt und sind bereit für den Rest des Tutorials! . ",
    "url": "/optimist/guided_tour/step02/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step02/#abschluss"
  },"61": {
    "doc": "02: SSH-Key per Horizon anlegen",
    "title": "02: SSH-Key per Horizon anlegen",
    "content": " ",
    "url": "/optimist/guided_tour/step02/",
    
    "relUrl": "/optimist/guided_tour/step02/"
  },"62": {
    "doc": "03: Einen Stack starten",
    "title": "Schritt 3: Einen Stack starten",
    "content": " ",
    "url": "/optimist/guided_tour/step03/#schritt-3-einen-stack-starten",
    
    "relUrl": "/optimist/guided_tour/step03/#schritt-3-einen-stack-starten"
  },"63": {
    "doc": "03: Einen Stack starten",
    "title": "Vorwort",
    "content": "In diesem Schritt beschäftigen wir uns damit, im Horizon Dashboard einen Stack zu starten und damit auch das Horizon Dashboard besser kennenzulernen. Wichtige Voraussetzung ist an dieser Stelle ein SSH-Key, den wir in Schritt 2 erzeugt haben. ",
    "url": "/optimist/guided_tour/step03/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step03/#vorwort"
  },"64": {
    "doc": "03: Einen Stack starten",
    "title": "Start",
    "content": "Um einen Stack zu starten, loggen wir uns zunächst im Horizon Dashboard mit denen in Schritt 1 geänderten Zugangsdaten ein. Hier navigieren wir über Orchestration zu Stacks und klicken auf Launch Stack. Um den Stack auch zu starten, benötigen wir zunächst ein Template, welches in dem Stack eine Instanz startet. Hierfür nutzen wir die SingleServer.yaml aus dem GECio Github Repository. In dem sich nun öffnenden Fenster, wählen wir bei Template Source File aus und nehmen bei Template File, die eben heruntergeladene SingleServer.yaml. Den Rest belassen wir so wie es ist und klicken auf Next. Nun werden weitere Eingaben benötigt, genauer sind das folgende und am Ende klicken wir auf Launch: . | Stack Name: BeispielServer | Creation Timeout: 60 | Password for User: Bitte das eigene Passwort eintragen | availability_zone: ix1 | flavor_name: m1.micro | key_name: BeispielKey | machine_name: singleserver | public_network_id: provider | . Nun wird der Stack auch direkt gestartet und das Horizon Dashboard sieht dann so aus: . Um nun zu überprüfen ob die Instanz korrekt gestartet wurde, wechseln wir in der Navigation auf Compute → Instances und die Übersicht sieht dann wie folgt aus: . Nachdem nun also der Stack und auch die darin enthaltene Instanz gestartet wurden, löschen wir jetzt wieder den Stack inklusive Instanz. Wir könnten auch die Instanz alleine löschen, das kann aber im Nachgang zu Problemen beim löschen des Stacks führen. Um den Stack zu löschen, wechseln wir in der Navigation wieder auf Orchestration → Stacks. Klicken hinter dem Stack, unter Actions, auf den Pfeil nach unten und wählen dort Delete Stack. ",
    "url": "/optimist/guided_tour/step03/#start",
    
    "relUrl": "/optimist/guided_tour/step03/#start"
  },"65": {
    "doc": "03: Einen Stack starten",
    "title": "Abschluss",
    "content": "Wir haben unseren ersten Stack erstellt … und ihn dann gelöscht! . ",
    "url": "/optimist/guided_tour/step03/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step03/#abschluss"
  },"66": {
    "doc": "03: Einen Stack starten",
    "title": "03: Einen Stack starten",
    "content": " ",
    "url": "/optimist/guided_tour/step03/",
    
    "relUrl": "/optimist/guided_tour/step03/"
  },"67": {
    "doc": "04: Der Weg vom Horizon auf die Kommandozeile",
    "title": "Schritt 4: Der Weg vom Horizon auf die Kommandozeile",
    "content": " ",
    "url": "/optimist/guided_tour/step04/#schritt-4-der-weg-vom-horizon-auf-die-kommandozeile",
    
    "relUrl": "/optimist/guided_tour/step04/#schritt-4-der-weg-vom-horizon-auf-die-kommandozeile"
  },"68": {
    "doc": "04: Der Weg vom Horizon auf die Kommandozeile",
    "title": "Vorwort",
    "content": "Auf den ersten Blick kann es komfortabel erscheinen, seine OpenStack Umgebung mit dem Horizon Dashboard zu verwalten. Für einfache, nicht wiederkehrende Aufgaben, kann das Horizon Dashboard mit seinen grafische Ansichten wirklich hilfreich sein. Sobald Aufgaben regelmäßig wiederholt werden oder ein komplexerer Stack verwaltet werden soll, ist es sinnvoller, den OpenStack Client und auch Heat(welches in den späteren Schritten mit erklärt wird) zu verwenden. Anfangs mag die Handhabung ungewohnt sein, mit ein wenig Übung kann die Arbeit an den eigenen Stacks schnell und effizient erledigt werden. Der OpenStack Client ist sehr hilfreich bei der Administration der OpenStack Umgebung, da dort bereits Komponenten wie Nova, Glance, Heat, Cinder, Neutron enthalten sind. Da wir auch im weiteren Verlauf der Dokumentation den Client nutzen, installieren wir ihn in diesem Schritt. ",
    "url": "/optimist/guided_tour/step04/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step04/#vorwort"
  },"69": {
    "doc": "04: Der Weg vom Horizon auf die Kommandozeile",
    "title": "Installation",
    "content": "Um den OpenStackClient installieren zu können, wird mindestens Python 2.7 noch die Python Setuptools (diese sind bei macOS bereits vorinstalliert). Es gibt verschiedene Optionen die Installation durchzuführen, pip hat sich hierbei als eine gute Lösung herausgestellt und wird als Grundlage in der Dokumentation verwendet. Selbiges ist einfach zu bedienen, stellt sicher das die aktuellste Version der Pakete genutzt wird und kann im Nachhinein Updates einspielen. Man kann den Client ohne weiteres in root/admin installieren, das kann aber zu weiteren Problem führen, daher nutzen wir eine virtuelle Umgebung für den Clienten. macOS . Damit nun der OpenStack Client installieren werden kann, wird zunächst pip benötigt. Um pip zu installieren, wird zunächst die Konsole geöffnet (diese kann zum Beispiel über das Launchpad → Konsole geöffnet werden)  und dann folgender Befehl ausgeführt: . $ easy_install pip Searching for pip Best match: pip 9.0.1 Adding pip 9.0.1 to easy-install.pth file Installing pip script to /usr/local/bin Installing pip2.7 script to /usr/local/bin Installing pip2 script to /usr/local/bin Using /usr/local/lib/python2.7/site-packages Processing dependencies for pip Finished processing dependencies for pip . Sobald die Installation von pip abgeschlossen ist, wird nun die Virtuelle Umgebung angelegt: . $ pip install virtualenv Collecting virtualenv Downloading virtualenv-15.1.0-py2.py3-none-any.whl (1.8MB) 100% |????????????????????????????????| 1.8MB 619kB/s Installing collected packages: virtualenv Successfully installed virtualenv-15.1.0 . Nachdem wir nun mit virtualenv eine virtuelle Umgebung nutzen können, erstellen wir direkt eine: . $ virtualenv ~/.virtualenvs/openstack New python executable in /Users/iNNOVO/.virtualenvs/openstack/bin/python Installing setuptools, pip, wheel...done. Jetzt wechseln wir in die erzeugte virtuelle Umgebung: . $ source ~/.virtualenvs/openstack/bin/activate (openstack) $ . Nachdem dieser Wechsel funktioniert hat, können wir in der geschaffenen Umgebung auch direkt den OpenStackClient installieren: . (openstack) $ pip install python-openstackclient . Da wir im Verlauf der Dokumentation auch andere Services benutzen, installieren wir die entsprechenden Clienten gleich mit: . (openstack) $ pip install python-heatclient python-designateclient python-octaviaclient . Nun verlassen wir die Umgebung auch direkt wieder: . (openstack) $ deactivate . Damit wir den OpenStackClient auch nutzen können, ist es nun notwendig dies in die Path Variablen aufzunehmen. export PATH=\"$HOME/.virtualenvs/openstack/bin/:$PATH\" . Um zu sehen ob alles korrekt funktioniert hat, testen wir die Ausgabe: . $ type -a openstack openstack is /home/iNNOVO/.virtualenvs/openstack/bin/openstack . Windows . Um pip zu nutzen, ist zuerst der Wechsel in den Ordner der Python Installation notwendig (Speicherort der Standart Installation: C:\\Python27\\Scripts). pip wird dann mit dem Befehl easy_install pip installiert: . C:\\Python27\\Scripts&gt;easy_install pip Searching for pip Best match: pip 9.0.1 Adding pip 9.0.1 to easy-install.pth file Installing pip-script.py script to c:\\python27\\scripts Installing pip.exe script to c:\\python27\\scripts Installing pip.exe.manifest cript to c:\\python27\\scripts Installing pip3.5-script.py script to c:\\python27\\scripts Installing pip3.5.exe script to c:\\python27\\scripts Installing pip3.5.exe.manifest script to c:\\python27\\scripts Installing pip3-script.py script to c:\\python27\\scripts Installing pip3.exe script to c:\\python27\\scripts Installing pip3.exe.manifest script to c:\\python27\\scripts Using c:\\python27\\lib\\site-packages Processing dependencies for pip Finished processing dependencies for pip . Nach der erfolgreichen Installation von pip, kann direkt mit pip install python-openstackclient der OpenStackClient auch installiert werden: . C:\\Python27\\Scripts&gt;pip install python-openstackclient Collecting python-openstackclient Downloading python_openstackclient-3.12.0-py2.py3-none-any.whl (772kB) 100% |################################| 778kB 1.1MB/s . Linux (in diesem Beispiel Ubuntu) . Zunächst wird auch hier pip benötigt, dafür wird apt-get genutzt: . $ sudo apt-get install python3-pip Reading package lists... Done Building dependency tree Reading state information... Done . Sobald die Installation von pip abgeschlossen ist, wird nun die Virtuelle Umgebung angelegt: . $ sudo apt-get install python3-virtualenv Reading package lists... Done Building dependency tree Reading state information... Done . Nachdem wir nun mit virtualenv eine virtuelle Umgebung nutzen können, erstellen wir direkt eine: . $ virtualenv ~/.virtualenvs/openstack New python executable in /Users/iNNOVO/.virtualenvs/openstack/bin/python Installing setuptools, pip, wheel...done. Jetzt wechseln wir in die erzeugte virtuelle Umgebung: . $ source ~/.virtualenvs/openstack/bin/activate (openstack) $ . Nachdem dieser Wechsel funktioniert hat, können wir in der geschaffenen Umgebung auch direkt den OpenStackClient installieren: . (openstack) $ pip install python-openstackclient . Da wir im Verlauf der Dokumentation auch Heat benutzen, installieren wir den entsprechenden Heat Clienten gleich mit: . (openstack) $ pip install python-heatclient . Nun verlassen wir die Umgebung auch direkt wieder: . (openstack) $ deactivate . Damit wir den OpenStackClient auch nutzen können, ist es nun notwendig dies in die Path Variablen aufzunehmen. export PATH=\"$HOME/.virtualenvs/openstack/bin/:$PATH\" . Um zu sehen ob alles korrekt funktioniert hat, testen wir die Ausgabe: . $ type -a openstack openstack is /home/iNNOVO/.virtualenvs/openstack/bin/openstack . ",
    "url": "/optimist/guided_tour/step04/#installation",
    
    "relUrl": "/optimist/guided_tour/step04/#installation"
  },"70": {
    "doc": "04: Der Weg vom Horizon auf die Kommandozeile",
    "title": "Zugangsdaten",
    "content": "Nachdem der OpenStackClient nun installiert ist, werden noch die Zugangsdaten für Openstack benötigt. Diese können direkt im Horizon Dashboard heruntergeladen werden. Dafür loggen wir uns ein und klicken dann rechts oben in der Ecke auf die E-Mail-Adresse und dann auf Download OpenStack RC File v3. Die heruntergeladene Datei trägt den Projektnamen (Projektname.sh), in unserem Beispiel nennen wir sie Beispiel.sh . macOS | Linux . Um die Zugangsdaten in den OpenStackClienten einzulesen, führen wir nun folgenden Befehl aus: . source Beispiel.sh . Windows . Um unter Windows die Zugangsdaten einzulesen, ist es notwendig entweder PowerShell, Git for Windows oder Linux on Windows zu nutzen. Bei Linux on Windows und Git for Windows via Git Bash, wird der gleiche Befehl wie im Beispiel für macOS | Linux genutzt: . source Beispiel.sh . Bei der Nutzung von PowerShell müssen die Variablen einzeln gesetzt werden. Alle notwendigen Variablen befinden sich in der Datei Beispiel.sh und diese kann mit einem Editor geöffnet werden. Um die Variablen zu setzen, kann folgender Befehl genutzt werden: . set-item env:OS_AUTH_URL -value \"https://identity.optimist.gec.io/v3\" set-item env:OS_PROJECT_ID -value \"Projekt ID eintragen\" set-item env:OS_PROJECT_NAME -value \"Namen eintrage\" set-item env:OS_USER_DOMAIN_NAME -value \"Default\" set-item env:OS_USERNAME -value \"Usernamen eintragen\" set-item env:OS_PASSWORD -value \"Passwort eingeben\" set-item env:OS_USER_DOMAIN_NAME -value \"Default\" set-item env:OS_REGION_NAME -value \"fra\" set-item env:OS_INTERFACE -value \"public\" set-item env:OS_IDENTITY_API_VERSION -value \"3\" . ",
    "url": "/optimist/guided_tour/step04/#zugangsdaten",
    
    "relUrl": "/optimist/guided_tour/step04/#zugangsdaten"
  },"71": {
    "doc": "04: Der Weg vom Horizon auf die Kommandozeile",
    "title": "Ziel",
    "content": "Die Installation des OpenStackClienten ist abgeschlossen und die ersten Befehle können damit getestet werden. Eine Übersicht über alle Befehle, kann mit folgendem Kommando abgerufen werden: . openstack --help . ",
    "url": "/optimist/guided_tour/step04/#ziel",
    
    "relUrl": "/optimist/guided_tour/step04/#ziel"
  },"72": {
    "doc": "04: Der Weg vom Horizon auf die Kommandozeile",
    "title": "04: Der Weg vom Horizon auf die Kommandozeile",
    "content": " ",
    "url": "/optimist/guided_tour/step04/",
    
    "relUrl": "/optimist/guided_tour/step04/"
  },"73": {
    "doc": "05: Die wichtigsten Befehle des OpenStackClients",
    "title": "Schritt 5: Die wichtigsten Befehle des OpenStackClients",
    "content": " ",
    "url": "/optimist/guided_tour/step05/#schritt-5-die-wichtigsten-befehle-des-openstackclients",
    
    "relUrl": "/optimist/guided_tour/step05/#schritt-5-die-wichtigsten-befehle-des-openstackclients"
  },"74": {
    "doc": "05: Die wichtigsten Befehle des OpenStackClients",
    "title": "Vorwort",
    "content": "Nachdem in Schritt 4 der OpenStack Client installiert wurde, werden wir in diesem Schritt alle wichtigen Befehle einmal auflisten. Die Übersicht der spezifischen Subbefehle kann in der Kommandozeile mit einem --help hinter dem eigentlichen Befehl separat angezeigt werden. Um alle Befehle aufzulisten, kann der Schalter --help auch ohne weitere Angaben von einem Bestandteil  verwendet werden: . openstack --help . ",
    "url": "/optimist/guided_tour/step05/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step05/#vorwort"
  },"75": {
    "doc": "05: Die wichtigsten Befehle des OpenStackClients",
    "title": "Server",
    "content": "Mit dem Kommando openstack server ist es mögliche eigene Instanz zu erstellen, diese zu verwalten, zu löschen und andere} administrative Aufgaben durchzuführen. Hier eine Liste der wichtigsten Kommandos:} . | openstack server add Einer bestehenden Instanz können verschiedene Bestandteile (Fixed IP, Floating IP, Security Group, Volume) zugewiesen werden | openstack server create Mit diesem Befehl kann eine neue Instanz erstellt werden | openstack server delete Löscht die im Befehl angegebene Instanz | openstack server list Listet alle bestehenden Instanzen auf | openstack server remove Kann verschiedene Bestandteile (Fixed IP, Floating IP, Security Group, Volume) wieder entfernen | openstack server show Zeigt alle verfügbaren Informationen zu der im Befehl genannten Instanz an | . ",
    "url": "/optimist/guided_tour/step05/#server",
    
    "relUrl": "/optimist/guided_tour/step05/#server"
  },"76": {
    "doc": "05: Die wichtigsten Befehle des OpenStackClients",
    "title": "Stack (Heat)",
    "content": "Genauso wie mit openstack server ... Befehlen einzelne Instanzen administriert werden, kann man mit openstack stack ... ganze Stacks verwalten. Auch hier eine kurze Auflistung der wichtigsten Befehle: . | openstack stack create Kann einen neuen Stack erstellen | openstack stack list Listet alle bestehenden Stacks auf | openstack stack show Zeigt alle Informationen zu dem im Befehl angegebenen Stack | openstack stack delete Löscht den im Befehl angegebenen Stack | . ",
    "url": "/optimist/guided_tour/step05/#stack-heat",
    
    "relUrl": "/optimist/guided_tour/step05/#stack-heat"
  },"77": {
    "doc": "05: Die wichtigsten Befehle des OpenStackClients",
    "title": "Security Group",
    "content": "Security Groups werden verwendet, um Instanzen eingehende und ausgehende Netzwerk-Verbindungen basierend auf IP-Adressen und Ports zu erlauben oder zu verbieten. Auch Security Groups kann man mit dem OpenStackClienten verwalten. Hier eine beispielhafte Liste üblicher Aufrufe: . | openstack security group create Erstellt eine neue Security Group | openstack security group delete Löscht die im Befehl angegebene Security Group | openstack security group list Listet alle bestehenden Gruppen auf | openstack security group show Zeigt alle verfügbaren Informationen zu der im Befehl angegebenen Security Group | openstack security group rule create Fügt eine Regel zu einer Security Group hinzu | openstack security group rule delete Löscht die im Befehl angegeben Regel | . ",
    "url": "/optimist/guided_tour/step05/#security-group",
    
    "relUrl": "/optimist/guided_tour/step05/#security-group"
  },"78": {
    "doc": "05: Die wichtigsten Befehle des OpenStackClients",
    "title": "Network",
    "content": "Um später auch Instanzen sinnvoll nutzen zu können, benötigen diesen ein Netzwerk, hier eine kurze Auflistung der wichtigsten Befehle um ein Netzwerk zu erstellen: . | openstack network create Erstellt ein neues Netzwerk | openstack nerwork list Listet alle bestehenden Netzwerke auf | openstack network show Zeigt alle Informationen zu dem im Befehl angegebenen Netzwerk | openstack network delete Löscht das im Befehl angegebene Netzwerk | . ",
    "url": "/optimist/guided_tour/step05/#network",
    
    "relUrl": "/optimist/guided_tour/step05/#network"
  },"79": {
    "doc": "05: Die wichtigsten Befehle des OpenStackClients",
    "title": "Router",
    "content": "Damit eine Instanz mit einem Netzwerk verbunden werden kann, ist ein virtueller Router notwendig und lässt sich mit diesen Kommando administrieren. Hier eine kurze Liste der möglichen Befehle: . | openstack router create Erstellt einen neuen Router | openstack router delete Löscht einen bestehenden Router | openstack router add port Weist dem angegebenen Router, den angegebenen Port zu | openstack router add subnet Weist dem angegeben Router, das angegebene Subnet zu | . ",
    "url": "/optimist/guided_tour/step05/#router",
    
    "relUrl": "/optimist/guided_tour/step05/#router"
  },"80": {
    "doc": "05: Die wichtigsten Befehle des OpenStackClients",
    "title": "Subnet",
    "content": "Um den virtuellen Router korrekt zu betreiben, wird auch ein Subnet benötigt, welches mit dem Kommando openstack subnet administriert werden kann. Möglich Befehle sind: . | openstack subnet create Erstellt ein neues Subnet | openstack subnet delete Löscht ein bestehendes Subnet | openstack subnet show Zeigt alle verfügbaren Informationen zu einem Subnet an | . ",
    "url": "/optimist/guided_tour/step05/#subnet",
    
    "relUrl": "/optimist/guided_tour/step05/#subnet"
  },"81": {
    "doc": "05: Die wichtigsten Befehle des OpenStackClients",
    "title": "Port",
    "content": "Nachdem nun bereits virtuelle Router und Subnets bekannt sind, darf der Port nicht fehlen. Die wichtigsten Befehle: . | openstack port create Erstellt einen neuen Port | openstack port delete Löscht einen bestehenden Port | openstack port show Zeigt alle verfügbaren Informationen zu einem Port an | . ",
    "url": "/optimist/guided_tour/step05/#port",
    
    "relUrl": "/optimist/guided_tour/step05/#port"
  },"82": {
    "doc": "05: Die wichtigsten Befehle des OpenStackClients",
    "title": "Volume",
    "content": "Volumes sind persistente Speicherorte, die über die Existenz von einzelnen Instanzen hinaus erhalten bleiben. Wichtige Befehle sind: . | openstack volume create Erstellt ein neues Volume | openstack volume delete Löscht ein bestehendes Volume | openstack volume show Zeigt alle verfügbaren Informationen zu einem Volume an | . ",
    "url": "/optimist/guided_tour/step05/#volume",
    
    "relUrl": "/optimist/guided_tour/step05/#volume"
  },"83": {
    "doc": "05: Die wichtigsten Befehle des OpenStackClients",
    "title": "Abschluss",
    "content": "In diesem Schritt wurden die wichtigsten Befehle einmal aufgelistet und auch mit einer kleinen Erklärung versehen. Die genannten Befehle werden in den nächsten Schritten benötigti und bilden somit die Grundlage für die Guided Tour. In Schritt 6 wird das Thema ein selbst erstelltes SSH Key Pair sein. ",
    "url": "/optimist/guided_tour/step05/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step05/#abschluss"
  },"84": {
    "doc": "05: Die wichtigsten Befehle des OpenStackClients",
    "title": "05: Die wichtigsten Befehle des OpenStackClients",
    "content": " ",
    "url": "/optimist/guided_tour/step05/",
    
    "relUrl": "/optimist/guided_tour/step05/"
  },"85": {
    "doc": "06: Einen eigenen SSH-Key per Konsole erstellen und nutzen",
    "title": "Schritt 6: Einen eigenen SSH-Key per Konsole erstellen und nutzen",
    "content": " ",
    "url": "/optimist/guided_tour/step06/#schritt-6-einen-eigenen-ssh-key-per-konsole-erstellen-und-nutzen",
    
    "relUrl": "/optimist/guided_tour/step06/#schritt-6-einen-eigenen-ssh-key-per-konsole-erstellen-und-nutzen"
  },"86": {
    "doc": "06: Einen eigenen SSH-Key per Konsole erstellen und nutzen",
    "title": "Vorwort",
    "content": "Um später Zugriff auf den ersten deployten Stack per SSH zu erhalten, ist es notwendig, ein Key Pair zu erzeugen und dieses im Gegensatz zu Schritt 2 auch zu nutzen. Sollte bereits ein Keypair vorhanden sein, ist es nicht notwendig einen neuen zu erstellen. ",
    "url": "/optimist/guided_tour/step06/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step06/#vorwort"
  },"87": {
    "doc": "06: Einen eigenen SSH-Key per Konsole erstellen und nutzen",
    "title": "Installation",
    "content": "Wie in Schritt 2 erwähnt, gibt es mehrere Optionen um einen Key zu erstellen. Da wir bereits per Horizon(Dashboard) einen Key erzeugt haben, wird in diesem Schritt er direkt über einen Befehl in der Kommandozeile erstellt. $ ssh-keygen -t rsa -f Beispiel.key Generating public/private rsa key pair. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in Beispiel.key. Your public key has been saved in Beispiel.key.pub. The key fingerprint is: SHA256:UKSodmr6MFCO1fSqNYAoyM7uX8n/O5a43cPEV5vJXW8 The key's randomart image is: +---[RSA 2048]----+ | .o |+. o o o |=.+ o + |+= o ..|oo+ = S . o B|o. =... o . =E|o.+ + . + . |.= ...+.o |.oo. o++o.. | +----[SHA256]-----+ . Mit dem oben genutzten Befehl (ssh-keygen -t rsa -f Beispiel.key) werden zwei Dateien erzeugt, also das vorher genannte Keypair. Zum einen die Beispiel.key Datei und die Beispiel.key.pub, dabei ist Beispiel.key der private Teil, der nur uns bekannt sein soll und Beispiel.key.pub wird als öffentlicher Teil genutzt. ",
    "url": "/optimist/guided_tour/step06/#installation",
    
    "relUrl": "/optimist/guided_tour/step06/#installation"
  },"88": {
    "doc": "06: Einen eigenen SSH-Key per Konsole erstellen und nutzen",
    "title": "Einsatzort",
    "content": "Um den gerade erstellten Key zu nutzen, muss dieser eingebunden und für später erstellte Instanzen/Stacks bereit gestellt werden. Dies geht direkt mit dem vorher installierten OpenStackClient. In der Dokumentation gehen wir davon aus, dass der erzeugte Key in ~/.ssh/ liegt, sollte sich dieser an einer anderen Stelle befinden, muss das Keypair dorthin kopiert werden oder der Befehl entsprechend angepasst werden: . $ openstack keypair create --public-key ~/.ssh/Beispiel.key.pub Beispiel +-------------+-------------------------------------------------+ | Field | Value | +-------------+-------------------------------------------------+ | fingerprint | ec:a6:75:f9:33:4b:e0:ba:e7:bb:b6:8a:a1:5d:48:ff | name | Beispiel | user_id | 9bf501f4c3d14b7eb0f1443efe80f656 | +-------------+-------------------------------------------------+ . Da im weiteren Verlauf der SSH-Key genutzt wird, sollte der Name, der statt Beispiel vergeben wird, leicht merkbar sein. Um zu überprüfen, ob der Key korrekt abgelegt wurde oder um sich den Namen erneut anzeigen zu lassen, nutzt man folgenden Befehl: . $ openstack keypair list +----------+-------------------------------------------------+ | Name | Fingerprint | +----------+-------------------------------------------------+ | Beispiel | ec:a6:75:f9:33:4b:e0:ba:e7:bb:b6:8a:a1:5d:48:ff | +----------+-------------------------------------------------+ . ",
    "url": "/optimist/guided_tour/step06/#einsatzort",
    
    "relUrl": "/optimist/guided_tour/step06/#einsatzort"
  },"89": {
    "doc": "06: Einen eigenen SSH-Key per Konsole erstellen und nutzen",
    "title": "Abschluss",
    "content": "Da der SSH Key jetzt genutzt werden kann, wird es Zeit weiter vorzugehen und eine eigene Instanz zu erstellen. Wie das genau funktioniert, erklären wir in Schritt 7. ",
    "url": "/optimist/guided_tour/step06/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step06/#abschluss"
  },"90": {
    "doc": "06: Einen eigenen SSH-Key per Konsole erstellen und nutzen",
    "title": "06: Einen eigenen SSH-Key per Konsole erstellen und nutzen",
    "content": " ",
    "url": "/optimist/guided_tour/step06/",
    
    "relUrl": "/optimist/guided_tour/step06/"
  },"91": {
    "doc": "07: Die erste eigene Instanz",
    "title": "Schritt 7: Die erste eigene Instanz",
    "content": " ",
    "url": "/optimist/guided_tour/step07/#schritt-7-die-erste-eigene-instanz",
    
    "relUrl": "/optimist/guided_tour/step07/#schritt-7-die-erste-eigene-instanz"
  },"92": {
    "doc": "07: Die erste eigene Instanz",
    "title": "Vorwort",
    "content": "Wir wissen jetzt alles, was nötig ist, um die erste eigene Instanz anzulegen und zu starten. Es ist am sinnvollsten, das gleich in einem Stack zu organisieren und diesen mit einem Template zu beschreiben, anstatt alle notwendigen Arbeitsschritte von Hand durchzuführen. Trotzdem erzeugen wir im allerersten Schritt erst mal eine Instanz von Hand. ",
    "url": "/optimist/guided_tour/step07/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step07/#vorwort"
  },"93": {
    "doc": "07: Die erste eigene Instanz",
    "title": "Installation",
    "content": "Der Grundbefehl für das Erstellen einer Instanz in der Kommandozeile lautet: . openstack server create test . Wenn der Befehl ohne weitere Zusätze ausgeführt wird, erscheint direkt eine Fehlermeldung: . usage: openstack server create [-h] [-f {json,shell,table,value,yaml}] [-c COLUMN] [--max-width &lt;integer&gt;] [--print-empty] [--noindent] [--prefix PREFIX] (--image &lt;image&gt; | --volume &lt;volume&gt;) --flavor &lt;flavor&gt; [--security-group &lt;security-group-name&gt;] [--key-name &lt;key-name&gt;] [--property &lt;key=value&gt;] [--file &lt;dest-filename=source-filename&gt;] [--user-data &lt;user-data&gt;] [--availability-zone &lt;zone-name&gt;] [--block-device-mapping &lt;dev-name=mapping&gt;] [--nic &lt;net-id=net-uuid,v4-fixed-ip=ip-addr,v6-fixed-ip=ip-addr,port-id=port-uuid,auto,none&gt;] [--hint &lt;key=value&gt;] [--config-drive &lt;config-drive-volume&gt;|True] [--min &lt;count&gt;] [--max &lt;count&gt;] [--wait] &lt;server-name&gt; openstack server create: error: argument --flavor is required . Der Fehler besagt, dass kein Flavor angegeben ist. Damit nun eine Instanz gestartet werden kann, wird dem Befehl noch der entsprechende Flavor hinzugefügt. Um zu sehen welche Flavors bereit stehen, führen wir folgenden Befehl aus: . $ openstack flavor list +--------------------------------------+------------+-------+------+-----------+-------+-----------+ | ID | Name | RAM | Disk | Ephemeral | VCPUs | Is Public | +--------------------------------------+------------+-------+------+-----------+-------+-----------+ | 090bcc91-6207-465d-aff0-bfcc10a9e063 | m1.medium | 8192 | 20 | 0 | 4 | True | 4ade7a50-f829-4bf6-af15-266798ea8d6f | win.large | 32768 | 80 | 0 | 8 | True | 5dd72380-088e-48cd-9a18-112cb5a9cab5 | win.small | 8192 | 80 | 0 | 2 | True | 884d5b93-1467-4bc1-a445-ff7c74271cbd | m1.micro | 1024 | 20 | 0 | 1 | True | b7c4fa0b-7960-4311-a86b-507dbf58e8ac | m1.small | 4096 | 20 | 0 | 2 | True | d45e3029-8364-4e4c-beab-242e8b4622a3 | win.medium | 16384 | 80 | 0 | 4 | True | dfead62e-96a8-46e9-bdae-342ecce32d41 | win.micro | 2048 | 80 | 0 | 1 | True | ed18c320-324a-487f-88e1-3e9eb9244509 | m1.large | 16384 | 20 | 0 | 8 | True | +--------------------------------------+------------+-------+------+-----------+-------+-----------+ . Sollte man nun den Befehl openstack stack create Beispiel mit --flavor m1.micro starten, würde erneut eine Fehlermeldung angezeigt werden, da weitere Parameter fehlen. Um eine Instanz über diesen Weg zu starten, wird neben dem Flavor(--flavor) auch noch der SSH-Key (--key-name), das Image (--image), das verfügbare Netz (--network, in alten Versionen des Clients muss --nic net_id= verwendet werden) und eine SecurityGroup (--security-group) benötigt. Der SSH-Key wurde im letzten Schritt bereits erstellt und braucht so nicht erneut angelegt werden. Damit fehlen noch das Image (--image) und das Netz. Starten wir zunächst mit dem Image, wie bereits bei dem Flavor, kann auch hier eine Übersicht der möglichen Images mit folgendem Befehl angezeigt werden: . $ openstack image list +--------------------------------------+---------------------------------------+--------+ | ID | Name | Status | +--------------------------------------+---------------------------------------+--------+ | fd8ad5aa-6b33-4198-a05d-8be42fc0f20e | CentOS 7 - Latest | active | 82242d21-d990-4fc2-92a5-c7bd7820e790 | Ubuntu 16.04 Xenial Xerus - Latest | active | 8e82fd42-3d6f-44a7-9f20-92f5661823cf | Windows Server 2012 R2 Std - Latest | active | 536c086c-d2a4-43dd-80ea-a9d05ee2b97f | Windows Server 2016 Std - Latest | active | c94ced87-a03e-4eec-89f7-48f2c0ec6cd2 | debian-9.1.5-20170910-openstack-amd64 | active | b1195ddf-9336-42a7-a134-4f2e7ea57710 | iNNOVO-OPNsense-17.7.8 | active | 9134b6ed-8c5a-4a9a-907e-733dc2b5f0ef | iNNOVO_pfSense 2.3.4 | active | +--------------------------------------+---------------------------------------+--------+ . Nun fehlt noch ein Netzwerk. An dieser Stelle gibt es 2 Möglichkeiten für das Netzwerk, zum einen kann man ein sehr simples Netzwerk anlegen und so die Instanz starten, dafür nutzt man folgenden Befehl: . $ openstack network create BeispielNetzwerk +---------------------------+--------------------------------------+ | Field | Value | +---------------------------+--------------------------------------+ | admin_state_up | UP | availability_zone_hints | | availability_zones | | created_at | 2017-12-08T08:32:44Z | description | | dns_domain | None | id | a783d691-7efe-4f67-9226-99a014fa8926 | ipv4_address_scope | None | ipv6_address_scope | None | is_default | False | mtu | 1500 | name | BeispielNetzwerk | port_security_enabled | True | project_id | b15cde70d85749689e08106f973bb002 | provider:network_type | None | provider:physical_network | None | provider:segmentation_id | None | qos_policy_id | None | revision_number | 2 | router:external | Internal | segments | None | shared | False | status | ACTIVE | subnets | | updated_at | 2017-12-08T08:32:44Z | +---------------------------+--------------------------------------+ . Der Nachteil an diesem Netzwerk ist, dass man die Instanz nicht erreichen kann. Soll die Instanz nutzbar sein, wird ein funktionierendes Netz benötigt, welches in Schritt 10 komplett angelegt wird. Nachdem alle Bestandteile jetzt bekannst sind, kann die erste Instanz erstellt werden. Dafür wird das --flavor m1.small, der SSH-Key aus Schritt 6, das Netzwerk von weiter oben, das --image \"Ubuntu 16.04 Xenial Xerus - Latest\" und die --security-group default: . $ openstack server create BeispielServer --flavor m1.small --key-name Beispiel --image 82242d21-d990-4fc2-92a5-c7bd7820e790 --network=BeispielNetzwerk --security-group default +-----------------------------+--------------------------------------------------------+ | Field | Value | +-----------------------------+--------------------------------------------------------+ | OS-DCF:diskConfig | MANUAL | OS-EXT-AZ:availability_zone | es1 | OS-EXT-STS:power_state | NOSTATE | OS-EXT-STS:task_state | scheduling | OS-EXT-STS:vm_state | building | OS-SRV-USG:launched_at | None | OS-SRV-USG:terminated_at | None | accessIPv4 | | accessIPv6 | | addresses | | adminPass | 6MotuEMy4c3t | config_drive | | created | 2017-12-06T14:15:02Z | flavor | m1.small (676d2587-b5aa-49eb-998d-d91c1bd6c056) | hostId | | id | 44ff2688-4ce5-417d-962b-3a80199bf1bc | image | cirros-tempest1 (2fbe66ef-adc8-44d0-b2e2-03d95dc36936) | key_name | cg | name | BeispielServer | progress | 0 | project_id | 1e775e2cc71a461991be42d4fad8a5cb | properties | | security_groups | name='3265503b-ac24-4f60-a8d0-466b7c812916' | status | BUILD | updated | 2017-12-06T14:15:02Z | user_id | b54fda3f4d1a484797b3ad4de9b3f4f9 | volumes_attached | +-----------------------------+--------------------------------------------------------+ . Weitere mögliche Parameter für die Erstellung einer Instanz können mit --help abgefragt werden: . $ openstack server create --help usage: openstack server create [-h] [-f {json,shell,table,value,yaml}] [-c COLUMN] [--max-width &lt;integer&gt;] [--print-empty] [--noindent] [--prefix PREFIX] (--image &lt;image&gt; | --volume &lt;volume&gt;) --flavor &lt;flavor&gt; [--security-group &lt;security-group-name&gt;] [--key-name &lt;key-name&gt;] [--property &lt;key=value&gt;] [--file &lt;dest-filename=source-filename&gt;] [--user-data &lt;user-data&gt;] [--availability-zone &lt;zone-name&gt;] [--block-device-mapping &lt;dev-name=mapping&gt;] [--nic &lt;net-id=net-uuid,v4-fixed-ip=ip-addr,v6-fixed-ip=ip-addr,port-id=port-uuid,auto,none&gt;] [--hint &lt;key=value&gt;] [--config-drive &lt;config-drive-volume&gt;|True] [--min &lt;count&gt;] [--max &lt;count&gt;] [--wait] &lt;server-name&gt; Create a new server positional arguments: &lt;server-name&gt; New server name optional arguments: -h, --help show this help message and exit --image &lt;image&gt; Create server boot disk from this image (name or ID) --volume &lt;volume&gt; Create server using this volume as the boot disk (name or ID) --flavor &lt;flavor&gt; Create server with this flavor (name or ID) --security-group &lt;security-group-name&gt; Security group to assign to this server (name or ID) (repeat option to set multiple groups) --key-name &lt;key-name&gt; Keypair to inject into this server (optional extension) --property &lt;key=value&gt; Set a property on this server (repeat option to set multiple values) --file &lt;dest-filename=source-filename&gt; File to inject into image before boot (repeat option to set multiple files) --user-data &lt;user-data&gt; User data file to serve from the metadata server --availability-zone &lt;zone-name&gt; Select an availability zone for the server --block-device-mapping &lt;dev-name=mapping&gt; Map block devices; map is &lt;id&gt;:&lt;type&gt;:&lt;size(GB)&gt;:&lt;delete_on_terminate&gt; (optional extension) --nic &lt;net-id=net-uuid,v4-fixed-ip=ip-addr,v6-fixed-ip=ip-addr,port-id=port-uuid,auto,none&gt; Create a NIC on the server. Specify option multiple times to create multiple NICs. Either net-id or port- id must be provided, but not both. net-id: attach NIC to network with this UUID, port-id: attach NIC to port with this UUID, v4-fixed-ip: IPv4 fixed address for NIC (optional), v6-fixed-ip: IPv6 fixed address for NIC (optional), none: (v2.37+) no network is attached, auto: (v2.37+) the compute service will automatically allocate a network. Specifying a --nic of auto or none cannot be used with any other --nic value. --hint &lt;key=value&gt; Hints for the scheduler (optional extension) --config-drive &lt;config-drive-volume&gt;|True Use specified volume as the config drive, or 'True' to use an ephemeral drive --min &lt;count&gt; Minimum number of servers to launch (default=1) --max &lt;count&gt; Maximum number of servers to launch (default=1) --wait Wait for build to complete output formatters: output formatter options -f {json,shell,table,value,yaml}, --format {json,shell,table,value,yaml} the output format, defaults to table -c COLUMN, --column COLUMN specify the column(s) to include, can be repeated table formatter: --max-width &lt;integer&gt; Maximum display width, &lt;1 to disable. You can also use the CLIFF_MAX_TERM_WIDTH environment variable, but the parameter takes precedence. --print-empty Print empty table if there is no data to show. json formatter: --noindent whether to disable indenting the JSON shell formatter: a format a UNIX shell can parse (variable=\"value\") --prefix PREFIX add a prefix to all variable names . ",
    "url": "/optimist/guided_tour/step07/#installation",
    
    "relUrl": "/optimist/guided_tour/step07/#installation"
  },"94": {
    "doc": "07: Die erste eigene Instanz",
    "title": "Abschluss",
    "content": "Nachdem wir in diesem Schritt nicht nur eine neue Instanz erstellt , sondern auch noch einige Basis Befehle für OpenStack angewedet haben. Werden wir im Schritt 8 diese VM wieder löschen. ",
    "url": "/optimist/guided_tour/step07/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step07/#abschluss"
  },"95": {
    "doc": "07: Die erste eigene Instanz",
    "title": "07: Die erste eigene Instanz",
    "content": " ",
    "url": "/optimist/guided_tour/step07/",
    
    "relUrl": "/optimist/guided_tour/step07/"
  },"96": {
    "doc": "08: Löschen der ersten eigenen Instanz",
    "title": "Schritt 8: Löschen der ersten eigenen Instanz",
    "content": " ",
    "url": "/optimist/guided_tour/step08/#schritt-8-l%C3%B6schen-der-ersten-eigenen-instanz",
    
    "relUrl": "/optimist/guided_tour/step08/#schritt-8-löschen-der-ersten-eigenen-instanz"
  },"97": {
    "doc": "08: Löschen der ersten eigenen Instanz",
    "title": "Vorwort",
    "content": "Nachdem in Schritt 7: Die erste eigene Instanz die Instanz in der Kommandozeile angelegt wurde, wird in diesem Schritt erklärt, wie man eine Instanz wieder löscht. ",
    "url": "/optimist/guided_tour/step08/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step08/#vorwort"
  },"98": {
    "doc": "08: Löschen der ersten eigenen Instanz",
    "title": "Vorgehen",
    "content": "Damit eine Instanz generell gelöscht werden kann, wird entweder der Name oder die ID der zu löschenden Instanz benötigt. Bei wenigen Instanzen in einem Stack, kann der Name für das Löschen verwendet werden. Sobald allerdings mehrere Instanzen verwendet werden, wird von uns empfohlen für das Löschen die ID zu nutzen, da Namen im Gegensatz zu IDs nicht einzigartig sind. Der OpenStackClient zeigt einem sonst an, dass es mehrere Instanzen mit dem entsprechenden Namen gibt. Um nun eine Liste aller verfügbaren Instanzen zu erhalten, kann openstack server list als Befehl ausgeführt werden: . $ openstack server list +--------------------------------------+--------------+--------+---------------------------------------------------+------------------------------------+ | ID | Name | Status | Networks | Image Name | +--------------------------------------+--------------+--------+---------------------------------------------------+------------------------------------+ | 801b3021-0c00-4566-881e-b50d47152e63 | singleserver | ACTIVE | single_internal_network=10.0.0.12, 185.116.245.39 | Ubuntu 16.04 Xenial Xerus - Latest | +--------------------------------------+--------------+--------+---------------------------------------------------+------------------------------------+ . Die angezeigte Liste listet alle verfügbaren Instanzen auf und enthält neben dem Namen der jeweiligen Instanz, auch die zugehörige ID. Um die im vorigen Schritt erstelle Instanz zu löschen, wird der Befehl openstack server delete ID verwendet, wobei “ID” durch die korrekte ID der Instanz ausgetauscht wird. In unserem Beispiel lautet der Befehl also wie folgt: . openstack server delete 801b3021-0c00-4566-881e-b50d47152e63 . Bei einer erneuten Ausgabe von openstack server list, sollte kein Server mehr angezeigt werden: . $ openstack server list $ . ",
    "url": "/optimist/guided_tour/step08/#vorgehen",
    
    "relUrl": "/optimist/guided_tour/step08/#vorgehen"
  },"99": {
    "doc": "08: Löschen der ersten eigenen Instanz",
    "title": "Abschluss",
    "content": "Nachdem im vorigen Schritt eine Instanz per Hand erstellt wurde, haben wir diese Instanz hier gelöscht. Außerdem konnte mit dem Befehl openstack server list eine Übersicht über alle Instanzen gewonnen werden. In Schritt 9: Die erste Security-Group wird an den bisherigen Erfahrungen angeknüpft und das gewonnene Wissen um das Thema Security-Groups erweitert. ",
    "url": "/optimist/guided_tour/step08/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step08/#abschluss"
  },"100": {
    "doc": "08: Löschen der ersten eigenen Instanz",
    "title": "08: Löschen der ersten eigenen Instanz",
    "content": " ",
    "url": "/optimist/guided_tour/step08/",
    
    "relUrl": "/optimist/guided_tour/step08/"
  },"101": {
    "doc": "09: Die erste Security-Group",
    "title": "Schritt 9: Die erste Security-Group",
    "content": " ",
    "url": "/optimist/guided_tour/step09/#schritt-9-die-erste-security-group",
    
    "relUrl": "/optimist/guided_tour/step09/#schritt-9-die-erste-security-group"
  },"102": {
    "doc": "09: Die erste Security-Group",
    "title": "Vorwort",
    "content": "Standardmäßig ist jeglicher Zugriff auf eine Instanz von außerhalb verboten. Um Zugriff auf eine Instanz zu erlauben, muss (mindestens) eine Security Group definiert und der Instanz zugewiesen werden. Es ist möglich, alle Zugriffsregeln in einer Security Group zusammenzufassen, doch für komplexe Stacks macht es Sinn, die Regeln nach Aufgabe einzelner Instanzen in eigenen Security Groups zu hinterlegen. ",
    "url": "/optimist/guided_tour/step09/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step09/#vorwort"
  },"103": {
    "doc": "09: Die erste Security-Group",
    "title": "Vorgehen",
    "content": "Der Grundbefehl für das erstellen einer Security Group lautet openstack security group create allow-ssh-from-anywhere --description Beispiel: . $ openstack security group create allow-ssh-from-anywhere --description Beispiel +-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+ | created_at | 2017-12-08T12:01:42Z | description | Beispiel | id | 1cab4a62-0fda-40d9-bac8-fd73275b472d | name | allow-ssh-from-anywhere | project_id | b15cde70d85749689e08106f973bb002 | revision_number | 2 | rules | created_at='2017-12-08T12:01:42Z', direction='egress', ethertype='IPv6', id='5a852e4b-1d79-4fe9-b359-64ca54c98501', | | updated_at='2017-12-08T12:01:42Z' | | created_at='2017-12-08T12:01:42Z', direction='egress', ethertype='IPv4', id='fa90a1ee-d3b9-40d4-9bb5-89fdd5005c02', | | updated_at='2017-12-08T12:01:42Z' | updated_at | 2017-12-08T12:01:42Z | +-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+ . Damit eine Security Group nicht nur eine leere Hülle ist, kann der Befehl durch weitere Zusätze sinnvoll erweitert werden. Hier eine kurze Übersicht der wichtigsten Optionen: . | --protocol = Definition des genutzten Protokolls (mögliche Optionen: icmp, tcp, udp) | --dst-port = Gibt den Port oder die Range der Ports an. (22:22 ist Port 22, 1:[65535 würde alle Ports definieren)]{style=”color: rgb(34,34,34);”} | --remote-ip = Kann eine IP oder IP-Range definieren. (Default um den Zugang über alle IPs zu gewähren ist 0.0.0.0/0 | --ingress bzw. --egress = ingress definiert den eingehenden Verkehr, egress den ausgehenden | . Da die wichtigsten Optionen nun bekannt sind, kann jetzt eine Security Group erstellt werden, die es erlaubt theoretisch Zugriff per SSH zu erhalten. Der Befehl lautet openstack security group rule create allow-ssh-from-anywhere --protocol tcp --dst-port 22:22 --remote-ip 0.0.0.0/0 . $ openstack security group rule create allow-ssh-from-anywhere --protocol tcp --dst-port 22:22 --remote-ip 0.0.0.0/0 +-------------------+--------------------------------------+ | Field | Value | +-------------------+--------------------------------------+ | created_at | 2017-12-08T12:02:15Z | description | | direction | ingress | ether_type | IPv4 | id | 694a0573-b4c3-423c-847d-550f79e83f2b | name | None | port_range_max | 22 | port_range_min | 22 | project_id | b15cde70d85749689e08106f973bb002 | protocol | tcp | remote_group_id | None | remote_ip_prefix | 0.0.0.0/0 | revision_number | 0 | security_group_id | 1cab4a62-0fda-40d9-bac8-fd73275b472d | updated_at | 2017-12-08T12:02:15Z | +-------------------+--------------------------------------+ . Um zu prüfen, ob die Security Group korrekt angelegt wurde und um eine Übersicht über alle zu erhalten, kann folgender Befehl genutzt werden, openstack security group show allow-ssh-from-anywhere . $ openstack security group show allow-ssh-from-anywhere +-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+ | created_at | 2017-12-08T12:01:42Z | description | Beispiel | id | 1cab4a62-0fda-40d9-bac8-fd73275b472d | name | allow-ssh-from-anywhere | project_id | b15cde70d85749689e08106f973bb002 | revision_number | 3 | rules | created_at='2017-12-08T12:01:42Z', direction='egress', ethertype='IPv6', id='5a852e4b-1d79-4fe9-b359-64ca54c98501', | | updated_at='2017-12-08T12:01:42Z' | | created_at='2017-12-08T12:02:15Z', direction='ingress', ethertype='IPv4', id='694a0573-b4c3-423c-847d-550f79e83f2b', port_range_max='22', | | port_range_min='22', protocol='tcp', remote_ip_prefix='0.0.0.0/0', updated_at='2017-12-08T12:02:15Z' | | created_at='2017-12-08T12:01:42Z', direction='egress', ethertype='IPv4', id='fa90a1ee-d3b9-40d4-9bb5-89fdd5005c02', | | updated_at='2017-12-08T12:01:42Z' | updated_at | 2017-12-08T12:02:15Z | +-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+ . ",
    "url": "/optimist/guided_tour/step09/#vorgehen",
    
    "relUrl": "/optimist/guided_tour/step09/#vorgehen"
  },"104": {
    "doc": "09: Die erste Security-Group",
    "title": "Abschluss",
    "content": "Nach dem erfolgreichen erstellen der Security-Group, ist der nächste Schritt ein Netzwerk hinzuzufügen. Dies erfolgt im Schritt 10: Zugriff aus dem Internet vorbereiten: Ein Netzwerk anlegen. ",
    "url": "/optimist/guided_tour/step09/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step09/#abschluss"
  },"105": {
    "doc": "09: Die erste Security-Group",
    "title": "09: Die erste Security-Group",
    "content": " ",
    "url": "/optimist/guided_tour/step09/",
    
    "relUrl": "/optimist/guided_tour/step09/"
  },"106": {
    "doc": "10: Zugriff aus dem Internet vorbereiten; Ein Netzwerk anlegen",
    "title": "Schritt 10: Zugriff aus dem Internet vorbereiten: Ein Netzwerk anlegen",
    "content": " ",
    "url": "/optimist/guided_tour/step10/#schritt-10-zugriff-aus-dem-internet-vorbereiten-ein-netzwerk-anlegen",
    
    "relUrl": "/optimist/guided_tour/step10/#schritt-10-zugriff-aus-dem-internet-vorbereiten-ein-netzwerk-anlegen"
  },"107": {
    "doc": "10: Zugriff aus dem Internet vorbereiten; Ein Netzwerk anlegen",
    "title": "Vorwort",
    "content": "In Schritt 7 wurde zuerst eine Instanz manuell erstellt und in Schritt 9 dann eine Security Group. Nun ist der nächste Schritt ein virtuelles Netzwerk zu erstellen. ",
    "url": "/optimist/guided_tour/step10/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step10/#vorwort"
  },"108": {
    "doc": "10: Zugriff aus dem Internet vorbereiten; Ein Netzwerk anlegen",
    "title": "Netzwerk",
    "content": "Den Start dafür macht das eigentliche Netzwerk. Wie bisher gibt es mehrere zusätzliche Optionen, die wie gewohnt mit dem Zusatz --help aufgelistet werden können. Um das Netzwerk zu erstellen, nutzen wir den Befehl: . $ openstack network create BeispielNetzwerk +---------------------------+--------------------------------------+ | Field | Value | +---------------------------+--------------------------------------+ | admin_state_up | UP | availability_zone_hints | | availability_zones | | created_at | 2017-12-08T12:06:38Z | description | | dns_domain | None | id | ff6d8654-66d6-4881-9528-2686bddcb6dc | ipv4_address_scope | None | ipv6_address_scope | None | is_default | False | mtu | 1500 | name | BeispielNetzwerk | port_security_enabled | True | project_id | b15cde70d85749689e08106f973bb002 | provider:network_type | None | provider:physical_network | None | provider:segmentation_id | None | qos_policy_id | None | revision_number | 2 | router:external | Internal | segments | None | shared | False | status | ACTIVE | subnets | | updated_at | 2017-12-08T12:06:38Z | +---------------------------+--------------------------------------+ . ",
    "url": "/optimist/guided_tour/step10/#netzwerk",
    
    "relUrl": "/optimist/guided_tour/step10/#netzwerk"
  },"109": {
    "doc": "10: Zugriff aus dem Internet vorbereiten; Ein Netzwerk anlegen",
    "title": "Subnet",
    "content": "Da das Netzwerk nun angelegt wurde, ist der nächste logische Schritt, ein zugehöriges Subnet. Auch das Subnet hat sehr viele zusätzliche Optionen, für das Beispiel werden folgende genutzt: . | --network = Gibt an, in welchem Netzwerk das Subnet angelegt werden soll | --subnet-range = CIDR des Subnets. Im Beispiel wird 192.168.2.0/24 verwendet | . Um das Subnet in das vorher erstellte Netzwerk zu integrieren und die CIDR zu definieren lautet der korrekte Befehl: . $ openstack subnet create BeispielSubnet --network BeispielNetzwerk --subnet-range 192.168.2.0/24 +-------------------------+--------------------------------------+ | Field | Value | +-------------------------+--------------------------------------+ | allocation_pools | 192.168.2.2-192.168.2.254 | cidr | 192.168.2.0/24 | created_at | 2017-12-08T12:09:07Z | description | | dns_nameservers | | enable_dhcp | True | gateway_ip | 192.168.2.1 | host_routes | | id | 984b24bf-db60-46a9-83c3-d68f6f1062e4 | ip_version | 4 | ipv6_address_mode | None | ipv6_ra_mode | None | name | BeispielSubnet | network_id | ff6d8654-66d6-4881-9528-2686bddcb6dc | project_id | b15cde70d85749689e08106f973bb002 | revision_number | 0 | segment_id | None | service_types | | subnetpool_id | None | updated_at | 2017-12-08T12:09:07Z | use_default_subnet_pool | None | +-------------------------+--------------------------------------+ . ",
    "url": "/optimist/guided_tour/step10/#subnet",
    
    "relUrl": "/optimist/guided_tour/step10/#subnet"
  },"110": {
    "doc": "10: Zugriff aus dem Internet vorbereiten; Ein Netzwerk anlegen",
    "title": "Router",
    "content": "Damit das Subnet auch sinnvoll genutzt werden kann, wird noch ein virtueller Router benötigt: . $ openstack router create BeispielRouter +-------------------------+--------------------------------------+ | Field | Value | +-------------------------+--------------------------------------+ | admin_state_up | UP | availability_zone_hints | | availability_zones | | created_at | 2017-12-08T12:09:49Z | description | | distributed | False | external_gateway_info | None | flavor_id | None | ha | False | id | bfb91c7f-acca-450a-aae0-c519ab563d38 | name | BeispielRouter | project_id | b15cde70d85749689e08106f973bb002 | revision_number | None | routes | | status | ACTIVE | updated_at | 2017-12-08T12:09:49Z | +-------------------------+--------------------------------------+ . Um eine Verbindung ins Internet zu ermöglichen, benötigt der Router ein externes Gateway, welches mit diesem Befehl gesetzt wird: . openstack router set BeispielRouter --external-gateway provider . Da nun schon die Verbindung hergestellt ist, wird dem Router nun noch das Subnet zugewiesen: . openstack router add subnet BeispielRouter BeispielSubnet . ",
    "url": "/optimist/guided_tour/step10/#router",
    
    "relUrl": "/optimist/guided_tour/step10/#router"
  },"111": {
    "doc": "10: Zugriff aus dem Internet vorbereiten; Ein Netzwerk anlegen",
    "title": "Port",
    "content": "Nachdem nun bereits der Router und das Subnet erstellt wurden, fehlt im letzten Schritt noch der zugehörige Port. Bei der Erstellung wird mit --network definiert, in welchem Netzwerk der Port verwendet werden soll: . $ openstack port create BeispielPort --network BeispielNetzwerk +-----------------------+----------------------------------------------------------------------------+ | Field | Value | +-----------------------+----------------------------------------------------------------------------+ | admin_state_up | UP | allowed_address_pairs | | binding_host_id | None | binding_profile | None | binding_vif_details | None | binding_vif_type | None | binding_vnic_type | normal | created_at | 2017-12-08T12:12:13Z | description | | device_id | | device_owner | | dns_assignment | None | dns_name | None | extra_dhcp_opts | | fixed_ips | ip_address='192.168.2.8', subnet_id='984b24bf-db60-46a9-83c3-d68f6f1062e4' | id | 31777c0a-a952-43ca-bb7f-11ad33926dae | ip_address | None | mac_address | fa:16:3e:09:88:c8 | name | BeispielPort | network_id | ff6d8654-66d6-4881-9528-2686bddcb6dc | option_name | None | option_value | None | port_security_enabled | True | project_id | b15cde70d85749689e08106f973bb002 | qos_policy_id | None | revision_number | 3 | security_group_ids | 3d3e3074-3087-4965-9a64-34a6d56193b9 | status | DOWN | subnet_id | None | updated_at | 2017-12-08T12:12:13Z | +-----------------------+----------------------------------------------------------------------------+ . ",
    "url": "/optimist/guided_tour/step10/#port",
    
    "relUrl": "/optimist/guided_tour/step10/#port"
  },"112": {
    "doc": "10: Zugriff aus dem Internet vorbereiten; Ein Netzwerk anlegen",
    "title": "Abschluss",
    "content": "Nachdem Router, Subnet und Port angelegt und diese miteinander verknüpft wurden, ist die Einrichtung des Beispielnetzwerks abgeschlossen und im nächsten Schritt fügen wir noch den Zugriff per IPv6 hinzu. ",
    "url": "/optimist/guided_tour/step10/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step10/#abschluss"
  },"113": {
    "doc": "10: Zugriff aus dem Internet vorbereiten; Ein Netzwerk anlegen",
    "title": "10: Zugriff aus dem Internet vorbereiten; Ein Netzwerk anlegen",
    "content": " ",
    "url": "/optimist/guided_tour/step10/",
    
    "relUrl": "/optimist/guided_tour/step10/"
  },"114": {
    "doc": "11: Zugriff aus dem Internet vorbereiten; Wir ergänzen IPv6",
    "title": "Schritt 11: Zugriff aus dem Internet vorbereiten: Wir ergänzen IPv6",
    "content": " ",
    "url": "/optimist/guided_tour/step11/#schritt-11-zugriff-aus-dem-internet-vorbereiten-wir-erg%C3%A4nzen-ipv6",
    
    "relUrl": "/optimist/guided_tour/step11/#schritt-11-zugriff-aus-dem-internet-vorbereiten-wir-ergänzen-ipv6"
  },"115": {
    "doc": "11: Zugriff aus dem Internet vorbereiten; Wir ergänzen IPv6",
    "title": "Vorwort",
    "content": "In Schritt 10 wurde von uns bereits ein Netzwerk angelegt und in diesem Schritt erweitern wir selbiges um IPv6. Dabei nutzen wir die bereits bestehenden Router etc. Wichtig ist, dass die IPv4-Adresse auf dem ersten Interface läuft. Die Cloud Images sind so konzipiert, dass das primäre Interface mit DHCP vorkonfiguriert ist. Erst wenn das erfolgt ist, wird auf den Metadata Service zugegriffen um IPv6 überhaupt hoch zu fahren. ",
    "url": "/optimist/guided_tour/step11/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step11/#vorwort"
  },"116": {
    "doc": "11: Zugriff aus dem Internet vorbereiten; Wir ergänzen IPv6",
    "title": "Subnet",
    "content": "Für die IPv6 Netze gibt es bereits einen Pool, aus dem man sich einfach ein eigenes Subnetz generieren lassen kann. Welche Pools es gibt, findet man mit diesem Befehl heraus: . $ openstack subnet pool list +--------------------------------------+---------------+---------------------+ | ID | Name | Prefixes | +--------------------------------------+---------------+---------------------+ | f541f3b6-af22-435a-9cbb-b233d12e74f4 | customer-ipv6 | 2a00:c320:1000::/48 | +--------------------------------------+---------------+---------------------+ . Aus diesem Pool kann man sich nun eigene Subnetze generieren lassen und die Prefixlänge von 64Bit ist dabei pro generiertem Subnet fest vorgegeben. Bei der Erstellung der Pools kann man die Subnets direkt mit angeben oder man überlässt es OpenStack. Dafür wird im Befehl openstack subnet create --network BeispielNetzwerk --ip-version 6 --use-default-subnet-pool --ipv6-address-mode dhcpv6-stateful --ipv6-ra-mode dhcpv6-stateful BeispielSubnetIPv6 einfach der Zusatz --use-default-subnet-pool genutzt. $ openstack subnet create --network BeispielNetzwerk --ip-version 6 --use-default-subnet-pool --ipv6-address-mode dhcpv6-stateful --ipv6-ra-mode dhcpv6-stateful BeispielSubnetIPv6 +-------------------------+----------------------------------------------------------+ | Field | Value | +-------------------------+----------------------------------------------------------+ | allocation_pools | 2a00:c320:1000:2::2-2a00:c320:1000:2:ffff:ffff:ffff:ffff | cidr | 2a00:c320:1000:2::/64 | created_at | 2017-12-08T12:41:42Z | description | | dns_nameservers | | enable_dhcp | True | gateway_ip | 2a00:c320:1000:2::1 | host_routes | | id | 0046c29b-a9b0-47c3-b5dd-704aa801704d | ip_version | 6 | ipv6_address_mode | dhcpv6-stateful | ipv6_ra_mode | dhcpv6-stateful | name | BeispielSubnetIPv6 | network_id | ff6d8654-66d6-4881-9528-2686bddcb6dc | project_id | b15cde70d85749689e08106f973bb002 | revision_number | 0 | segment_id | None | service_types | | subnetpool_id | f541f3b6-af22-435a-9cbb-b233d12e74f4 | updated_at | 2017-12-08T12:41:42Z | use_default_subnet_pool | True | +-------------------------+----------------------------------------------------------+ . ",
    "url": "/optimist/guided_tour/step11/#subnet",
    
    "relUrl": "/optimist/guided_tour/step11/#subnet"
  },"117": {
    "doc": "11: Zugriff aus dem Internet vorbereiten; Wir ergänzen IPv6",
    "title": "Router",
    "content": "Da nun das IPv6 Netz auch erstellt ist, werden wir in diesem Schritt das neue Netz mit dem in Schritt 10 erstellten Router verbinden. Dafür nutzen wir den Befehl: . openstack router add subnet BeispielRouter BeispielSubnetIPv6 . ",
    "url": "/optimist/guided_tour/step11/#router",
    
    "relUrl": "/optimist/guided_tour/step11/#router"
  },"118": {
    "doc": "11: Zugriff aus dem Internet vorbereiten; Wir ergänzen IPv6",
    "title": "Security Group",
    "content": "Die Regeln die wir zuvor in Schritt 9 angelegt haben, beziehen sich nur auf IPv4. Damit auch IPv6 genutzt werden kann, legen wir noch zwei weitere Regeln in den schon bestehenden SecurityGroups an. Um auch per IPv6 Zugriff per SSH auf die VM zu erlangen nutzen wir den Befehl: . $ openstack security group rule create --remote-ip \"::/0\" --protocol tcp --dst-port 22:22 --ethertype IPv6 --ingress allow-ssh-from-anywhere +-------------------+--------------------------------------+ | Field | Value | +-------------------+--------------------------------------+ | created_at | 2017-12-08T12:44:04Z | description | | direction | ingress | ether_type | IPv6 | id | 7d871e85-05fa-4620-b558-c6fc64076cde | name | None | port_range_max | 22 | port_range_min | 22 | project_id | b15cde70d85749689e08106f973bb002 | protocol | tcp | remote_group_id | None | remote_ip_prefix | ::/0 | revision_number | 0 | security_group_id | 1cab4a62-0fda-40d9-bac8-fd73275b472d | updated_at | 2017-12-08T12:44:04Z | +-------------------+--------------------------------------+ . Nun fehlt noch der Zugriff per ICMP, damit wir die VM auch über IPv6 per Ping erreichen können. Dies geht mit diesem Befehl: . $ openstack security group rule create --remote-ip \"::/0\" --protocol ipv6-icmp --ingress allow-ssh-from-anywhere +-------------------+--------------------------------------+ | Field | Value | +-------------------+--------------------------------------+ | created_at | 2017-12-08T12:44:44Z | description | | direction | ingress | ether_type | IPv6 | id | f63e4787-9965-4732-b9d2-20ce0fedc974 | name | None | port_range_max | None | port_range_min | None | project_id | b15cde70d85749689e08106f973bb002 | protocol | ipv6-icmp | remote_group_id | None | remote_ip_prefix | ::/0 | revision_number | 0 | security_group_id | 1cab4a62-0fda-40d9-bac8-fd73275b472d | updated_at | 2017-12-08T12:44:44Z | +-------------------+--------------------------------------+ . ",
    "url": "/optimist/guided_tour/step11/#security-group",
    
    "relUrl": "/optimist/guided_tour/step11/#security-group"
  },"119": {
    "doc": "11: Zugriff aus dem Internet vorbereiten; Wir ergänzen IPv6",
    "title": "Anpassungen am Betriebssystem",
    "content": "Startet man nun eine VM innerhalb des angelegten Netzes, bekommt diese eine IPv4, als auch eine IPv6 Adresse. Die Standardimages der Hersteller sind aber leider noch nicht für IPv6 vorkonfiguriert, weshalb tatsächlich nur die IPv4 Adresse in der VM ankommt. Nutzt man unsere bereitgestellten Heat Templates, sind die notwendigen Anpassungen bereits im Template enthalten. Um dies auch bei bestehenden Instanzen nachträglich auch zu ermöglichen, gibt es hier für verschiedene Distributionen einen Anleitung. Ubuntu 16.04 . Um IPv6 korrekt nutzen zu können, müssen folgende Dateien, mit dem angegeben Inhalt erstellt werden. | /etc/dhcp/dhclient6.conf . timeout 30; . | /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg . network: {config: disabled} . | /etc/network/interfaces.d/lo.cfg . auto lo iface lo inet loopback . | /etc/network/interfaces.d/ens3.cfg . iface ens3 inet6 auto up sleep 5 up dhclient -1 -6 -cf /etc/dhcp/dhclient6.conf -lf /var/lib/dhcp/dhclient6.ens3.leases -v ens3 || true . | . Im Anschluss wird das entsprechende Interface neugestartet: . sudo ifdown ens3 &amp;&amp; sudo ifup ens3 . Die VM hat jetzt eine weitere IPv6 Adresse auf dem Interface, auf dem vorher nur die IPv4 Adresse existierte und kann somit auch per IPv6 korrekt erreicht werden. Damit man die beschriebenen Punkte nicht jedes mal manuell abarbeiten muss, kann man folgende cloud-init Konfiguration verwenden (Was cloud-init genau ist, erklären wir in Schritt 19: Unsere Instanz lernt IPv6): . #cloud-config write_files: - path: /etc/dhcp/dhclient6.conf content: \"timeout 30;\" - path: /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg content: \"network: {config: disabled}\" - path: /etc/network/interfaces.d/lo.cfg content: | auto lo iface lo inet loopback - path: /etc/network/interfaces.d/ens3.cfg content: | iface ens3 inet6 auto up sleep 5 up dhclient -1 -6 -cf /etc/dhcp/dhclient6.conf -lf /var/lib/dhcp/dhclient6.ens3.leases -v ens3 || true runcmd: - [ ifdown, ens3] - [ ifup, ens3] . CentOS 7 . Die genannten Parameter müssen den angegebenen Dateien neu hinzugefügt oder falls diese bereits vorhanden sind ergänzt werden: . | /etc/sysconfig/network . NETWORKING_IPV6=yes . | /etc/sysconfig/network-scripts/ifcfg-eth0 . IPV6INIT=yes DHCPV6C=yes . | . Anschließend wird das entsprechende Interface neugestartet: . sudo ifdown eth0 &amp;&amp; sudo ifup eth0 . Die VM hat jetzt eine weitere IPv6 Adresse auf dem Interface, auf dem vorher nur die IPv4 Adresse existierte und kann somit auch per IPv6 korrekt erreicht werden. Damit man die beschriebenen Punkte nicht jedes mal manuell abarbeiten muss, kann man folgende cloud-init Konfiguration verwenden(Was cloud-init genau ist, erklären wir für Ubuntu 16.04 in Schritt 19: Unsere Instanz lernt IPv6: . #cloud-config write_files: - path: /etc/sysconfig/network owner: root:root permissions: '0644' content: | NETWORKING=yes NOZEROCONF=yes NETWORKING_IPV6=yes - path: /etc/sysconfig/network-scripts/ifcfg-eth0 owner: root:root permissions: '0644' content: | DEVICE=\"eth0\" BOOTPROTO=\"dhcp\" ONBOOT=\"yes\" TYPE=\"Ethernet\" USERCTL=\"yes\" PEERDNS=\"yes\" PERSISTENT_DHCLIENT=\"1\" IPV6INIT=yes DHCPV6C=yes runcmd: - [ ifdown, eth0] - [ ifup, eth0] . ",
    "url": "/optimist/guided_tour/step11/#anpassungen-am-betriebssystem",
    
    "relUrl": "/optimist/guided_tour/step11/#anpassungen-am-betriebssystem"
  },"120": {
    "doc": "11: Zugriff aus dem Internet vorbereiten; Wir ergänzen IPv6",
    "title": "Externer Zugriff",
    "content": "Wichtig: Diese VM ist ab sofort von überall auf der Welt über ihre IPv6 Adresse zu erreichen. Natürlich nur auf den Ports, die wir auch in den Security Groups aktiviert haben. Wir benötigen also keine weitere Floating IP um externen Zugriff auf diese VM zu ermöglichen. Es ist deshalb so wichtig zu erwähnen, da wir hier ein anderes Verhalten haben, als mit den IPv4 Adressen. Möchte man per IPv4 aus dem Internet auf diese VM zugreifen, muss man weiterhin auf die Floating IPs zurückgreifen. Hat man selbst lokal kein IPv6, möchte aber testen ob seine VM prinzipiell erreichbar ist, kann man auf Online Tools zurückgreifen, wie z.B. https://www.subnetonline.com/pages/ipv6-network-tools/online-ipv6-ping.php . ",
    "url": "/optimist/guided_tour/step11/#externer-zugriff",
    
    "relUrl": "/optimist/guided_tour/step11/#externer-zugriff"
  },"121": {
    "doc": "11: Zugriff aus dem Internet vorbereiten; Wir ergänzen IPv6",
    "title": "Abschluss",
    "content": "Nachdem im letzten Schritt bereits eine Verbindung per IPv4 erfolgte, wurde nun auch noch der Zugriff per IPv6 hinzugefügt. Im nächsten Schritt wird dann die Instanz aus Schritt 7 als Vorlage genutzt und erreichbar von außen. ",
    "url": "/optimist/guided_tour/step11/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step11/#abschluss"
  },"122": {
    "doc": "11: Zugriff aus dem Internet vorbereiten; Wir ergänzen IPv6",
    "title": "11: Zugriff aus dem Internet vorbereiten; Wir ergänzen IPv6",
    "content": " ",
    "url": "/optimist/guided_tour/step11/",
    
    "relUrl": "/optimist/guided_tour/step11/"
  },"123": {
    "doc": "12: Eine nutzbare Instanz",
    "title": "Schritt 12: Eine nutzbare Instanz",
    "content": " ",
    "url": "/optimist/guided_tour/step12/#schritt-12-eine-nutzbare-instanz",
    
    "relUrl": "/optimist/guided_tour/step12/#schritt-12-eine-nutzbare-instanz"
  },"124": {
    "doc": "12: Eine nutzbare Instanz",
    "title": "Vorwort",
    "content": "In Schritt 7 wurde bereits eine Instanz erstellt, diese konnte jedoch nur genutzt werden, wenn man ein paar Schritte übersprungen hat und das entsprechende Netzwerk mit erstellt. Es gab nur so die Möglichkeit eine Verbindung zu dieser herzustellen. Daher werden wir in diesem Schritt eine Instanz erstellen, die diese Problematik nicht mehr hat. ",
    "url": "/optimist/guided_tour/step12/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step12/#vorwort"
  },"125": {
    "doc": "12: Eine nutzbare Instanz",
    "title": "Installation",
    "content": "Damit die Instanz all die fehlenden Einstellungen enthält, wird der Befehl aus Schritt 7 modifiziert: . openstack server create BeispielInstanz --flavor m1.small --key-name Beispiel --image \"Ubuntu 16.04 Xenial Xerus - Latest\" --security-group allow-ssh-from-anywhere --network=BeispielNetzwerk . $ openstack server create BeispielInstanz --flavor m1.small --key-name Beispiel --image \"Ubuntu 16.04 Xenial Xerus - Latest\" --security-group allow-ssh-from-anywhere --network=BeispielNetzwerk +-----------------------------+---------------------------------------------------------------------------+ | Field | Value | +-----------------------------+---------------------------------------------------------------------------+ | OS-DCF:diskConfig | MANUAL | OS-EXT-AZ:availability_zone | es1 | OS-EXT-STS:power_state | NOSTATE | OS-EXT-STS:task_state | scheduling | OS-EXT-STS:vm_state | building | OS-SRV-USG:launched_at | None | OS-SRV-USG:terminated_at | None | accessIPv4 | | accessIPv6 | | addresses | | adminPass | jkSdvP3A9yo6 | config_drive | | created | 2017-12-08T12:52:37Z | flavor | m1.small (b7c4fa0b-7960-4311-a86b-507dbf58e8ac) | hostId | | id | 1de98aa4-7d2b-4427-a8a5-d369ea8bdaf5 | image | Ubuntu 16.04 Xenial Xerus - Latest (82242d21-d990-4fc2-92a5-c7bd7820e790) | key_name | Beispiel | name | BeispielInstanz | progress | 0 | project_id | b15cde70d85749689e08106f973bb002 | properties | | security_groups | name='allow-ssh-from-anywhere' | status | BUILD | updated | 2017-12-08T12:52:37Z | user_id | 9bf501f4c3d14b7eb0f1443efe80f656 | volumes_attached | +-----------------------------+---------------------------------------------------------------------------+ . Genutzt wurden die folgenden Parameter: . | --flavor = Gibt den Flavor (Größe) der Instanz an. Eine Übersicht aller verfügbaren Flavors kann mit  openstack flavor list aufgerufen werden | --key-name = Der Name des zu verwendenden SSH-Keys | --image = Gibt an welches Image für die Instanz genutzt wird. Auch ist es möglich, sich im Vorfeld eine Liste aller verfügbaren Images anzusehen \"openstack image list\" | --security-group = Gibt an, welche Security-Groups genutzt wird | --network = Mit diesem Parameter kann unter anderem das gewünscht Netzwerk angeben werden (in alten Versionen des clients --nic net-id=&lt;network&gt;) | . Damit die erstellte Instanz über das Internet erreichbar ist, wird noch eine IP benötigt, welche zuerst angelegt wird. $ openstack floating ip create provider +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | created_at | 2017-12-08T12:53:37Z | description | | fixed_ip_address | None | floating_ip_address | 185.116.245.65 | floating_network_id | 54258498-a513-47da-9369-1a644e4be692 | id | 84eca140-9ac1-42c3-baf6-860ba920a23c | name | None | port_id | None | project_id | b15cde70d85749689e08106f973bb002 | revision_number | 0 | router_id | None | status | DOWN | updated_at | 2017-12-08T12:53:37Z | +---------------------+--------------------------------------+ . Die gerade erstellte IP wird im nächsten Schritt mit der vorher erstellten Instanz verbunden. openstack server add floating ip BeispielInstanz 185.116.245.145 . ",
    "url": "/optimist/guided_tour/step12/#installation",
    
    "relUrl": "/optimist/guided_tour/step12/#installation"
  },"126": {
    "doc": "12: Eine nutzbare Instanz",
    "title": "Nutzung",
    "content": "Die erstellte Instanz ist nun erreichbar. Um zu testen, ob alle Schritte funktionieren, stellen wir nun eine Verbindung per SSH her. Wichtig ist hierbei, dass eine Verbindung nur funktioniert, wenn der weiter oben genutzte SSH Key auch existiert und verwendet wird (Siehe Schritt 6: Einen eigenen SSH-Key per Konsole erstellen und nutzen): . $ ssh ubuntu@185.116.245.145 The authenticity of host '185.116.245.145 (185.116.245.145)' can't be established. ECDSA key fingerprint is SHA256:kbSkm8eJA0748911RkbWK2/pBVQOjJBASD1oOOXalk. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added '185.116.245.145' (ECDSA) to the list of known hosts. Enter passphrase for key '/Users/ubuntu/.ssh/id_rsa': . ",
    "url": "/optimist/guided_tour/step12/#nutzung",
    
    "relUrl": "/optimist/guided_tour/step12/#nutzung"
  },"127": {
    "doc": "12: Eine nutzbare Instanz",
    "title": "Clean-Up",
    "content": "Für den Fall, dass die in den vorigen Schritten erstellten Bestandteile wieder gelöscht werden sollen, muss das in folgender Reihenfolge, mit dem entsprechenden Befehl geschehen. Sollte man dies nicht befolgen, kann es dazu führen, dass Bestandteile sich nicht löschen lassen. | Instanz . | openstack server delete BeispielInstanz | . | Floating-IP . | openstack floating ip delete 185.116.245.145 | . | Port . | openstack port delete BeispielPort | . | Router . | openstack router delete BeispielRouter | . | Subnet . | openstack subnet delete BeispielSubnet | . | Netzwerk . | openstack network delete BeispielNetzwerk | . | . ",
    "url": "/optimist/guided_tour/step12/#clean-up",
    
    "relUrl": "/optimist/guided_tour/step12/#clean-up"
  },"128": {
    "doc": "12: Eine nutzbare Instanz",
    "title": "Abschluss",
    "content": "In den Schritten 7 bis 11 wurde eine Instanz Schritt für Schritt erstellt und jeder Schritt hat einen Teilbereich hinzugefügt (inklusive Netzwerk und einer eigenen Security-Group). Im nächsten Schritt lösen wir uns von einzelnen Instanzen und erstellen einen Stack. ",
    "url": "/optimist/guided_tour/step12/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step12/#abschluss"
  },"129": {
    "doc": "12: Eine nutzbare Instanz",
    "title": "12: Eine nutzbare Instanz",
    "content": " ",
    "url": "/optimist/guided_tour/step12/",
    
    "relUrl": "/optimist/guided_tour/step12/"
  },"130": {
    "doc": "13: Der strukturierte Weg zu einer Instanz (mit Stacks)",
    "title": "Schritt 13: Der strukturierte Weg zu einer Instanz (mit Stacks)",
    "content": " ",
    "url": "/optimist/guided_tour/step13/#schritt-13-der-strukturierte-weg-zu-einer-instanz-mit-stacks",
    
    "relUrl": "/optimist/guided_tour/step13/#schritt-13-der-strukturierte-weg-zu-einer-instanz-mit-stacks"
  },"131": {
    "doc": "13: Der strukturierte Weg zu einer Instanz (mit Stacks)",
    "title": "Vorwort",
    "content": "Nachdem die erste Instanz, inklusive einer Security Group und virtuellem Netzwerk, in einem sehr aufwendigen Prozess per Hand angelegt wurde, wird in diesem Schritt eine Alternative aufgezeigt. Wie diese funktioniert und was es hierbei zu beachten gibt, wird auch wie gewohnt Schritt für Schritt erklärt. Voraussetzung für die folgenden Schritte ist die Installation des Paketes python-heatclient. Siehe Schritt 4: Der Weg vom Horizon auf die Kommandozeile. ",
    "url": "/optimist/guided_tour/step13/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step13/#vorwort"
  },"132": {
    "doc": "13: Der strukturierte Weg zu einer Instanz (mit Stacks)",
    "title": "Installation",
    "content": "Anstatt einzelne Instanzen von Hand anzulegen, kann man beliebige OpenStack Resourcen (z.B. Instanzen, Netzwerke, Router, Security Groups) auch in einem definierten Verbund, einem so genannten Stack (oder Heat Stack) betreiben. Dadurch werden sie logisch zusammengefaßt und können einfach erstellt und gelöscht werden – je nach Verwendungszweck. Wir verwenden in diesem Schritt Heat-Templates, die auch Grundlage für folgende Schritte sind. Die bisherigen Schritte 9 bis 11, lassen sich einfach in einem Template zusammenfassen. Um nicht zu theoretisch zu bleiben, gibt es ein Template unter BeispielTemplates. Dieses Template erstellt einen Stack, in diesem ist eine Instanz, zwei Security Groups, ein virtuelles Netzwerk (inkl. Router, Port, Subnet) und eine Floating-IP enthalten. Um den Stack zu erstellen, ist es notwendig, sich im Verzeichnis des Templates zu befinden und dann folgenden Befehl zu nutzen: . $ openstack stack create -t SingleServer.yaml --parameter key_name=Beispiel SingleServer --wait 2017-12-08 13:13:43Z [SingleServer]: CREATE_IN_PROGRESS Stack CREATE started 2017-12-08 13:13:44Z [SingleServer.router]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:45Z [SingleServer.enable_traffic]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:45Z [SingleServer.enable_traffic]: CREATE_COMPLETE state changed 2017-12-08 13:13:46Z [SingleServer.internal_network_id]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:46Z [SingleServer.router]: CREATE_COMPLETE state changed 2017-12-08 13:13:46Z [SingleServer.internal_network_id]: CREATE_COMPLETE state changed 2017-12-08 13:13:47Z [SingleServer.enable_ssh]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:47Z [SingleServer.subnet]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:47Z [SingleServer.enable_ssh]: CREATE_COMPLETE state changed 2017-12-08 13:13:48Z [SingleServer.start-config]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:48Z [SingleServer.subnet]: CREATE_COMPLETE state changed 2017-12-08 13:13:49Z [SingleServer.router_subnet_bridge]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:49Z [SingleServer.port]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:50Z [SingleServer.start-config]: CREATE_COMPLETE state changed 2017-12-08 13:13:50Z [SingleServer.port]: CREATE_COMPLETE state changed 2017-12-08 13:13:50Z [SingleServer.host]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:52Z [SingleServer.router_subnet_bridge]: CREATE_COMPLETE state changed 2017-12-08 13:13:53Z [SingleServer.floating_ip]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:55Z [SingleServer.floating_ip]: CREATE_COMPLETE state changed 2017-12-08 13:14:05Z [SingleServer.host]: CREATE_COMPLETE state changed 2017-12-08 13:14:06Z [SingleServer]: CREATE_COMPLETE Stack CREATE completed successfully +---------------------+-------------------------------------------------+ | Field | Value | +---------------------+-------------------------------------------------+ | id | 0f5cdf0e-24cc-4292-a0bc-adf2e9f8618a | stack_name | SingleServer | description | A simple template to deploy your first instance | creation_time | 2017-12-08T13:13:42Z | updated_time | None | stack_status | CREATE_COMPLETE | stack_status_reason | Stack CREATE completed successfully | +---------------------+-------------------------------------------------+ . Der Befehl openstack stack create erstellt dabei den Stack, mit -t SingleServer.yaml wird festgelegt, dass das angegebene Template verwendet werden soll. Außerdem wird mit --parameter key_name=BEISPIEL noch ein SSH-Schlüssel angegeben und mit SingleServer wird der Name des Stacks festgelegt. Der letzte Bestandteil --wait zeigt alle Zwischenschritte der Erstellung an. (Siehe das obere Bild) . Nach kurzer Zeit ist der Stack inkl. Instanz erstellt und kann per SSH erreicht werden. Die notwendige IP lässt sich mit folgendem Befehl herausfinden, wichtig ist dabei die korrekte ID zu nutzen: . $ openstack stack output show 0f5cdf0e-24cc-4292-a0bc-adf2e9f8618a instance_fip +--------------+---------------------------------+ | Field | Value | +--------------+---------------------------------+ | description | External IP address of instance | output_key | instance_fip | output_value | 185.116.245.70 | +--------------+---------------------------------+ . Nun stellen wir noch die Verbindung per SSH her: . $ ssh ubuntu@185.116.245.70 The authenticity of host '185.116.245.70 (185.116.245.70)' can't be established. ECDSA key fingerprint is SHA256:kbSkm8eJA0748911RkbWK2/pBVQOjJBASD1oOOXalk. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added '185.116.245.70' (ECDSA) to the list of known hosts. Enter passphrase for key '/Users/ubuntu/.ssh/id_rsa': . ",
    "url": "/optimist/guided_tour/step13/#installation",
    
    "relUrl": "/optimist/guided_tour/step13/#installation"
  },"133": {
    "doc": "13: Der strukturierte Weg zu einer Instanz (mit Stacks)",
    "title": "Abschluss",
    "content": "Wir haben die Schritte 9 bis 11 in einem Heat-Template zusammengefaßt und können sie nun leicht wiederholen. In den folgenden Schritten gehen wir weiter auf Heat ein und zeigen weiterführende Beispiele. ",
    "url": "/optimist/guided_tour/step13/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step13/#abschluss"
  },"134": {
    "doc": "13: Der strukturierte Weg zu einer Instanz (mit Stacks)",
    "title": "13: Der strukturierte Weg zu einer Instanz (mit Stacks)",
    "content": " ",
    "url": "/optimist/guided_tour/step13/",
    
    "relUrl": "/optimist/guided_tour/step13/"
  },"135": {
    "doc": "14: Unsere ersten Schritte mit Heat",
    "title": "Schritt 14: Unsere ersten Schritte mit Heat",
    "content": " ",
    "url": "/optimist/guided_tour/step14/#schritt-14-unsere-ersten-schritte-mit-heat",
    
    "relUrl": "/optimist/guided_tour/step14/#schritt-14-unsere-ersten-schritte-mit-heat"
  },"136": {
    "doc": "14: Unsere ersten Schritte mit Heat",
    "title": "Vorwort",
    "content": "Nachdem im letzten Schritt die erste Berührung mit einem Template erfolgte, ist der nächste Schritt, zu verstehen, wie Templates mit Heat aufgebaut sind und funktionieren. Dieser Schritt erklärt nur die einzelnen Punkte eines Templates und soll diese näher bringen. ",
    "url": "/optimist/guided_tour/step14/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step14/#vorwort"
  },"137": {
    "doc": "14: Unsere ersten Schritte mit Heat",
    "title": "Das Template",
    "content": "Jedes Heat-Template folgt der gleichen Struktur und diese ist wie folgt aufgebaut: . heat_template_version: 2016-10-14   description: # Die Beschreibung des Templates (optional)   parameter_groups: # Die Definition der Eingabeparameter Gruppen und deren Reihenfolge   parameters: # Die Definition der Eingabeparameter   resources: # Die Definition der Ressourcen des Templates   outputs: # Die Definition der Ausgangsparameter   conditions: # Die Definition der Bedingungen . ",
    "url": "/optimist/guided_tour/step14/#das-template",
    
    "relUrl": "/optimist/guided_tour/step14/#das-template"
  },"138": {
    "doc": "14: Unsere ersten Schritte mit Heat",
    "title": "Heat Template Version",
    "content": "Die Template Version kann tatsächlich nicht willkürlich gewählt werden, sondern hat feste Vorgaben. Diese unterscheiden sich in den möglichen Befehlen und aktuell sind folgende Daten möglich: . | 2013-05-23 | 2014-10-16 | 2015-04-30 | 2015-10-15 | 2016-04-08 | 2016-10-14 | 2017-02-24 | . ",
    "url": "/optimist/guided_tour/step14/#heat-template-version",
    
    "relUrl": "/optimist/guided_tour/step14/#heat-template-version"
  },"139": {
    "doc": "14: Unsere ersten Schritte mit Heat",
    "title": "Description",
    "content": "Die Description oder auch Beschreibung ist ein komplett optionales Feld. Das bedeutet, dass das Feld nicht genutzt werden muss. Es bietet sich allerdings an, denn damit kann das Template in seinen Grundzügen beschrieben und direkt auf mögliche Besonderheiten hingewiesen werden. Auch besteht die Möglichkeit jederzeit eine Zeile mit dem Zeichen # auszukommentieren und dadurch das Template nach den jeweiligen Bedürfnissen zu verändern oder mehr Kommentare zu verfassen. ",
    "url": "/optimist/guided_tour/step14/#description",
    
    "relUrl": "/optimist/guided_tour/step14/#description"
  },"140": {
    "doc": "14: Unsere ersten Schritte mit Heat",
    "title": "Parameter Groups",
    "content": "In diesem Bereich ist es möglich zu spezifizieren, wie Parameter gruppiert werden sollen und die Reihenfolge für die Gruppierung festzulegen. Die Gruppen sind in eine Liste aufgegliedert, welche wiederum die einzelnen Parameter enthält. Jeder Parameter sollte nur einer Gruppe zugeordnet sein, damit es später zu keinen Problemen führt. Jede Parameter Group ist dabei wie folgt aufgebaut: . parameter_groups: - label: &lt;Name der Gruppe&gt; description: &lt;Beschreibung der Gruppe&gt; parameters: - &lt;Name des Parameters&gt; - &lt;Name des Parameters&gt; . | label: Name der Gruppe. | description: Dieses Attribut gibt  die Möglichkeit die Parameter Gruppe zu beschreiben und so für jeden verständlich zu machen, wofür diese genutzt wird. | parameter: Eine Auflistung aller Parameter die für diese Parameter Gruppe gelten. | Name des Parameters: Der in der Parameter Sektion definiert wurde. | . ",
    "url": "/optimist/guided_tour/step14/#parameter-groups",
    
    "relUrl": "/optimist/guided_tour/step14/#parameter-groups"
  },"141": {
    "doc": "14: Unsere ersten Schritte mit Heat",
    "title": "Parameter",
    "content": "Diese Sektion erlaubt es die eingegebenen Parameter zu spezifizieren, welche für die Ausführung benötigt werden. Die Parameter werden typischerweise dafür genutzt, jedes deployment zu individualisieren. Dabei wird jeder Parameter in einem separaten Block definiert, wobei am Anfang immer der Parameter genannt wird und dann weitere Attribute diesem zugeordnet werden.  parameters: &lt;Parameter Name&gt;: type: &lt;string | number | json | comma_delimited_list | boolean&gt; label: &lt;Name des Parameters&gt; description: &lt;Beschreibung des Parameters&gt; default: &lt;Standardwert des Parameters&gt; hidden: &lt;true | false&gt; constraints: &lt;Vorgaben für den Parameter&gt; immutable: &lt;true | false&gt; . | Parameter Name: Der Name des Parameters. | type: Der Typ des Parameters. Unterstützte Typen: string, number, json, comma_delimited_list, boolean. | label: Name des Parameters. (optional) | description: Dieses Attribut gibt die Möglichkeit den Parameter zu beschreiben und so für jeden verständlich zu machen, wofür dieser genutzt wird. (optional) | default: Der vorgegebene Wert des Parameters. Dieser Wert wird genutzt, wenn durch den User kein spezifischer Wert festgelegt werden soll. (optional) | hidden: Gibt an ob der Parameter bei einer Abfrage nach der Erstellung angezeigt wird oder versteckt ist. (optional und per Default auf false gesetzt) | constraints: Hier kann eine Liste von Vorgaben definiert werden. Sollten diese beim deployment nicht erfüllt werden, schlägt die Erstellung des Stacks fehl. | immutable: Definiert ob der Parameter aktualisiert werden kann. Für den Fall, dass der Parameter auf true gesetzt ist und sich der Wert bei einem stack update ändert, schlägt das Update fehl. | . ",
    "url": "/optimist/guided_tour/step14/#parameter",
    
    "relUrl": "/optimist/guided_tour/step14/#parameter"
  },"142": {
    "doc": "14: Unsere ersten Schritte mit Heat",
    "title": "Resources",
    "content": "Dieser Punkt definiert die aktuell genutzten Resourcen, die bei der Erstellung des Stacks genutzt werden. Dabei wird jede Ressource in einem eigenen Block definiert: . resources: &lt;ID der Ressource&gt;: type: &lt;Ressourcen Typ&gt; properties: &lt;Name der Eigenschaft&gt;: &lt;Wert der Eigenschaft&gt; metadata: &lt;Ressourcen spezifische Metadaten&gt; depends_on: &lt;Ressourcen ID oder eine Liste der IDs&gt; update_policy: &lt;Update Regel&gt; deletion_policy: &lt;Regel für das Löschen&gt; external_id: &lt;Externe Ressourcen ID&gt; condition: &lt;Name der Kondition oder Ausdruck oder boolean&gt; . | ID der Ressource: Diese muss einzigartig im Bereich der Ressourcen sein. Es darf durchaus ein sprechender Name gewählt werden z. B. (web_network). | type: Der Typ der Ressource, wie zum Beispiel OS::NEUTRON::SecurityGroup (für eine Security Group). (benötigt) | properties: Eine Liste der Ressourcen spezifischen Eigenschaften. (optional) | metadata: Hier können für die jeweilige Ressource spezifische Metadaten hinterlegt werden. (optional) | depends_on: Hier können verschiedene Abhängigkeiten zu anderen Ressourcen hinterlegt werden. (optional) | update_policy: Hier können Update Regeln festgelegt werden. Die Voraussetzung dafür ist, dass die entsprechende Resource dies auch unterstützt. (optional) | deletion_policy: Hier werden die Regeln für das Löschen festgelegt. Erlaubt sind Delete, Retain und Snapshot. Mit der heat_template_version 2016-10-14 ist nun auch die Kleinschreibung der Werte erlaubt . | external_id: Falls notwendig, können externe Ressourcen IDs verwendet werden | condition: Anhand der Kondition wird entschieden, ob die Ressource erstellt wird oder nicht. (optional) | . ",
    "url": "/optimist/guided_tour/step14/#resources",
    
    "relUrl": "/optimist/guided_tour/step14/#resources"
  },"143": {
    "doc": "14: Unsere ersten Schritte mit Heat",
    "title": "Output",
    "content": "Die Output-Sektion definiert die ausgegeben Parameter die für den User oder auch für andere Templates nach dem Erstellen des Stacks verfügbar sind. Das können zum Beispiel Parameter wie die IP-Adresse der erstellten Instanz oder auch die URL der erstellten Web-App sein. Auch wird jeder Parameter in einem eigenen Block definiert: . outputs: &lt;Name des Parameters&gt;: description: &lt;Beschreibung&gt; value: &lt;Wert des Parameters&gt; condition: &lt;Name der Kondition oder Ausdruck oder boolean&gt; . | Name des Parameters: Dieser muss wieder einzigartig sein. | description: Es kann eine Beschreibung für den Parameter hinterlegt werden. (optional) | value: Hier wird der Wert des Parameters vermerkt. (benötigt) | condition: Hier kann eine Kondition für den Parameter festlegt werden. (optional) | . ",
    "url": "/optimist/guided_tour/step14/#output",
    
    "relUrl": "/optimist/guided_tour/step14/#output"
  },"144": {
    "doc": "14: Unsere ersten Schritte mit Heat",
    "title": "Condition",
    "content": "Dieser Bereich legt die verschiedenen Konditionen fest und das auf Basis der eingegebenen Parameter des Users, beim Erstellen oder updaten des Stacks. Die Konditionen können mit Resources, Resource properties und Outputs verbunden werden. Auch dieser Bereich folgt wieder einem Muster: . conditions: &lt;Name der Condition1&gt;: {Bezeichnung1} &lt;Name der Condition2&gt;: {Bezeichnung2} . | Name der Condition: Dieser muss wieder einzigartig im Bereich der Condition sein. | Bezeichnung: Bei der Bezeichnung wird erwartet, dass sie ein true oder false zurückgibt. | . ",
    "url": "/optimist/guided_tour/step14/#condition",
    
    "relUrl": "/optimist/guided_tour/step14/#condition"
  },"145": {
    "doc": "14: Unsere ersten Schritte mit Heat",
    "title": "Abschluss",
    "content": "In diesem Schritt wurden wichtige Bestandteile eines Heat-Templates vorgestellt. Mit diesem Wissen wird im nächsten Schritt das erste eigene Heat-Template erstellt. ",
    "url": "/optimist/guided_tour/step14/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step14/#abschluss"
  },"146": {
    "doc": "14: Unsere ersten Schritte mit Heat",
    "title": "14: Unsere ersten Schritte mit Heat",
    "content": " ",
    "url": "/optimist/guided_tour/step14/",
    
    "relUrl": "/optimist/guided_tour/step14/"
  },"147": {
    "doc": "15: Das erste eigene Heat Orchestration Template (HOT)",
    "title": "Schritt 15: Das erste eigene Heat Orchestration Template (HOT)",
    "content": " ",
    "url": "/optimist/guided_tour/step15/#schritt-15-das-erste-eigene-heat-orchestration-template-hot",
    
    "relUrl": "/optimist/guided_tour/step15/#schritt-15-das-erste-eigene-heat-orchestration-template-hot"
  },"148": {
    "doc": "15: Das erste eigene Heat Orchestration Template (HOT)",
    "title": "Vorwort",
    "content": "Im folgenden Schritt sind die wichtigsten Elemente eines Templates erläutert worden und auf dieses Wissen, wird in diesem Schritt aufgebaut. ",
    "url": "/optimist/guided_tour/step15/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step15/#vorwort"
  },"149": {
    "doc": "15: Das erste eigene Heat Orchestration Template (HOT)",
    "title": "Der Anfang",
    "content": "Dieser ist bei jedem Template gleich und ist immer heat_template_version . Für das Beispiel wird Version 2016-10-14 genutzt und somit sieht das Template erst einmal so aus: . heat_template_version: 2016-10-14 . Nachdem die heat_template_version festgelegt ist, wird dem Template nun eine Beschreibung hinzugefügt: . heat_template_version: 2016-10-14   description: Ein einfaches Template, um eine Instanz zu erstellen . Nachdem die Beschreibung in das Template integriert wurde, wird nun eine Ressource, also die Instanz hinzugefügt. Dabei sind einige Punkte zu beachten, starten wir zunächst mit der Ressource. Wichtig ist dabei, dass eine Strukturierung mit Leerzeichen genutzt wird. Dies dient der Übersichtlichkeit, außerdem würden Tabstops zu Fehlern führen und nur so kann das Template korrekt ausgeführt werden: . heat_template_version: 2016-10-14 description: Ein einfaches Template, um eine Instanz zu erstellen resources: Instanz: . Der nächste Schritt ist dann den Typ der Ressource zu benennen. Eine ausführliche Liste aller verfügbaren Typen befindet sich unter anderem in der offiziellen OpenStack Dokumentation . Da im Beispiel eine Instanz erstellt werden soll, ist der Typ dann folgender: . heat_template_version: 2016-10-14 description: Ein einfaches Template, um eine Instanz zu erstellen resources: Instanz: type: OS::Nova::Server . Nach dem Typ sind dann die Eigenschaften der nächste Punkt. Im Beispiel soll dies ein SSH-Key, ein Flavor und ein Image sein: . heat_template_version: 2016-10-14 description: Ein einfaches Template, um eine Instanz zu erstellen resources: Instanz: type: OS::Nova::Server properties: key_name: BeispielKey image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small . ",
    "url": "/optimist/guided_tour/step15/#der-anfang",
    
    "relUrl": "/optimist/guided_tour/step15/#der-anfang"
  },"150": {
    "doc": "15: Das erste eigene Heat Orchestration Template (HOT)",
    "title": "Abschluss",
    "content": "Damit ist das Erste eigenes Template fertiggestellt und kann, wenn es gespeichert wird, einfach mit dem OpenStackClienten wie in Schritt 13: “Der strukturierte Weg zu einer Instanz (mit Stacks)” beschrieben, gestartet werden. ",
    "url": "/optimist/guided_tour/step15/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step15/#abschluss"
  },"151": {
    "doc": "15: Das erste eigene Heat Orchestration Template (HOT)",
    "title": "15: Das erste eigene Heat Orchestration Template (HOT)",
    "content": " ",
    "url": "/optimist/guided_tour/step15/",
    
    "relUrl": "/optimist/guided_tour/step15/"
  },"152": {
    "doc": "16: Wir lernen Heat besser kennen",
    "title": "Schritt 16: Wir lernen Heat besser kennen",
    "content": " ",
    "url": "/optimist/guided_tour/step16/#schritt-16-wir-lernen-heat-besser-kennen",
    
    "relUrl": "/optimist/guided_tour/step16/#schritt-16-wir-lernen-heat-besser-kennen"
  },"153": {
    "doc": "16: Wir lernen Heat besser kennen",
    "title": "Vorwort",
    "content": "Bisher konnte der Eindruck entstehen, das Heat und das manuelle Erstellen per Kommandozeile genau so viel Zeit in Anspruch nimmt, was beim einmaligen Erstellen auch stimmt. Dadurch das nun ein Template existiert, können wir diese Grundlage immer wieder nutzen und im Zweifel weiter entwickeln. Damit dies auch möglich ist, wird in diesem Schritt weiter auf Heat eingegangen. ",
    "url": "/optimist/guided_tour/step16/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step16/#vorwort"
  },"154": {
    "doc": "16: Wir lernen Heat besser kennen",
    "title": "Parameter",
    "content": "Da das Ganze aufgebaut werden soll, ist es zunächst sinnvoll, bekannte oder individuelle Parameter zu definieren. In diesem Kontext wird der vorgegebene SSH-Key ersetzt und statt einem festen Wert, wird er als individueller Parameter definiert, der beim Start angegeben werden kann: . heat_template_version: 2014-10-16   parameters: key_name: type: string . Wie bisher gelernt, beginnt das Template mit der Version und wird dann mit parameters fortgeführt. Nach dem Parameter wird der Name, welcher individuell benannt werden kann, vergeben. Auch ist es notwendig den Typ anzugeben, in diesem Fall ist es string. Nachdem der Parameter festgelegt ist, nutzen wir als Vorlage das vorige Template und ergänzen es. Damit sieht das Template dann so aus: . heat_template_version: 2014-10-16 parameters: key_name: type: string resources: Instanz: type: OS::Nova::Server properties: key_name: BeispielKey image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small . Um das Template zu komplettieren, wird key_name durch den vorher definierten Parameter ersetzt. Der Befehl dafür lautet get_param. Dieser sagt aus, dass er einen definierten Parameter nutzen soll und damit das Template weiß, welchen Parameter er nutzen soll, ergänzen wir den Befehl get_param um key_name: . heat_template_version: 2014-10-16 parameters: key_name: type: string resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small . ",
    "url": "/optimist/guided_tour/step16/#parameter",
    
    "relUrl": "/optimist/guided_tour/step16/#parameter"
  },"155": {
    "doc": "16: Wir lernen Heat besser kennen",
    "title": "Abschluss",
    "content": "Das Template wurde jetzt bereits über einen frei definierbaren Parameter erweitert und im nächsten Schritt wird das Netzwerk hinzugefügt. ",
    "url": "/optimist/guided_tour/step16/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step16/#abschluss"
  },"156": {
    "doc": "16: Wir lernen Heat besser kennen",
    "title": "16: Wir lernen Heat besser kennen",
    "content": " ",
    "url": "/optimist/guided_tour/step16/",
    
    "relUrl": "/optimist/guided_tour/step16/"
  },"157": {
    "doc": "17: Das Netzwerk im Heat",
    "title": "Schritt 17: Das Netzwerk im Heat",
    "content": " ",
    "url": "/optimist/guided_tour/step17/#schritt-17-das-netzwerk-im-heat",
    
    "relUrl": "/optimist/guided_tour/step17/#schritt-17-das-netzwerk-im-heat"
  },"158": {
    "doc": "17: Das Netzwerk im Heat",
    "title": "Vorwort",
    "content": "Im letzten Schritt war ein individueller Parameter das Ziel und in diesem das komplette Netzwerk. ",
    "url": "/optimist/guided_tour/step17/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step17/#vorwort"
  },"159": {
    "doc": "17: Das Netzwerk im Heat",
    "title": "Das Template",
    "content": "Um nicht bei Null zu starten, dient das Template aus dem vorigen Schritt als Vorlage. Wichtig ist dabei, dass direkt ein neuer Parameter hinzufügt wird, genauer die ID des öffentlichen Netzwerks: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small . ",
    "url": "/optimist/guided_tour/step17/#das-template",
    
    "relUrl": "/optimist/guided_tour/step17/#das-template"
  },"160": {
    "doc": "17: Das Netzwerk im Heat",
    "title": "Netzwerk",
    "content": "Nachdem dieser Parameter eingefügt wurde, ist es Zeit das Netzwerk in das Template einzufügen. Hierbei handelt es sich um eine weitere Ressource und wird unter dem Punkt resources eingefügt. Der zugehörige Typ lautet OS::Neutron::Net: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk . ",
    "url": "/optimist/guided_tour/step17/#netzwerk",
    
    "relUrl": "/optimist/guided_tour/step17/#netzwerk"
  },"161": {
    "doc": "17: Das Netzwerk im Heat",
    "title": "Der Port",
    "content": "Der nächste Schritt ist dann der Port, der Typ lautet dafür OS::Neutron::Port. Wichtig ist, dass der Port in das bestehende Netzwerk eingegliedert wird und die Instanz dem Port zuzuordnen ist. Um dies zu erreichen, wird erneut ein get Befehl genutzt und statt dem Parameter eine Ressource eingebunden: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } . ",
    "url": "/optimist/guided_tour/step17/#der-port",
    
    "relUrl": "/optimist/guided_tour/step17/#der-port"
  },"162": {
    "doc": "17: Das Netzwerk im Heat",
    "title": "Der Router",
    "content": "Nachdem Netzwerk und Port, wird nun ein Router (Typ = OS::Neutron::Router) in das Template eingebunden. Bei diesem Typ ist es wichtig, das öffentliche Netzwerk einzubinden: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } name: BeispielRouter . ",
    "url": "/optimist/guided_tour/step17/#der-router",
    
    "relUrl": "/optimist/guided_tour/step17/#der-router"
  },"163": {
    "doc": "17: Das Netzwerk im Heat",
    "title": "Das Subnet",
    "content": "Der vorletzte Schritt ist das Subnet (Typ = OS::Neutron::Subnet ) . In selbigem werden eigene Nameserver eintragen, die Informationen des Netzwerks eingebunden, die IP-Version sowie der IP-Adressraum festgelegt und die verfügbaren IPs definiert: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } name: BeispielRouter Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 } . ",
    "url": "/optimist/guided_tour/step17/#das-subnet",
    
    "relUrl": "/optimist/guided_tour/step17/#das-subnet"
  },"164": {
    "doc": "17: Das Netzwerk im Heat",
    "title": "Subnet Bridge",
    "content": "Im letzten Schritt wird eine Subnet Bridge (Typ = OS::Neutron::RouterInterface) angelegt, also eine Brücke zwischen Router und Subnet. Auch gibt es hier eine weitere neue Komponente, genauer depends_on. Damit können wir Resource erstellen lassen, die nur dann gebaut werden, wenn es die referenzierte Resource auch gibt.   . In unserem Beispiel wird die Bridge zwischen Subnet und Router nur gebaut, wenn es auch ein Subnet gibt. heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } } name: BeispielRouter Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 } Router_Subnet_Bridge: type: OS::Neutron::RouterInterface depends_on: Subnet properties: router: { get_resource: Router } subnet: { get_resource: Subnet } . ",
    "url": "/optimist/guided_tour/step17/#subnet-bridge",
    
    "relUrl": "/optimist/guided_tour/step17/#subnet-bridge"
  },"165": {
    "doc": "17: Das Netzwerk im Heat",
    "title": "Abschluss",
    "content": "Nachdem nun das komplette Netzwerk eingerichtet wurde, wird im nächsten Schritt eine eigene Security Group erstellt und zusätzlich der Instanz eine öffentliche IP-Adresse zugewiesen. ",
    "url": "/optimist/guided_tour/step17/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step17/#abschluss"
  },"166": {
    "doc": "17: Das Netzwerk im Heat",
    "title": "17: Das Netzwerk im Heat",
    "content": " ",
    "url": "/optimist/guided_tour/step17/",
    
    "relUrl": "/optimist/guided_tour/step17/"
  },"167": {
    "doc": "18: Unsere Instanz wird von außen per IPv4 erreichbar",
    "title": "Schritt 18: Unsere Instanz wird von außen per IPv4 erreichbar",
    "content": " ",
    "url": "/optimist/guided_tour/step18/#schritt-18-unsere-instanz-wird-von-au%C3%9Fen-per-ipv4-erreichbar",
    
    "relUrl": "/optimist/guided_tour/step18/#schritt-18-unsere-instanz-wird-von-außen-per-ipv4-erreichbar"
  },"168": {
    "doc": "18: Unsere Instanz wird von außen per IPv4 erreichbar",
    "title": "Vorwort",
    "content": "Im letzten Schritt wurde das komplette Netzwerk eingerichtet und es ist nun an der Zeit, die Instanz von außen zu erreichen (u.a. per ICMP und SSH Zugriff). ",
    "url": "/optimist/guided_tour/step18/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step18/#vorwort"
  },"169": {
    "doc": "18: Unsere Instanz wird von außen per IPv4 erreichbar",
    "title": "Floating-IP",
    "content": "Den Anfang macht die öffentliche IP-Adresse, welche auch als Ressource hinzugefügt wird. (Der zugehörige Typ lautet OS::Neutron::FloatingIP). Wichtig ist, dass der Floating IP der entsprechende Port und welches das öffentliche Netz genutzt wird, mitgeteilt wird: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } } name: BeispielRouter Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 } Router_Subnet_Bridge: type: OS::Neutron::RouterInterface depends_on: Subnet properties: router: { get_resource: Router } subnet: { get_resource: Subnet } Floating_IP: type: OS::Neutron::FloatingIP properties: floating_network: { get_param: public_network_id } port_id: { get_resource: Port } . ",
    "url": "/optimist/guided_tour/step18/#floating-ip",
    
    "relUrl": "/optimist/guided_tour/step18/#floating-ip"
  },"170": {
    "doc": "18: Unsere Instanz wird von außen per IPv4 erreichbar",
    "title": "Security Groups",
    "content": "Wird das oben geschriebene Template gestartet, würde die Instanz erstellt werden, nur kann diese aufgrund der voreingestellten Security Group nicht erreicht werden. Um dies zu ändern, wird eine Security Group (Typ = OS::Neutron::SecurityGroup). Auch gibt es einige Besonderheiten zu beachten, es wird zum einen mit Regeln (rules) gearbeitet und zum anderen müssen selbige noch dem Port zugewiesen werden. So kann die direction (Richtung des Traffics) in ingress (eingehend) oder egress (ausgehend) eingeteilt, der entsprechende Port oder auch die Range der Ports definiert und auch das Protokoll festgelegt werden. Außerdem kann mit remote_ip_prefix noch festgelegt werden, wer die Instanz erreicht (falls nötig). heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } } name: BeispielRouter Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 } Router_Subnet_Bridge: type: OS::Neutron::RouterInterface depends_on: Subnet properties: router: { get_resource: Router } subnet: { get_resource: Subnet } Floating_IP: type: OS::Neutron::FloatingIP properties: floating_network: { get_param: public_network_id } port_id: { get_resource: Port } Sec_SSH: type: OS::Neutron::SecurityGroup properties: description: Diese Security Group erlaubt den eingehenden SSH-Traffic über Port22 und ICMP name: Ermöglicht SSH (Port22) und ICMP rules: - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, port_range_min: 22, port_range_max: 22, protocol: tcp } - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, protocol: icmp } . ",
    "url": "/optimist/guided_tour/step18/#security-groups",
    
    "relUrl": "/optimist/guided_tour/step18/#security-groups"
  },"171": {
    "doc": "18: Unsere Instanz wird von außen per IPv4 erreichbar",
    "title": "Abschluss",
    "content": "Die erstellte Instanz ist von außen erreichbar inklusive einer öffentliche IP. Im nächsten Schritt wird die Instanz per CloudConfig angepasst. ",
    "url": "/optimist/guided_tour/step18/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step18/#abschluss"
  },"172": {
    "doc": "18: Unsere Instanz wird von außen per IPv4 erreichbar",
    "title": "18: Unsere Instanz wird von außen per IPv4 erreichbar",
    "content": " ",
    "url": "/optimist/guided_tour/step18/",
    
    "relUrl": "/optimist/guided_tour/step18/"
  },"173": {
    "doc": "19: Unsere Instanz lernt IPv6",
    "title": "Schritt 19: Unsere Instanz lernt IPv6",
    "content": " ",
    "url": "/optimist/guided_tour/step19/#schritt-19-unsere-instanz-lernt-ipv6",
    
    "relUrl": "/optimist/guided_tour/step19/#schritt-19-unsere-instanz-lernt-ipv6"
  },"174": {
    "doc": "19: Unsere Instanz lernt IPv6",
    "title": "Vorwort",
    "content": "Nachdem im letzten Schritt die Instanz mit einer öffentlichen IPv4 Adresse versehen wurde und diese auch per SSH erreichbar ist, wird es nun Zeit die Instanz selber anzupassen. Dafür nutzen wir in diesem Schritt CloudConfig und passen auch die Security Group an. ",
    "url": "/optimist/guided_tour/step19/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step19/#vorwort"
  },"175": {
    "doc": "19: Unsere Instanz lernt IPv6",
    "title": "CloudConfig",
    "content": "Ist eine Ressource und wird daher auch unter resources geführt. (Typ = OS::HEAT::CloudConfig) . Es gibt sehr viele Möglichkeiten, was alles in einer Instanz mit CloudConfig bearbeiten werden kann. Im diesem Schritt beschäftigen wir uns damit, alles notwendige für IPv6 vorzubereiten. Der Start macht hierbei das erstellen der entsprechenden Dateien mit dem notwendigen Inhalt, den wir bereits aus Schritt 11: Zugriff aus dem Internet vorbereiten: Wir ergänzen IPv6 kennen und nutzen CloudConfig in der cloud_config den Befehl write_files: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small networks: - port: { get_resource: Port }   Instanz-Config: type: OS::Heat::CloudConfig properties: cloud_config: write_files: - path: /etc/dhcp/dhclient6.conf content: \"timeout 30;\" - path: /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg content: \"network: {config: disabled}\" - path: /etc/network/interfaces.d/lo.cfg content: | auto lo iface lo inet loopback - path: /etc/network/interfaces.d/ens3.cfg content: | iface ens3 inet6 auto up sleep 5 up dhclient -1 -6 -cf /etc/dhcp/dhclient6.conf -lf /var/lib/dhcp/dhclient6.ens3.leases -v ens3 || true Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } security_groups: { get_resource: Sec_SSH } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } } name: BeispielRouter Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - #MussNochEingetragenWerden - #MussNochEingetragenWerden network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 } Router_Subnet_Bridge: type: OS::Neutron::RouterInterface depends_on: Subnet properties: router: { get_resource: Router } subnet: { get_resource: Subnet } Floating_IP: type: OS::Neutron::FloatingIP properties: floating_network: { get_param: public_network_id } port_id: { get_resource: Port } Sec_SSH: type: OS::Neutron::SecurityGroup properties: description: Diese Security Group erlaubt den eingehenden SSH-Traffic über Port22 und ICMP name: Ermöglicht SSH (Port22) und ICMP rules: - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, port_range_min: 22, port_range_max: 22, protocol:tcp } - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, protocol: icmp } . Wir haben die Dateien erstellt und den entsprechenden Inhalt eingefügt. Wie in Schritt 11: Zugriff aus dem Internet vorbereiten: Wir ergänzen IPv6 beschrieben, ist es noch notwendig das Interface mit dem Befehl runcmd neu zustarten. heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small networks: - port: { get_resource: Port } Instanz-Config: type: OS::Heat::CloudConfig properties: cloud_config: write_files: - path: /etc/dhcp/dhclient6.conf content: \"timeout 30;\" - path: /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg content: \"network: {config: disabled}\" - path: /etc/network/interfaces.d/lo.cfg content: | auto lo iface lo inet loopback - path: /etc/network/interfaces.d/ens3.cfg content: | iface ens3 inet6 auto up sleep 5 up dhclient -1 -6 -cf /etc/dhcp/dhclient6.conf -lf /var/lib/dhcp/dhclient6.ens3.leases -v ens3 || true runcmd: - [ ifdown, ens3] - [ ifup, ens3] Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } security_groups: { get_resource: Sec_SSH } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } } name: BeispielRouter Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 } Router_Subnet_Bridge: type: OS::Neutron::RouterInterface depends_on: Subnet properties: router: { get_resource: Router } subnet: { get_resource: Subnet } Floating_IP: type: OS::Neutron::FloatingIP properties: floating_network: { get_param: public_network_id } port_id: { get_resource: Port } Sec_SSH: type: OS::Neutron::SecurityGroup properties: description: Diese Security Group erlaubt den eingehenden SSH-Traffic über Port22 und ICMP name: Ermöglicht SSH (Port22) und ICMP rules: - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, port_range_min: 22, port_range_max: 22, protocol:tcp } - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, protocol: icmp } . Im letzten Schritt passen wir die Security Group an, damit auch ein Zugriff über IPv6 möglich ist. heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small networks: - port: { get_resource: Port } Instanz-Config: type: OS::Heat::CloudConfig properties: cloud_config: write_files: - path: /etc/dhcp/dhclient6.conf content: \"timeout 30;\" - path: /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg content: \"network: {config: disabled}\" - path: /etc/network/interfaces.d/lo.cfg content: | auto lo iface lo inet loopback - path: /etc/network/interfaces.d/ens3.cfg content: | iface ens3 inet6 auto up sleep 5 up dhclient -1 -6 -cf /etc/dhcp/dhclient6.conf -lf /var/lib/dhcp/dhclient6.ens3.leases -v ens3 || true runcmd: - [ ifdown, ens3] - [ ifup, ens3] Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } security_groups: { get_resource: Sec_SSH } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } } name: BeispielRouter Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 } Router_Subnet_Bridge: type: OS::Neutron::RouterInterface depends_on: Subnet properties: router: { get_resource: Router } subnet: { get_resource: Subnet } Floating_IP: type: OS::Neutron::FloatingIP properties: floating_network: { get_param: public_network_id } port_id: { get_resource: Port } Sec_SSH: type: OS::Neutron::SecurityGroup properties: description: Diese Security Group erlaubt den eingehenden SSH-Traffic über Port22 und ICMP name: Ermöglicht SSH (Port22) und ICMP rules: - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, port_range_min: 22, port_range_max: 22, protocol:tcp } - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, protocol: icmp } - { direction: ingress, remote_ip_prefix: \"::/0\", port_range_min: 22, port_range_max: 22, protocol: tcp, ethertype: IPv6 } - { direction: ingress, remote_ip_prefix: \"::/0\", protocol: ipv6-icmp, ethertype: IPv6 } . ",
    "url": "/optimist/guided_tour/step19/#cloudconfig",
    
    "relUrl": "/optimist/guided_tour/step19/#cloudconfig"
  },"176": {
    "doc": "19: Unsere Instanz lernt IPv6",
    "title": "Abschluss",
    "content": "Wir haben nun die Möglichkeit Instanzen per Cloud-Init anzupassen und IPv6 nutzbar gemacht. Im nächsten und letzten Schritt werden wir mehrere Instanzen per Heat starten. ",
    "url": "/optimist/guided_tour/step19/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step19/#abschluss"
  },"177": {
    "doc": "19: Unsere Instanz lernt IPv6",
    "title": "19: Unsere Instanz lernt IPv6",
    "content": " ",
    "url": "/optimist/guided_tour/step19/",
    
    "relUrl": "/optimist/guided_tour/step19/"
  },"178": {
    "doc": "20: Mehrere Instanzen gleichzeitig erstellen",
    "title": "Schritt 20: Mehrere Instanzen gleichzeitig erstellen",
    "content": " ",
    "url": "/optimist/guided_tour/step20/#schritt-20-mehrere-instanzen-gleichzeitig-erstellen",
    
    "relUrl": "/optimist/guided_tour/step20/#schritt-20-mehrere-instanzen-gleichzeitig-erstellen"
  },"179": {
    "doc": "20: Mehrere Instanzen gleichzeitig erstellen",
    "title": "Vorwort",
    "content": "Nachdem im Schritt 15 eine Instanz inklusive aller wichtigen Einstellungen angelegt wurde, ist der nächste Schritt, mehr als eine Instanz per Template zu starten. In diesem Schritt, werden zwei Instanzen erstellt, die ein gemeinsames Netzwerk nutzen. ",
    "url": "/optimist/guided_tour/step20/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step20/#vorwort"
  },"180": {
    "doc": "20: Mehrere Instanzen gleichzeitig erstellen",
    "title": "Start",
    "content": "Neben den beiden Instanzen wird auch das Template aufgeteilt und in zwei Dateien erstellt. Dies hat verschiedene Teile und den Start macht ein simples Template, welches nur ein Net und ein Subnet enthält: . heat_template_version: 2014-10-16 description: Ein simples Template welches 2 Instanzen erstellt resources: BeispielNet: type: OS::Neutron::Net properties: name: BeispielNet BeispielSubnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: {get_resource: BeispielNet} ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - {start: 10.0.0.10, end: 10.0.0.250} . Dies stellt das Grundgerüst des Stacks dar und wird vorerst als Gruppen.yaml gespeichert. Die Instanzen selber werden in einer zweiten Datei BeispielServer.yaml beschrieben, welche dem gleichen Aufbau wie in den vorigen Schritten folgt. Um image: zu füllen kann wahlweise der Image Name oder die Image-ID benutzt werden. Eine korrekte Image-ID bzw. einen korrekten Namen erhält man mit openstack image list. Es ist wichtig, dass kein Server-Namen definiert wird und network_id auch keinen Eintrag erfährt: . heat_template_version: 2014-10-16 description: Ein einzelner Server der durch eine Ressourcen Gruppe verwendet wird parameters: network_id: type: string server_name: type: string resources: Instanz: type: OS::Nova::Server properties: user_data_format: RAW image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small name: { get_param: server_name } networks: - port: { get_resource: BeispielPort } BeispielPort: type: OS::Neutron::Port properties: network: { get_param: network_id } . Nachdem die Datei fertiggestellt wurde, wird diese wie oben beschrieben als BeispielServer.yaml gespeichert. Um weiter fortzufahren, wird die Arbeit am ursprünglichen Template (Gruppen.yaml) fortgesetzt. Hier gilt es nun, das zweite erstellte Template als RessourceGroup einzubinden. Auch ist so direkt die Möglichkeit gegeben, die Anzahl der Instanzen, die Namen etc. anzugeben: . heat_template_version: 2014-10-16 description: Ein simples Template welches 2 Instanzen erstellt resources:   BeispielInstanzen: type: OS::Heat::ResourceGroup depends_on: BeispielSubnet properties: count: 2 resource_def: type: BeispielServer.yaml properties: network_id: { get_resource: BeispielNet } server_name: BeispielInstanz_%index% BeispielNet: type: OS::Neutron::Net properties: name: BeispielNet BeispielSubnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: {get_resource: BeispielNet} ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - {start: 10.0.0.10, end: 10.0.0.250} . Nachdem die Arbeit an Gruppen.yaml abgeschlossen wurde, kann der so erstellte Stack direkt mit dem OpenStackClient gestartet werden: . openstack stack create -t Gruppen.yaml &lt;Name des Stacks&gt; . ",
    "url": "/optimist/guided_tour/step20/#start",
    
    "relUrl": "/optimist/guided_tour/step20/#start"
  },"181": {
    "doc": "20: Mehrere Instanzen gleichzeitig erstellen",
    "title": "Abschluss",
    "content": "Nachdem am Anfang der Guided Tour noch Instanzen per Hand erstellt wurden, können nun bereits mehrere Instanzen gleichzeitig per Template ausgerollt werden und stellen einen guten Startpunkt für die Administration von OpenStack dar. ",
    "url": "/optimist/guided_tour/step20/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step20/#abschluss"
  },"182": {
    "doc": "20: Mehrere Instanzen gleichzeitig erstellen",
    "title": "20: Mehrere Instanzen gleichzeitig erstellen",
    "content": " ",
    "url": "/optimist/guided_tour/step20/",
    
    "relUrl": "/optimist/guided_tour/step20/"
  },"183": {
    "doc": "21: Eine Instanz von einem SSD-Volume starten",
    "title": "Schritt 21: Eine Instanz von einem SSD-Volume starten",
    "content": " ",
    "url": "/optimist/guided_tour/step21/#schritt-21-eine-instanz-von-einem-ssd-volume-starten",
    
    "relUrl": "/optimist/guided_tour/step21/#schritt-21-eine-instanz-von-einem-ssd-volume-starten"
  },"184": {
    "doc": "21: Eine Instanz von einem SSD-Volume starten",
    "title": "Vorwort",
    "content": "In den vorigen Schritten haben wir uns bereits eine eigene Instanz erstellt und auch die ersten Grundlagen in HEAT sind gelegt. Wir werden in diesem Schritt eine Instanz von einem Volume starten und dafür den SSD-Speicher nutzen. Auch hier gibt es mehrere Wege unser Ziel zu erreichen, daher werden wir in diesem Schritt sowohl das Horizon(Dashboard) nutzen, als auch unser HEAT Template aus Schritt 18 weiter modifizieren. ",
    "url": "/optimist/guided_tour/step21/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step21/#vorwort"
  },"185": {
    "doc": "21: Eine Instanz von einem SSD-Volume starten",
    "title": "Der Weg über das Horizon(Dashboard)",
    "content": "Um zu starten, loggen wir uns zunächst wie in “Schritt 1: Das Dashboard (Horizon)” erklärt ins Horizon(Dashboard) ein. Hier wechseln wir links in der Seitenleiste auf Project → Volumes → Volumes und klicken dann rechts auf “+ Create Volume” . In dem sich öffnenden Fenster geben wir folgende Optionen an und klicken dann auf “Create Volume”: . | Volume Name: Hier wird der Name des Volumes vergeben, dieser kann frei gewählt werden. Im Beispiel wird nach der Auswahl des Images automatisch “Ubuntu 16.04 Xenial Xerus - Latest” eingetragen. | Description: In diesem Feld kann eine Beschreibung hinzugefügt werden, je nach Bedarf. Im Beispiel wird keine Beschreibung verwendet. | Volume Source: Hier kann zwischen “Image” und “No source, empty image” gewählt werden. Für unser Beispiel nutzen wir “Image”. | Use image as a source: Es kann ein beliebiges Image genutzt werden. Im Beispiel wird “Ubuntu 16.04 Xenial Xerus - Latest (276.2 MB)” verwendet. | Type: Hier besteht die Wahl zwischen “high-iops”, “low-iops” und “default”. Da wir SSD-Speicher nutzen wollen, wählen wir “high-iops” aus. | Size: In diesem Feld bestimmen wir die Größe des Volumes, bei unseren Flavors sind es 20 GiB, daher nutzen wir dies auch für unser Beispiel | Availability Zone: Hier kann man zwischen 3 Optionen “Any Availability Zone”, “es1” oder “ix1” wählen und die entsprechende Zone festlegen. Im Beispiel nutzen wir ix1. | . Nachdem das Horizon das Volume korrekt erstellt hat, sollte es in etwa so aussehen: . Um eine neue Instanz von diesem Volume zu starten, können wir entweder rechts auf den Pfeil nach unten, neben “Edit Volume”, klicken und dann auf “Launch as Instance” oder alternativ dazu kann man auch links in der Seitenleiste auf Compute → Instances wechseln und dort auf “Launch Instane” klicken. Im sich öffnenden Fenster geben wir der Instanz einen Namen (Instance Name), wählen dieselbe Availability Zone wie weiter oben, also ix1 und wechseln dann links auf Source. Unter Source wählen wir Volume als Select Boot Source aus und klicken dann neben unserem erstellten Volume auf den Pfeil nach oben. Nun klicken wir links auf Flavor und wählen einen der möglichen Flavors aus, indem wir auf den Pfeil nach oben neben dem gewünschten Flavor klicken. Im nächsten Schritt wählen wir links, über den Reiter Networks das Netzwerk für die VM aus. Auch hier klicken wir neben dem gewünschten Netzwerk auf den Pfeil nach oben. Damit sind alle wichtigen Einstellungen getroffen und die Instanz kann mit “Launch Instance” gestartet werden. Falls benötigt, können noch eigene Security Groups und/oder Key Pairs der Instanz hinzugefügt werden. ",
    "url": "/optimist/guided_tour/step21/#der-weg-%C3%BCber-das-horizondashboard",
    
    "relUrl": "/optimist/guided_tour/step21/#der-weg-über-das-horizondashboard"
  },"186": {
    "doc": "21: Eine Instanz von einem SSD-Volume starten",
    "title": "Der Weg über HEAT",
    "content": "Wie bereits im Vorwort erwähnt, nutzen wir unser HEAT Template aus Schritt 18. Dieses Template startet bereits eine Instanz. Damit diese nun aber ein SSD-Volume nutzt, bedarf es einiger Änderungen. Zunächst fügen wir unseren Parametern noch die “availability_zone” hinzu: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider availability_zone: type: string default: ix1 . Der nächste Schritt ist am Ende des Templates einen eigenen Punkt “boot_ssd” für das Volume hinzuzufügen: . boot_ssd: type: OS::Cinder::Volume properties: name: boot_ssd size: 20 availability_zone: { get_param: availability_zone } volume_type: high-iops image: \"Ubuntu 16.04 Xenial Xerus - Latest\" . Nun haben wir bereits einen Parameter hinzugefügt und nutzten diesen auch direkt in unserem neu erstellten Boot-Volume. Damit die Instanz auch vom Volume startet, überarbeiten wir den Punkt “Instanz” in unserem HEAT-Template . Dort können wir den Punkt “image” entfernen (im Beispiel ist er per # auskommentiert), da dieser ja über das Volume bereitgestellt wird. Wir fügen nun noch die “availability_zone”, einen Namen “name”, das Netzwerk “networks” und das Volume “block_device_mapping” hinzu: . Instanz: type: OS::Nova::Server properties: name: SSD-Test availability_zone: { get_param: availability_zone } key_name: { get_param: key_name } #image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small networks: - port: { get_resource: Port } block_device_mapping: [ { device_name: \"vda\", volume_id: { get_resource: boot_ssd }, delete_on_termination: \"true\" } ] . Damit ist unser HEAT-Template für diesen Schritt fertig und sollte so aussehen: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider availability_zone: type: string default: ix1 resources: Instanz: type: OS::Nova::Server properties: name: SSD-Test availability_zone: { get_param: availability_zone } key_name: { get_param: key_name } #image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small networks: - port: { get_resource: Port } block_device_mapping: [ { device_name: \"vda\", volume_id: { get_resource: boot_ssd }, delete_on_termination: \"true\" } ] Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } name: BeispielRouter Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 } Router_Subnet_Bridge: type: OS::Neutron::RouterInterface depends_on: Subnet properties: router: { get_resource: Router } subnet: { get_resource: Subnet } Floating_IP: type: OS::Neutron::FloatingIP properties: floating_network: { get_param: public_network_id } port_id: { get_resource: Port } Sec_SSH: type: OS::Neutron:SecurityGroup properties: description: Diese Security Group erlaubt den eingehenden SSH-Traffic über Port22 und ICMP name: Ermöglicht SSH (Port22) und ICMP rules: - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, port_range_min: 22, port_range_max: 22, protocol: tcp } - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, protocol: icmp } boot_ssd: type: OS::Cinder::Volume properties: name: boot_ssd size: 20 availability_zone: { get_param: availability_zone } volume_type: high-iops image: \"Ubuntu 16.04 Xenial Xerus - Latest\" . ",
    "url": "/optimist/guided_tour/step21/#der-weg-%C3%BCber-heat",
    
    "relUrl": "/optimist/guided_tour/step21/#der-weg-über-heat"
  },"187": {
    "doc": "21: Eine Instanz von einem SSD-Volume starten",
    "title": "Abschluss",
    "content": "In diesem Schritt haben wir gelernt, dass es ohne Weiteres möglich ist, eine Instanz auch von einem Volume zu starten und auch gleichzeitig schnellen SSD Speicher zu nutzen. Außerdem haben wir unsere HEAT-Kenntnisse aufgefrischt und ein Volume mit eingebunden. ",
    "url": "/optimist/guided_tour/step21/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step21/#abschluss"
  },"188": {
    "doc": "21: Eine Instanz von einem SSD-Volume starten",
    "title": "21: Eine Instanz von einem SSD-Volume starten",
    "content": " ",
    "url": "/optimist/guided_tour/step21/",
    
    "relUrl": "/optimist/guided_tour/step21/"
  },"189": {
    "doc": "22: Anlegen eines DNS-Record in Designate",
    "title": "Schritt 22: Anlegen eines DNS-Record in Designate",
    "content": " ",
    "url": "/optimist/guided_tour/step22/#schritt-22-anlegen-eines-dns-record-in-designate",
    
    "relUrl": "/optimist/guided_tour/step22/#schritt-22-anlegen-eines-dns-record-in-designate"
  },"190": {
    "doc": "22: Anlegen eines DNS-Record in Designate",
    "title": "Vorwort",
    "content": "Die Openstackplattform Optimist enthält eine Technologie namens DNS-as-a-Service (DNSaaS), auch bekannt als Designate. DNSaaS enthält eine REST-API für die Domänen- und Datensatzverwaltung, ist multi-tenant und integriert den OpenStack Identity Service (Keystone) für die Authentifizierung. Wir werden in diesem Schritt eine fiktive Zone (Domain) mit MX und A-Records erstellen und die entsprechende IP/CNAME hinterlegen. Um zu starten, lesen wir uns zunächst wie in “Schritt 4: Der Weg vom Horizon auf die Kommandozeile” erklärt die Zugangsdaten ein und sorgen dafür das der python-designateclient installiert ist (pip install python-openstackclient python-designateclient) Anschliessend bedienen wir den Openstack-Client und erstellen zuerst eine Zone für unser Projekt. $ openstack zone create --email webmaster@foobar.cloud foobar.cloud. +----------------+--------------------------------------+ | Field | Value | +----------------+--------------------------------------+ | action | CREATE | attributes | | created_at | 2018-08-15T06:45:24.000000 | description | None | email | webmaster@foobar.cloud | id | 036ae6e6-6318-47e1-920f-be518d845fb5 | masters | | name | foobar.cloud. | pool_id | bb031d0d-b8ca-455a-8963-50ec70fe57cf | project_id | 2b62bc8ff48445f394d0318dbd058967 | serial | 1534315524 | status | PENDING | transferred_at | None | ttl | 3600 | type | PRIMARY | updated_at | None | version | 1 | +----------------+--------------------------------------+ . Man beachte den abschliessenden “.” an der zu erstellenden Zone/Domain. Das Resultat bisher: . $ openstack zone list +--------------------------------------+-----------------------+---------+------------+--------+--------+ | id | name | type | serial | status | action | +--------------------------------------+-----------------------+---------+------------+--------+--------+ | 036ae6e6-6318-47e1-920f-be518d845fb5 | foobar.cloud. | PRIMARY | 1534315524 | ACTIVE | NONE | +--------------------------------------+-----------------------+---------+------------+--------+--------+ $ openstack zone show foobar.cloud. +----------------+--------------------------------------+ | Field | Value | +----------------+--------------------------------------+ | action | NONE | attributes | | created_at | 2018-08-15T06:45:24.000000 | description | None | email | webmaster@foobar.cloud | id | 036ae6e6-6318-47e1-920f-be518d845fb5 | masters | | name | foobar.cloud. | pool_id | bb031d0d-b8ca-455a-8963-50ec70fe57cf | project_id | 2b62bc8ff48445f394d0318dbd058967 | serial | 1534315524 | status | ACTIVE | transferred_at | None | ttl | 3600 | type | PRIMARY | updated_at | 2018-08-15T06:45:30.000000 | version | 2 | +----------------+--------------------------------------+ . Nun ist die Domain “foobar.cloud” für unser Projekt registriert und einsatzbereit (status: ACTIVE). Wir wollen im nächsten Schritt MX-Records (Datensätze für Mailserver in dieser Zone) für diese Domain erstellen. Doch zuerst schauen wir, welche Inhalte (Recordsets) unsere neue Zone bereits jetzt besitzt. $ openstack recordset list foobar.cloud. +--------------------------------------+---------------+------+--------------------------------------------------------------------------------+--------+--------+ | id | name | type | records | status | action | +--------------------------------------+---------------+------+--------------------------------------------------------------------------------+--------+--------+ | 4b666317-6170-4d71-8962-94f1cbe94dda | foobar.cloud. | NS | dns1.ddns.innovo.cloud. | ACTIVE | NONE | | | dns2.ddns.innovo.cloud. | | 7915c9c1-de30-4144-b82b-36295687b0fe | foobar.cloud. | SOA | dns2.ddns.innovo.cloud. webmaster.foobar.cloud. 1534315524 3507 600 86400 3600 | ACTIVE | NONE | +--------------------------------------+---------------+------+--------------------------------------------------------------------------------+--------+--------+ . Hier sehen wir eine “leere Hülle” einer Domain mit automatisch erstellen NS und SOA-Einträgen, die sofort zur Abfrage bereit stehen. $ dig +short @dns1.ddns.innovo.cloud foobar.cloud NS dns1.ddns.innovo.cloud. dns2.ddns.innovo.cloud. $ dig +short @dns1.ddns.innovo.cloud foobar.cloud SOA dns2.ddns.innovo.cloud. webmaster.foobar.cloud. 1534315524 3507 600 86400 3600 . Anlegen eines MX-Records: Nun können wir Datensätze innerhalb dieser Zone hinzufügen, verändern oder löschen (openstack recordset –help). Als nächstes fügen wir einen MX und einen A-Record hinzu. Bei den MX-Records richten wir auch gleich die typischen Mailserver-Prioritäten (10,20) mit ein. Wobei immer der niedrigere Wert als erstes angesteuert wird und der zweite Eintrag als “Backup” dient. $ openstack recordset create --record '10 mx1.foobar.cloud.' --record '20 mx2.foobar.cloud.' --type MX foobar.cloud. foobar.cloud. +-------------+--------------------------------------+ | Field | Value | +-------------+--------------------------------------+ | action | CREATE | created_at | 2018-08-15T07:15:32.000000 | description | None | id | 4197e380-dc61-4e1d-bf92-0dcf5344eafa | name | foobar.cloud. | project_id | 2b62bc8ff48445f394d0318dbd058967 | records | 10 mx1.foobar.cloud. | | 20 mx2.foobar.cloud. | status | PENDING | ttl | None | type | MX | updated_at | None | version | 1 | zone_id | 036ae6e6-6318-47e1-920f-be518d845fb5 | zone_name | foobar.cloud. | +-------------+--------------------------------------+ $ openstack recordset list foobar.cloud. +--------------------------------------+---------------+------+--------------------------------------------------------------------------------+---------+--------+ | id | name | type | records | status | action | +--------------------------------------+---------------+------+--------------------------------------------------------------------------------+---------+--------+ | 4b666317-6170-4d71-8962-94f1cbe94dda | foobar.cloud. | NS | dns1.ddns.innovo.cloud. | ACTIVE | NONE | | | dns2.ddns.innovo.cloud. | | 7915c9c1-de30-4144-b82b-36295687b0fe | foobar.cloud. | SOA | dns2.ddns.innovo.cloud. webmaster.foobar.cloud. 1534317332 3507 600 86400 3600 | PENDING | UPDATE | 4197e380-dc61-4e1d-bf92-0dcf5344eafa | foobar.cloud. | MX | 20 mx2.foobar.cloud. | PENDING | CREATE | | | 10 mx1.foobar.cloud. | | +--------------------------------------+---------------+------+--------------------------------------------------------------------------------+---------+--------+ . $ openstack recordset create --type A --record 1.2.3.4 foobar.cloud. www +-------------+--------------------------------------+ | Field | Value | +-------------+--------------------------------------+ | action | CREATE | created_at | 2018-08-15T07:28:15.000000 | description | None | id | d932688f-21d5-44b1-aa27-030c342788e7 | name | www.foobar.cloud. | project_id | 2b62bc8ff48445f394d0318dbd058967 | records | 1.2.3.4 | status | PENDING | ttl | None | type | A | updated_at | None | version | 1 | zone_id | 036ae6e6-6318-47e1-920f-be518d845fb5 | zone_name | foobar.cloud. | +-------------+--------------------------------------+ . Resultat: . $ openstack recordset list foobar.cloud. +--------------------------------------+-------------------+------+--------------------------------------------------------------------------------+--------+--------+ | id | name | type | records | status | action | +--------------------------------------+-------------------+------+--------------------------------------------------------------------------------+--------+--------+ | 4b666317-6170-4d71-8962-94f1cbe94dda | foobar.cloud. | NS | dns1.ddns.innovo.cloud. | ACTIVE | NONE | | | dns2.ddns.innovo.cloud. | | 7915c9c1-de30-4144-b82b-36295687b0fe | foobar.cloud. | SOA | dns2.ddns.innovo.cloud. webmaster.foobar.cloud. 1534318095 3507 600 86400 3600 | ACTIVE | NONE | 4197e380-dc61-4e1d-bf92-0dcf5344eafa | foobar.cloud. | MX | 20 mx2.foobar.cloud. | ACTIVE | NONE | | | 10 mx1.foobar.cloud. | | d932688f-21d5-44b1-aa27-030c342788e7 | www.foobar.cloud. | A | 1.2.3.4 | ACTIVE | NONE | +--------------------------------------+-------------------+------+--------------------------------------------------------------------------------+--------+--------+ . Wenn die Recordsets aktiv sind können wir die dafür vorgesehenen DNS-Server . | dns1.ddns.innovo.cloud | dns2.ddns.innovo.cloud nach diesen Records abfragen. | . $ dig +short @dns1.ddns.innovo.cloud foobar.cloud NS dns1.ddns.innovo.cloud. dns2.ddns.innovo.cloud. $ dig +short @dns1.ddns.innovo.cloud foobar.cloud MX 10 mx1.foobar.cloud. 20 mx2.foobar.cloud. $ dig +short @dns1.ddns.innovo.cloud foobar.cloud www.foobar.cloud 1.2.3.4 . ACHTUNG! Zu diesem Zeitpunkt ist diese Domaine (foobar.cloud) noch nicht weltweit auflösbar. Damit dieses Konstrukt weltweit benutzt werden kann, muss jede im Designate verwaltete Domain bei dem jeweiligen Registrar die Delegation zu den Nameservern dns1.ddns.innovo.cloud und dns2.ddns.innovo.cloud eingerichtet haben. Details zu unseren authoritativen DNS-Server: . | dns1.ddns.innovo.cloud: ‘185.116.244.45’ / ‘2a00:c320:0:1::d’ | dns2.ddns.innovo.cloud: ‘185.116.244.46’ / ‘2a00:c320:0:1::e’ | . Um die die Mail-Records abzuschliessen, bietet sich noch an für die Mailserver entsprechende A-Records zu hinterlegen . $ openstack recordset create --type A --record 2.3.4.5 foobar.cloud. mx1 +-------------+--------------------------------------+ | Field | Value | +-------------+--------------------------------------+ | action | CREATE | created_at | 2018-08-15T07:42:31.000000 | description | None | id | 630d5103-7c02-4a58-83a5-97f802cf141c | name | mx1.foobar.cloud. | project_id | 2b62bc8ff48445f394d0318dbd058967 | records | 2.3.4.5 | status | PENDING | ttl | None | type | A | updated_at | None | version | 1 | zone_id | 036ae6e6-6318-47e1-920f-be518d845fb5 | zone_name | foobar.cloud. | +-------------+--------------------------------------+ und $ openstack recordset create --type A --record 3.4.5.6 foobar.cloud. mx2 +-------------+--------------------------------------+ | Field | Value | +-------------+--------------------------------------+ | action | CREATE | created_at | 2018-08-15T07:42:56.000000 | description | None | id | 2b47dbe4-70b9-4edb-ac2f-25cb8398bacb | name | mx2.foobar.cloud. | project_id | 2b62bc8ff48445f394d0318dbd058967 | records | 3.4.5.6 | status | PENDING | ttl | None | type | A | updated_at | None | version | 1 | zone_id | 036ae6e6-6318-47e1-920f-be518d845fb5 | zone_name | foobar.cloud. | +-------------+--------------------------------------+ . Das Resultat nach einigen Sekunden: . $ openstack recordset list foobar.cloud. +--------------------------------------+-------------------+------+--------------------------------------------------------------------------------+--------+--------+ | id | name | type | records | status | action | +--------------------------------------+-------------------+------+--------------------------------------------------------------------------------+--------+--------+ | 4b666317-6170-4d71-8962-94f1cbe94dda | foobar.cloud. | NS | dns1.ddns.innovo.cloud. | ACTIVE | NONE | | | dns2.ddns.innovo.cloud. | | 7915c9c1-de30-4144-b82b-36295687b0fe | foobar.cloud. | SOA | dns2.ddns.innovo.cloud. webmaster.foobar.cloud. 1534318976 3507 600 86400 3600 | ACTIVE | NONE | 4197e380-dc61-4e1d-bf92-0dcf5344eafa | foobar.cloud. | MX | 20 mx2.foobar.cloud. | ACTIVE | NONE | | | 10 mx1.foobar.cloud. | | d932688f-21d5-44b1-aa27-030c342788e7 | www.foobar.cloud. | A | 1.2.3.4 | ACTIVE | NONE | 630d5103-7c02-4a58-83a5-97f802cf141c | mx1.foobar.cloud. | A | 2.3.4.5 | ACTIVE | NONE | 2b47dbe4-70b9-4edb-ac2f-25cb8398bacb | mx2.foobar.cloud. | A | 3.4.5.6 | ACTIVE | NONE | +--------------------------------------+-------------------+------+--------------------------------------------------------------------------------+--------+--------+ . ",
    "url": "/optimist/guided_tour/step22/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step22/#vorwort"
  },"191": {
    "doc": "22: Anlegen eines DNS-Record in Designate",
    "title": "Abschluss",
    "content": "In diesem Schritt haben wir gelernt, wie man eine Zone anlegt, einen Recordset konfiguriert und diesen abfragen kann. ",
    "url": "/optimist/guided_tour/step22/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step22/#abschluss"
  },"192": {
    "doc": "22: Anlegen eines DNS-Record in Designate",
    "title": "22: Anlegen eines DNS-Record in Designate",
    "content": " ",
    "url": "/optimist/guided_tour/step22/",
    
    "relUrl": "/optimist/guided_tour/step22/"
  },"193": {
    "doc": "23: Der Object Storage (S3 kompatibel)",
    "title": "Schritt 23: Der Object Storage (S3 kompatibel)",
    "content": " ",
    "url": "/optimist/guided_tour/step23/#schritt-23-der-object-storage-s3-kompatibel",
    
    "relUrl": "/optimist/guided_tour/step23/#schritt-23-der-object-storage-s3-kompatibel"
  },"194": {
    "doc": "23: Der Object Storage (S3 kompatibel)",
    "title": "Vorwort",
    "content": "In den vorigen Schritten haben wir bereits einige interessante Bausteine kennengelernt. Als nächstes widmen wir uns dem Object Storage, der uns interessante Möglichkeiten bietet, Dateien zu speichern. ",
    "url": "/optimist/guided_tour/step23/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step23/#vorwort"
  },"195": {
    "doc": "23: Der Object Storage (S3 kompatibel)",
    "title": "Der Start (Benutzerdaten)",
    "content": "Damit wir auf den Object Storage zugreifen können, benötigen wir zunächst Login Daten(Credentials), um auf diesen zugreifen zu können. Dafür benötigen wir den OpenStackClienten(siehe Schritt 4), damit wir per OpenstackAPI die entsprechenden Daten erstellen. Der Befehl in der Kommandozeile dafür lautet: . openstack ec2 credentials create . Wenn die Daten korrekt erstellt worden sind, sieht die Ausgabe in etwa so aus: . $ openstack ec2 credentials create +------------+-----------------------------------------------------------------+ | Field | Value | +------------+-----------------------------------------------------------------+ | access | aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa | links | {u'self': u'https://identity.optimist.gec.io/v3/users/bbb | | bbbbbbbbbbbbbbbbbbbbbbbbbbbbb/credentials/OS- | | EC2/aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'} | project_id | cccccccccccccccccccccccccccccccc | secret | dddddddddddddddddddddddddddddddd | trust_id | None | user_id | bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb | +------------+-----------------------------------------------------------------+ . Nachdem die Zugangsdaten (Credentials) vorliegen, brauchen wir eine Möglichkeit auf den S3 kompatiblen ObjectStorage zuzugreifen. ",
    "url": "/optimist/guided_tour/step23/#der-start-benutzerdaten",
    
    "relUrl": "/optimist/guided_tour/step23/#der-start-benutzerdaten"
  },"196": {
    "doc": "23: Der Object Storage (S3 kompatibel)",
    "title": "Zugriff auf den S3 kompatiblen ObjectStorage",
    "content": "Es gibt natürlich verschiedene Optionen auf den ObjectStorage zuzugreifen. Wir empfehlen dafür die Nutzung von s3cmd Dieses kleine Tool ist einfach zu bedienen und zu nutzen. Da wir bereits in Schritt 4 “pip” als Paketmanager installiert haben und nutzen, können wir S3cmd auch über “pip” installieren: . pip install s3cmd . Da jetzt S3cmd installiert ist, müssen die vorher erstellten Zugangsdaten (Credentials) eingetragen werden, nur so lässt sich S3cmd auch korrekt nutzen. Alle wichtigen Informationen finden wir in der .s3cfg , sollte diese noch nicht existieren, erstellen wir diese vorher. Folgende Daten tragen wir dann in der .s3cfg ein: . access_key = aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa check_ssl_certificate = True check_ssl_hostname = True host_base = s3.es1.fra.optimist.gec.io host_bucket = s3.es1.fra.optimist.gec.io secret_key = dddddddddddddddddddddddddddddddd use_https = True . ",
    "url": "/optimist/guided_tour/step23/#zugriff-auf-den-s3-kompatiblen-objectstorage",
    
    "relUrl": "/optimist/guided_tour/step23/#zugriff-auf-den-s3-kompatiblen-objectstorage"
  },"197": {
    "doc": "23: Der Object Storage (S3 kompatibel)",
    "title": "Der Bucket",
    "content": "Da wir Zugriff auf den S3 kompatiblen Object Storage haben, ist es an der Zeit damit auch zu arbeiten. Alle verfügbaren Befehle von s3cmd können mit folgendem Befehl angezeigt werden: . s3cmd --help . Als Nächstes erstellen wir einen Bucket. Buckets entsprechen dabei im weitesten Sinne Ordnern, die wir für eine Struktur benötigen. Eine Datei kann also nur in einem existierenden Bucket gespeichert werden und der Name vom Bucket selber ist einzigartig (über den gesamten Optimist). Wenn also bereits ein Bucket mit dem Namen “Test” besteht, kann dieser nicht erneut angelegt werden. Daher ist es aus unserer Sicht eine gute Option, eine UUID zu nutzen und diese dann in der entsprechenden Applikation aufzulösen. Auch gibt es die Möglichkeit, bei Buckets und auch bei Dateien, zwischen public und private zu unterscheiden. Alle Buckets die erstellt und Dateien die hochgeladen werden, sind per default private, d.h. wenn keine weiteren Einstellungen vorgenommen werden, kann nur der Ersteller auf den Bucket und den Inhalt zugreifen. Dies lässt sich zum Beispiel per Access Control List (ACL) ändern. WICHTIG: Sollte man einen kompletten Bucket auf public stellen, können auch Informationen über Dateien, in diesem Bucket, die auf private gesetzt sind, abgerufen werden. Da wir die wichtigsten Details kennen, ist es Zeit, einen Bucket mit einer UUID zu erstellen: . $ s3cmd mb s3://e4d05df3-aa8e-4a37-b1b5-2745d189b189 Bucket 's3://e4d05df3-aa8e-4a37-b1b5-2745d189b189/' created . ",
    "url": "/optimist/guided_tour/step23/#der-bucket",
    
    "relUrl": "/optimist/guided_tour/step23/#der-bucket"
  },"198": {
    "doc": "23: Der Object Storage (S3 kompatibel)",
    "title": "Eine Datei hochladen",
    "content": "Da wir jetzt einen Bucket erstellt haben, ist der nächste Schritt, eine oder auch mehrere Dateien hochzuladen. Dafür nehmen wir den Befehl s3cmd put Dateiname s3://Name_des_Buckets und eine Ausgabe kann dann so aussehen: . $ s3cmd put test.yaml s3://e4d05df3-aa8e-4a37-b1b5-2745d189b189 upload: 'test.yaml' -&gt; 's3://e4d05df3-aa8e-4a37-b1b5-2745d189b189/test.yaml' [1 of 1] 4218 of 4218 100% in 0s 4.61 kB/s done . ",
    "url": "/optimist/guided_tour/step23/#eine-datei-hochladen",
    
    "relUrl": "/optimist/guided_tour/step23/#eine-datei-hochladen"
  },"199": {
    "doc": "23: Der Object Storage (S3 kompatibel)",
    "title": "Zugriff auf die Datei erhalten",
    "content": "Die generelle URL für den Zugriff auf Dateien lautet im Optimisten https://s3.es1.fra.optimist.gec.io/Name_des_Buckets/Dateiname. Damit auf die Datei aus unserem Beispiel zugegriffen werden kann, ist es notwendig, die Einstellung von private auf public zu ändern. Dafür können wir, wie bereits unter dem Punkt “Der Bucket” erwähnt, die Access Control List (ACL) nutzen: . $ s3cmd setacl s3://e4d05df3-aa8e-4a37-b1b5-2745d189b189/test.yaml --acl-public s3://e4d05df3-aa8e-4a37-b1b5-2745d189b189/test.yaml: ACL set to Public [1 of 1] . Die Datei kann jetzt über folgenden Link aufgerufen werden: https://s3.es1.fra.optimist.gec.io/e4d05df3-aa8e-4a37-b1b5-2745d189b189/test.yaml . Um sie wieder auf private zu stellen, nutzen wir folgenden Befehl: . $ s3cmd setacl s3://e4d05df3-aa8e-4a37-b1b5-2745d189b189/test.yaml --acl-private s3://e4d05df3-aa8e-4a37-b1b5-2745d189b189/test.yaml: ACL set to Private [1 of 1] . ",
    "url": "/optimist/guided_tour/step23/#zugriff-auf-die-datei-erhalten",
    
    "relUrl": "/optimist/guided_tour/step23/#zugriff-auf-die-datei-erhalten"
  },"200": {
    "doc": "23: Der Object Storage (S3 kompatibel)",
    "title": "Abschluss",
    "content": "In diesem Schritt haben wir den S3 kompatiblen Storage kennengelernt und die ersten Schritte im Umgang damit geübt. ",
    "url": "/optimist/guided_tour/step23/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step23/#abschluss"
  },"201": {
    "doc": "23: Der Object Storage (S3 kompatibel)",
    "title": "23: Der Object Storage (S3 kompatibel)",
    "content": " ",
    "url": "/optimist/guided_tour/step23/",
    
    "relUrl": "/optimist/guided_tour/step23/"
  },"202": {
    "doc": "Netzwerke",
    "title": "Netzwerke",
    "content": " ",
    "url": "/optimist/networking/",
    
    "relUrl": "/optimist/networking/"
  },"203": {
    "doc": "Port Forwarding auf Floating IPs",
    "title": "Port Forwarding auf Floating IPs",
    "content": "Floating IP Port Forwarding erlaubt die Weiterleitung eines beliebigen TCP/UDP/anderen Protokoll-Ports einer Floating IP-Adresse an einen TCP/UDP/anderen Protokoll-Port, der mit einer festen IP-Adresse eines Neutron-Ports verbunden ist. ",
    "url": "/optimist/networking/port_forwarding/",
    
    "relUrl": "/optimist/networking/port_forwarding/"
  },"204": {
    "doc": "Port Forwarding auf Floating IPs",
    "title": "Port Forwarding auf einer Floating IP erstellen",
    "content": "Um ein Port forwarding auf eine Floating IP anzuwenden, sind die folgenden Informationen erforderlich: . | Die zu verwendende interne IP-Adresse | Die UUID des Ports, der mit der Floating IP assoziiert werden soll | Die Portnummer des Netzwerkports der festen IPv4-Adresse | Die externe Portnummer der Floating IP-Adresse | Das spezifische Protokoll, das bei der Port-Weiterleitung zu verwenden ist (in diesem Beispiel TCP) | Die Floating IP, auf der dieser Port freigeschalten werden soll. (in diesem Beispiel 185.116.244.141) | . Das folgende Beispiel zeigt die Erstellung eines Port Forwarding auf einer Floating IP unter Verwendung der erforderlichen Optionen: . $ openstack floating ip port forwarding create \\ --internal-ip-address 10.0.0.14 \\ --port 12c29300-0f8a-4c54-a9dc-bee4c12c6ad2 \\ --internal-protocol-port 80 \\ --external-protocol-port 8080 \\ --protocol tcp 185.116.244.141 . ",
    "url": "/optimist/networking/port_forwarding/#port-forwarding-auf-einer-floating-ip-erstellen",
    
    "relUrl": "/optimist/networking/port_forwarding/#port-forwarding-auf-einer-floating-ip-erstellen"
  },"205": {
    "doc": "Port Forwarding auf Floating IPs",
    "title": "Anzeigen der Port Forwarding Einstellungen bestimmter Floating IPs",
    "content": "Innerhalb eines Projekts kann eine Liste der Port Forwarding-Regeln, die für eine bestimmte Floating IP gelten, mit dem folgenden Befehl abgerufen werden: . $ openstack floating ip port forwarding list 185.116.244.141 . Der obige Befehl kann weiter gefiltert werden, indem vor der Floating IP die Flags --sort-column, --port, --external-protcol-port und/oder --protocol verwendet werden. ",
    "url": "/optimist/networking/port_forwarding/#anzeigen-der-port-forwarding-einstellungen-bestimmter-floating-ips",
    
    "relUrl": "/optimist/networking/port_forwarding/#anzeigen-der-port-forwarding-einstellungen-bestimmter-floating-ips"
  },"206": {
    "doc": "Port Forwarding auf Floating IPs",
    "title": "Anzeigen der Details einer port forwarding-Regel",
    "content": "Um Details zu einer bestimmten Port Forwarding-Regel anzuzeigen, kann der folgende Befehl verwendet werden: . $ openstack floating ip port forwarding show &lt;floating-ip&gt; &lt;port-forwarding-id&gt; . ",
    "url": "/optimist/networking/port_forwarding/#anzeigen-der-details-einer-port-forwarding-regel",
    
    "relUrl": "/optimist/networking/port_forwarding/#anzeigen-der-details-einer-port-forwarding-regel"
  },"207": {
    "doc": "Port Forwarding auf Floating IPs",
    "title": "Ändern von Floating IP Port Forwarding-Eigenschaften",
    "content": "Wenn eine Port Forwarding-Konfiguration auf einer Floating IP bereits mit $ openstack floating ip port forwarding create erstellt wurde, können Änderungen an der bestehenden Konfiguration mit $ openstack floating ip port forwarding set ... vorgenommen werden. Die folgenden Parameter eines Port Forwardings können geändert werden: . | --port: Die UUID des Ports | --internal-ip-address: Die zum Zielport der Forwarding-Regel gehoerende feste interne IPv4-Adresse | --internal-protocol-port: Die interne TCP/UDP/etc. Portnummer auf die die Floating IPs Port Forwarding-Regel weiterleitet | --external-protocol-port: Die TCP/UDP/etc. Portnummer der Floating-IP-Adresse des Port Forwardings | --protocol: Das IP-Protokoll, das in der Floating IP Port Forwarding-Regel verwendet wird (TCP/UDP/andere) | --description: Text zur Beschreibung der Verwendung der Port Forwarding-Konfiguration | . Die Konfiguration jeder der oben genannten Parameter kann mit einer Variation des folgenden Befehls geändert werden: . $ openstack floating ip port forwarding set \\ --port &lt;port&gt; \\ --internal-ip-address &lt;internal-ip-address&gt; \\ --internal-protocol-port &lt;port-number&gt; \\ --extern-protocol-port &lt;port-number&gt; \\ --protocol &lt;protocol&gt; \\ --description &lt;description&gt; \\ &lt;Floating-ip&gt; &lt;port-forwarding-id&gt; . ",
    "url": "/optimist/networking/port_forwarding/#%C3%A4ndern-von-floating-ip-port-forwarding-eigenschaften",
    
    "relUrl": "/optimist/networking/port_forwarding/#ändern-von-floating-ip-port-forwarding-eigenschaften"
  },"208": {
    "doc": "Port Forwarding auf Floating IPs",
    "title": "Löschen der Port Forwarding-Konfiguration zu einer Floating IP",
    "content": "Um eine Port Forwarding-Regel von einer Floating IP zu entfernen, benötigen wir die folgenden Informationen: . | Die Floating IP dessen Port Forwarding-Regel entfernt werden soll | Die Port Forwarding ID (Diese ID wird bei der Erstellung erzeugt und kann mit dem Befehl $ openstack Floating ip port forwarding list ... angezeigt werden) | . Mit dem folgenden Befehl lässt sich die Konfiguration für ein Floating IP Port Forwarding löschen: . $ openstack floating ip port forwarding delete &lt;Floating-ip&gt; &lt;port-forwarding-id&gt; . ",
    "url": "/optimist/networking/port_forwarding/#l%C3%B6schen-der-port-forwarding-konfiguration-zu-einer-floating-ip",
    
    "relUrl": "/optimist/networking/port_forwarding/#löschen-der-port-forwarding-konfiguration-zu-einer-floating-ip"
  },"209": {
    "doc": "Geteilte Netzwerke",
    "title": "Geteilte Netzwerke",
    "content": " ",
    "url": "/optimist/networking/shared_networks/",
    
    "relUrl": "/optimist/networking/shared_networks/"
  },"210": {
    "doc": "Geteilte Netzwerke",
    "title": "Motivation",
    "content": "Es kommt oft die Frage auf, ob es möglich ist ein Netzwerk zwischen zwei Projekten im OpenStack zu teilen. ",
    "url": "/optimist/networking/shared_networks/#motivation",
    
    "relUrl": "/optimist/networking/shared_networks/#motivation"
  },"211": {
    "doc": "Geteilte Netzwerke",
    "title": "Netzwerk teilen",
    "content": "Zugriff auf beide Projekte ist vorhanden: . Damit das Netzwerk geteilt werden kann, brauchen wir zum einen den OpenStackClient, sowie die Projekt-ID in welches das Netzwerk geteilt werden soll und die Netzwerk-ID des zu teilenden Netzwerks. Die Projekt-ID findet sich in der Ausgabe unter “id” wenn wir folgenden Befehl benutzen: . openstack project show &lt;Name des Projekts&gt; -f value -c id . Als nächstes benötigen wir noch die Netzwerk-ID des zu teilenden Netzwerks, diese finden wir in der Ausgabe unter “id” wenn folgender Befehl genutzt wird: . openstack network show &lt;Name des Netzwerks&gt; -f value -c id . Mit den erhaltenen IDs kann nun das Netzwerk in das entsprechende Projekt geteilt werden, dafür benutzen wir die rollenbasierte Zugriffskontrolle (RBAC): . openstack network rbac create --type network --action access_as_shared --target-project &lt;ID des Projekts&gt; &lt;ID des zu teilenden Netzwerks&gt; . Zugriff auf beide Projekte ist nicht vorhanden: . In diesem Fall kann das Netzwerk nur vom Support, nach voriger Freigabe des anderen Projekt Inhabers, geteilt werden. Um ein Netzwerk mit einem Projekt zu teilen, schreiben Sie uns bitte eine E-Mail an support@gec.io mit folgenden Angaben: . | Name und ID des Netzwerks, welches geteilt werden soll | Name und ID des Projekts, in welchem das Netzwerk sichtbar sein soll | . ",
    "url": "/optimist/networking/shared_networks/#netzwerk-teilen",
    
    "relUrl": "/optimist/networking/shared_networks/#netzwerk-teilen"
  },"212": {
    "doc": "Geteilte Netzwerke",
    "title": "Wichtige Informationen zum geteilten Netzwerk",
    "content": "Wenn man auf ein geteiltes Netzwerk zugreift, gibt es Einschränkungen, die beachtet werden müssen. Eine Einschränkung ist, dass keine Remote Security-Groups benutzt werden können. Auch gibt es keinen Einblick in Ports und IP Adressen vom anderen Projekt gibt. Daher kann man auch keine konkrete IP Adressen für neue Ports in einem Subnetz (im geteilten Netzwerk) angeben, da es so möglich wäre, IPs zu finden die bereits genutzt werden. Damit man das geteilte Netzwerk sinnvoll nutzen kann, gibt es die Möglichkeit einen neuen Port zu erstellen. Dieser erhält dann eine beliebige IP-Adresse und kann weiter genutzt werden, um zum Beispiel einen Router über diesen Port hinzuzufügen. Im Dashboard (Horizon) ist dies nicht möglich und der OpenStackClient wird benötigt. Bitte achten Sie darauf bei den Bezeichnungen keine Leer- und/oder Sonderzeichen zu nutzen, da die Nutzung selbiger zu Problem führen kann. Zuerst erstellen wir den Port und geben dort das geteilte Netzwerk an: . openstack port create --network &lt;ID des geteilten Netzwerks&gt; &lt;Name des Ports&gt; . Jetzt kann zum Beispiel ein Router erstellt und dann dem neu erstellten Port zugeordnet werden: . ##Erstellung des Routers $ openstack router create &lt;Name des Routers&gt; ##Port dem Router zuordnen $ openstack router add port &lt;Name des Routers&gt; &lt;Name des Ports&gt; . ",
    "url": "/optimist/networking/shared_networks/#wichtige-informationen-zum-geteilten-netzwerk",
    
    "relUrl": "/optimist/networking/shared_networks/#wichtige-informationen-zum-geteilten-netzwerk"
  },"213": {
    "doc": "Geteilte Netzwerke",
    "title": "Netzwerk Topology Projekt 1",
    "content": ". Das Netzwerk “shared” aus dem Projekt 1 wird mit Projekt 2 geteilt. In diesem Netzwerk steht der Service “Example” zur Verfügung der dort auf einer Instanz läuft. ",
    "url": "/optimist/networking/shared_networks/#netzwerk-topology-projekt-1",
    
    "relUrl": "/optimist/networking/shared_networks/#netzwerk-topology-projekt-1"
  },"214": {
    "doc": "Geteilte Netzwerke",
    "title": "Netzwerk Topology Projekt 2",
    "content": ". Das Netzwerk “shared” ist auch in Projekt 2 sichtbar und wurde dort an den Router “router2” angehangen. Zusätzlich existiert dort das Netzwerk “network”, aus dem auf die Services in dem Netzwerk “shared” zugegriffen wird. Dabei muss berücksichtigt werden, das im Subnet des “shared” Networks in Projekt 1 die entsprechende Route unter dem Eintrag “Host Routes” gesetzt wird, um einen korrekten Rücktransport der Pakete zu ermöglichen. Im unserem Beispiel ist die folgende Route notwendig: 10.0.1.0/24,10.0.0.1 . ",
    "url": "/optimist/networking/shared_networks/#netzwerk-topology-projekt-2",
    
    "relUrl": "/optimist/networking/shared_networks/#netzwerk-topology-projekt-2"
  },"215": {
    "doc": "Octavia Loadbalancers",
    "title": "Der Octavia Loadbalancer",
    "content": " ",
    "url": "/optimist/networking/octavia_loadbalancer/#der-octavia-loadbalancer",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#der-octavia-loadbalancer"
  },"216": {
    "doc": "Octavia Loadbalancers",
    "title": "Vorwort",
    "content": "Octavia ist eine hochverfügbare und skalierbare Open-Source Load-Balancing-Lösung, die für die Arbeit mit OpenStack entwickelt wurde. Octavia erledigt das Load-Balancing nach Bedarf, indem es virtuelle Maschinen – auch Amphoren genannt – in seinem Projekt verwaltet und konfiguriert. In diesen Amphoren wirkt schlussendlich ein HAproxy. ",
    "url": "/optimist/networking/octavia_loadbalancer/#vorwort",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#vorwort"
  },"217": {
    "doc": "Octavia Loadbalancers",
    "title": "Der Start",
    "content": "Für die Nutzung von Octavia ist es notwendig, dass der Client auf dem eigenen System installiert ist. Eine Anleitung für sein System findet man unter Schritt 4) . ",
    "url": "/optimist/networking/octavia_loadbalancer/#der-start",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#der-start"
  },"218": {
    "doc": "Octavia Loadbalancers",
    "title": "Erstellung eines Octavia-Ladbalancer",
    "content": "Für unser Beispiel nutzen wir das aus Schritt 10 schon bestehende BeispielSubnet. $ openstack loadbalancer create --name Beispiel-LB --vip-subnet-id 32259126-dd37-44d5-922c-99d68ee870cd +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | admin_state_up | True | created_at | 2019-05-01T09:00:00 | description | | flavor_id | | id | e94827f0-f94d-40c7-a7fd-b91bf2676177 | listeners | | name | Beispiel-LB | operating_status | OFFLINE | pools | | project_id | b15cde70d85749689e08106f973bb002 | provider | amphora | provisioning_status | PENDING_CREATE | updated_at | None | vip_address | 10.0.0.10 | vip_network_id | f2a8f00e-204b-4c37-9d19-1d5c8e4efbf6 | vip_port_id | 37fc5b34-ee07-49c8-b054-a8d591a9679f | vip_qos_policy_id | None | vip_subnet_id | 32259126-dd37-44d5-922c-99d68ee870cd | +---------------------+--------------------------------------+ . Nun geht Octavia her und spawned seine Amphoreninstanzen im Hintergrund. $ openstack loadbalancer list +--------------------------------------+-------------+----------------------------------+--------------+---------------------+----------+ | id | name | project_id | vip_address | provisioning_status | provider | +--------------------------------------+-------------+----------------------------------+--------------+---------------------+----------+ | e94827f0-f94d-40c7-a7fd-b91bf2676177 | Beispiel-LB | b15cde70d85749689e08106f973bb002 | 10.0.0.10 | PENDING_CREATE | amphora | +--------------------------------------+-------------+----------------------------------+--------------+---------------------+----------+ . Mit dem provisioning_status ACTIVE ist dieser Vorgang erfolgreich abgeschlossen und der erste Octavia-Loadbalancer kann weiter konfiguiert werden. $ openstack loadbalancer list +--------------------------------------+-------------+----------------------------------+--------------+---------------------+----------+ | id | name | project_id | vip_address | provisioning_status | provider | +--------------------------------------+-------------+----------------------------------+--------------+---------------------+----------+ | e94827f0-f94d-40c7-a7fd-b91bf2676177 | Beispiel-LB | b15cde70d85749689e08106f973bb002 | 10.0.0.10 | ACTIVE | amphora | +--------------------------------------+-------------+----------------------------------+--------------+---------------------+----------+ . ",
    "url": "/optimist/networking/octavia_loadbalancer/#erstellung-eines-octavia-ladbalancer",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#erstellung-eines-octavia-ladbalancer"
  },"219": {
    "doc": "Octavia Loadbalancers",
    "title": "Erstellen eines LB-Listener",
    "content": "In unserem Beispiel wollen wir einen Listener für den HTTP-Port 80 erstellen. Als Listener ist hier - vergleichbar mit anderen LB-Lösungen - der Port des Frontends gemeint. $ openstack loadbalancer listener create --name Beispiel-listener --protocol HTTP --protocol-port 80 Beispiel-LB +-----------------------------+--------------------------------------+ | Field | Value | +-----------------------------+--------------------------------------+ | admin_state_up | True | connection_limit | -1 | created_at | 2019-05-01T09:00:00 | default_pool_id | None | default_tls_container_ref | None | description | | id | 0a3312d1-8cf7-41a8-8d24-181246468cd7 | insert_headers | None | l7policies | | loadbalancers | e94827f0-f94d-40c7-a7fd-b91bf2676177 | name | Beispiel-listener | operating_status | OFFLINE | project_id | b15cde70d85749689e08106f973bb002 | protocol | HTTP | protocol_port | 80 | provisioning_status | PENDING_CREATE | sni_container_refs | [] | timeout_client_data | 50000 | timeout_member_connect | 5000 | timeout_member_data | 50000 | timeout_tcp_inspect | 0 | updated_at | None | client_ca_tls_container_ref | | client_authentication | | client_crl_container_ref | +-----------------------------+--------------------------------------+ . Der Befehl war erfolgreich, wenn der admin_state_up True ist. $ openstack loadbalancer listener list +--------------------------------------+-----------------+-------------------+----------------------------------+----------+---------------+----------------+ | id | default_pool_id | name | project_id | protocol | protocol_port | admin_state_up | +--------------------------------------+-----------------+-------------------+----------------------------------+----------+---------------+----------------+ | 0a3312d1-8cf7-41a8-8d24-181246468cd7 | None | Beispiel-listener | b15cde70d85749689e08106f973bb002 | HTTP | 80 | True | +--------------------------------------+-----------------+-------------------+----------------------------------+----------+---------------+----------------+ . ",
    "url": "/optimist/networking/octavia_loadbalancer/#erstellen-eines-lb-listener",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#erstellen-eines-lb-listener"
  },"220": {
    "doc": "Octavia Loadbalancers",
    "title": "Erstellen eines LB-Pools",
    "content": "Als LB-Pool ist hier eine Ansammlung aller Objekte (Listeners, Member, etc.) für zum Beispiel eine Region gemeint - vergleichbar mit einem Pool an öffentlichen IP-Adressen aus derer man sich eine belegen kann. Einen Pool für unser Beispiel erstellt man wie folgt: . $ openstack loadbalancer pool create --name Beispiel-pool --lb-algorithm ROUND_ROBIN --listener Beispiel-listener --protocol HTTP +----------------------+--------------------------------------+ | Field | Value | +----------------------+--------------------------------------+ | admin_state_up | True | created_at | 2019-05-01T09:00:00 | description | | healthmonitor_id | | id | 4053e88e-c2b5-47c6-987e-4387d837c88d | lb_algorithm | ROUND_ROBIN | listeners | 0a3312d1-8cf7-41a8-8d24-181246468cd7 | loadbalancers | e94827f0-f94d-40c7-a7fd-b91bf2676177 | members | | name | Beispiel-pool | operating_status | OFFLINE | project_id | b15cde70d85749689e08106f973bb002 | protocol | HTTP | provisioning_status | PENDING_CREATE | session_persistence | None | updated_at | None | tls_container_ref | | ca_tls_container_ref | | crl_container_ref | | tls_enabled | +----------------------+--------------------------------------+ . Hier sei erwähnt, dass man mit openstack loadbalancer pool create --help sich alle möglichen Einstellungen anzeigen lassen kann. Die häufigsten Einstellungen und deren Auswahlmöglichkeiten: . --protocol: {TCP,HTTP,HTTPS,TERMINATED_HTTPS,PROXY,UDP} --lb-algorithm {SOURCE_IP,ROUND_ROBIN,LEAST_CONNECTIONS} . Der Pool ist erfolgreich erstellt, wenn der provisioning_status den Status ACTIVE erreicht hat. $ openstack loadbalancer pool list +--------------------------------------+---------------+----------------------------------+---------------------+----------+--------------+----------------+ | id | name | project_id | provisioning_status | protocol | lb_algorithm | admin_state_up | +--------------------------------------+---------------+----------------------------------+---------------------+----------+--------------+----------------+ | 4053e88e-c2b5-47c6-987e-4387d837c88d | Beispiel-pool | b15cde70d85749689e08106f973bb002 | ACTIVE | HTTP | ROUND_ROBIN | True | +--------------------------------------+---------------+----------------------------------+---------------------+----------+--------------+----------------+ . ",
    "url": "/optimist/networking/octavia_loadbalancer/#erstellen-eines-lb-pools",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#erstellen-eines-lb-pools"
  },"221": {
    "doc": "Octavia Loadbalancers",
    "title": "Erstellen der LB-member",
    "content": "Damit unser Loadbalancer weiß, an welche Backends er weiterleiten darf, fehlen uns noch sogenannte member, die wir wie folgt definieren: . $ openstack loadbalancer member create --subnet-id 32259126-dd37-44d5-922c-99d68ee870cd --address 10.0.0.11 --protocol-port 80 Beispiel-pool +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | address | 10.0.0.11 | admin_state_up | True | created_at | 2019-05-01T09:00:00 | id | 703e27e0-e7fe-474b-b32d-68f9a8aeef07 | name | | operating_status | NO_MONITOR | project_id | b15cde70d85749689e08106f973bb002 | protocol_port | 80 | provisioning_status | PENDING_CREATE | subnet_id | 32259126-dd37-44d5-922c-99d68ee870cd | updated_at | None | weight | 1 | monitor_port | None | monitor_address | None | backup | False | +---------------------+--------------------------------------+ . und . $ openstack loadbalancer member create --subnet-id 32259126-dd37-44d5-922c-99d68ee870cd --address 10.0.0.12 --protocol-port 80 Beispiel-pool +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | address | 10.0.0.12 | admin_state_up | True | created_at | 2019-05-01T09:00:00 | id | 2add1e17-73a6-4002-82af-538a3374e5dc | name | | operating_status | NO_MONITOR | project_id | b15cde70d85749689e08106f973bb002 | protocol_port | 80 | provisioning_status | PENDING_CREATE | subnet_id | 32259126-dd37-44d5-922c-99d68ee870cd | updated_at | None | weight | 1 | monitor_port | None | monitor_address | None | backup | False | +---------------------+--------------------------------------+ . Hier sei erwähnt, dass die beiden IP’s aus 10.0.0.* bereits vorhandene, auf Port 80 lauschende Webserver sind, die eine einfache Webseite mit Info’s über ihren Servicenamen ausliefern. Unter der Vorraussetzung es handelt sich bei diesen Webservern im folgenden Beispiel um ein Ubuntu/Debian und man hat root-Berechtigungen, könnte man die einfache Webseite schnell erstellen mit: . root@BeispielInstanz1:~# apt-get update; apt-get -y install apache2; echo \"you hit: you hit: webserver1\" &gt; /var/www/html/index.html . root@BeispielInstanz2:~# apt-get update; apt-get -y install apache2; echo \"you hit: you hit: webserver2\" &gt; /var/www/html/index.html . Das Resultat vom Anlegen der Member können wir wie folgt überprüfen: . $ openstack loadbalancer member list Beispiel-pool +--------------------------------------+------+----------------------------------+---------------------+--------------+---------------+------------------+--------+ | id | name | project_id | provisioning_status | address | protocol_port | operating_status | weight | +--------------------------------------+------+----------------------------------+---------------------+--------------+---------------+------------------+--------+ | 703e27e0-e7fe-474b-b32d-68f9a8aeef07 | b15cde70d85749689e08106f973bb002 | ACTIVE | 10.0.0.11 | 80 | NO_MONITOR | 1 | 2add1e17-73a6-4002-82af-538a3374e5dc | b15cde70d85749689e08106f973bb002 | ACTIVE | 10.0.0.12 | 80 | NO_MONITOR | 1 | +--------------------------------------+------+----------------------------------+---------------------+--------------+---------------+------------------+--------+ . Nun ist das “interne” Konstrukt des Loadbalancers konfiguriert. Wir haben nun: . | 2 member die über Port 80 den tatsächlichen Service bereitstellen und zwischen denen das Loadbalancing stattfindet, | einen pool für diese member, | einen listener, der auf Port TCP/80 lauscht und ein ROUND_ROBIN zu den beiden Endpunkten macht und | einen Loadbalancer, über den wir alle Komponenten vereint haben. | . Der operating_status NO_MONITOR wird unter healthmonitor korrigiert. ",
    "url": "/optimist/networking/octavia_loadbalancer/#erstellen-der-lb-member",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#erstellen-der-lb-member"
  },"222": {
    "doc": "Octavia Loadbalancers",
    "title": "Erstellen und konfigurieren der Floating-IP",
    "content": "Damit wir auch den Loadbalancer außerhalb unseres Beispiel-Netzwerk einsetzen können, müssen wir eine FloatingIP reservieren und diese dann mit dem vip_port_id des Beispiel-LB verknüpfen. Mit folgendem Befehl erstellen wir uns eine Floating IP aus dem provider-Netz: . $ openstack floating ip create provider +---------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +---------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | created_at | 2019-05-01T09:00:00Z | description | | dns_domain | None | dns_name | None | fixed_ip_address | None | floating_ip_address | 185.116.247.133 | floating_network_id | 54258498-a513-47da-9369-1a644e4be692 | id | 46c0e8cf-783d-44a0-8256-79f8ae0be7fe | location | Munch({'project': Munch({'domain_id': 'default', 'id': u'b15cde70d85749689e08106f973bb002', 'name': 'beispiel-tenant', 'domain_name': None}), 'cloud': '', 'region_name': 'fra', 'zone': None}) | name | 185.116.247.133 | port_details | None | port_id | None | project_id | b15cde70d85749689e08106f973bb002 | qos_policy_id | None | revision_number | 0 | router_id | None | status | DOWN | subnet_id | None | tags | [] | updated_at | 2019-05-01T09:00:00Z | +---------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ . Im nächsten Schritt benötigen wir die vip_port_id des Loadbalancers. Diese bekommt man mit folgendem Befehl heraus: . $ openstack loadbalancer show Beispiel-LB -f value -c vip_port_id 37fc5b34-ee07-49c8-b054-a8d591a9679f . Mit dem folgendem Befehl weisen wir dem Loadbalancer nun die öffentliche IP Adresse zu. Damit ist der LB (und somit auch die Endpunkte dahinter) aus dem Internet erreichbar. openstack floating ip set --port 37fc5b34-ee07-49c8-b054-a8d591a9679f 185.116.247.133 . Wir sind soweit, dass wir unser Loadbalancer-Deployment testen können. Mit folgendem Befehl fragen wir unseren Loadbalancer über Port TCP/80 an und bekommen anschließend eine entsprechende Antwort von den einzelnen member zurück: . $ for ((i=1;i&lt;=10;i++)); do curl http://185.116.247.133; sleep 1; done you hit: webserver1 you hit: webserver2 you hit: webserver1 you hit: webserver2 you hit: webserver1 you hit: webserver2 you hit: webserver1 ... (usw.) . ",
    "url": "/optimist/networking/octavia_loadbalancer/#erstellen-und-konfigurieren-der-floating-ip",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#erstellen-und-konfigurieren-der-floating-ip"
  },"223": {
    "doc": "Octavia Loadbalancers",
    "title": "Erstellen eines healthmonitor",
    "content": "Mit dem folgenden Befehl erstellen wir einen Monitor, der bei einem Ausfall eines der Backends genau dieses fehlerhafte Backend aus der Lastverteilung nimmt und somit die Webseite oder Applikation weiterhin sauber ausgeliefert wird. $ openstack loadbalancer healthmonitor create --delay 5 --max-retries 2 --timeout 10 --type HTTP --name Beispielmonitor --url-path / Beispiel-pool +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | project_id | b15cde70d85749689e08106f973bb002 | name | Beispielmonitor | admin_state_up | True | pools | 4053e88e-c2b5-47c6-987e-4387d837c88d | created_at | 2019-05-01T09:00:00 | provisioning_status | PENDING_CREATE | updated_at | None | delay | 5 | expected_codes | 200 | max_retries | 2 | http_method | GET | timeout | 10 | max_retries_down | 3 | url_path | / | type | HTTP | id | 368f9baa-708c-4e7f-ace1-02de15598c5d | operating_status | OFFLINE | http_version | | domain_name | +---------------------+--------------------------------------+ . In diesem Beispiel entfernt der Monitor das fehlerhafte Backend aus dem Pool, wenn die Integritätsprüfung (–type HTTP, –url-path / ) alle zwei Fünf-Sekunden-Intervalle fehlschlägt(–delay 5, –max-retries 2, –timeout 10). Wenn der Server wiederhergestellt wird und erneut auf TCP/80 reagiert, wird er erneut zum Pool hinzugefügt. Ein manueller Failover kann erzwungen werden, indem der Statuscodes des Webservers ungleich “200” ist, oder gar keine Antwort des Webservers erfolgt. $ openstack loadbalancer healthmonitor show Beispielmonitor +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | project_id | b15cde70d85749689e08106f973bb002 | name | Beispielmonitor | admin_state_up | True | pools | 4053e88e-c2b5-47c6-987e-4387d837c88d | created_at | 2019-05-01T09:00:00 | provisioning_status | ACTIVE | updated_at | 2019-05-01T09:00:00 | delay | 5 | expected_codes | 200 | max_retries | 2 | http_method | GET | timeout | 10 | max_retries_down | 3 | url_path | / | type | HTTP | id | 368f9baa-708c-4e7f-ace1-02de15598c5d | operating_status | ONLINE | http_version | | domain_name | +---------------------+--------------------------------------+ . Aus unserem Deployment-Beispiel würde das Resultat ungefähr so aussehen: . you hit: webserver1 Mi 22 Mai 2019 17:09:39 CEST you hit: webserver2 Mi 22 Mai 2019 17:09:40 CEST you hit: webserver1 Mi 22 Mai 2019 17:09:41 CEST you hit: webserver2 Mi 22 Mai 2019 17:09:42 CEST you hit: webserver1 Mi 22 Mai 2019 17:09:43 CEST - bis hier waren beide Webserver online, doch nun ist der webserver2 offline gegangen. you hit: webserver1 Mi 22 Mai 2019 17:09:44 CEST - noch erwarteter hit durch ROUND_ROBIN you hit: webserver1 Mi 22 Mai 2019 17:09:50 CEST - 1. retry zu webserver2 schlägt fehl you hit: webserver1 Mi 22 Mai 2019 17:09:56 CEST - 2. retry zu webserver2 schlägt fehl you hit: webserver1 Mi 22 Mai 2019 17:10:01 CEST - das Backend (webserver2) wurde aus dem Pool genommen. you hit: webserver1 Mi 22 Mai 2019 17:10:02 CEST you hit: webserver1 Mi 22 Mai 2019 17:10:03 CEST you hit: webserver1 Mi 22 Mai 2019 17:10:04 CEST you hit: webserver1 Mi 22 Mai 2019 17:10:05 CEST you hit: webserver1 Mi 22 Mai 2019 17:10:06 CEST . ",
    "url": "/optimist/networking/octavia_loadbalancer/#erstellen-eines-healthmonitor",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#erstellen-eines-healthmonitor"
  },"224": {
    "doc": "Octavia Loadbalancers",
    "title": "Monitoring mit Prometheus",
    "content": "Der Octavia Amphora-Driver bietet einen Prometheus-Endpunkt. Auf diese Weise können Sie Metriken von Octavia-Loadbalancern sammeln. Um einen Prometheus-Endpunkt zu einem vorhandenen Octavia-Load-Balancer hinzuzufügen, erstellen Sie einen Listener mit dem Protokoll PROMETHEUS. Dadurch wird der Endpunkt als /metrics auf dem Listener aktiviert. Der Listener unterstützt alle Funktionen eines Octavia-Load-Balancers, z. B. allowed_cidrs, unterstützt jedoch nicht das Anhängen von Pools oder L7-Richtlinien. Alle Metriken werden durch die Octavia-Objekt-ID (UUID) der Ressourcen identifiziert. Hinweis: Derzeit werden UDP- und SCTP-Metriken nicht über Prometheus-Endpunkte gemeldet, wenn der Amphora-Provider verwendet wird. Um beispielsweise einen Prometheus-Endpunkt auf Port 8088 für den Load Balancer lb1 zu erstellen, führen Sie den folgenden Befehl aus: . $ openstack loadbalancer listener create --name stats-listener --protocol PROMETHEUS --protocol-port 8088 lb1 +-----------------------------+--------------------------------------+ | Field | Value | +-----------------------------+--------------------------------------+ | admin_state_up | True | connection_limit | -1 | created_at | 2021-10-03T01:44:25 | default_pool_id | None | default_tls_container_ref | None | description | | id | fb57d764-470a-4b6b-8820-627452f55b96 | insert_headers | None | l7policies | | loadbalancers | b081ed89-f6f8-48cb-a498-5e12705e2cf9 | name | stats-listener | operating_status | OFFLINE | project_id | 4c1caeee063747f8878f007d1a323b2f | protocol | PROMETHEUS | protocol_port | 8088 | provisioning_status | PENDING_CREATE | sni_container_refs | [] | timeout_client_data | 50000 | timeout_member_connect | 5000 | timeout_member_data | 50000 | timeout_tcp_inspect | 0 | updated_at | None | client_ca_tls_container_ref | None | client_authentication | NONE | client_crl_container_ref | None | allowed_cidrs | None | tls_ciphers | None | tls_versions | None | alpn_protocols | None | tags | +-----------------------------+--------------------------------------+ . Sobald der PROMETHEUS-Listener AKTIV ist, können Sie Prometheus so konfigurieren, dass es Metriken vom Load Balancer sammelt, indem Sie die Datei prometheus.yml aktualisieren. [scrape_configs] - job_name: 'Octavia LB1' static_configs: - targets: ['192.0.2.10:8088'] . ",
    "url": "/optimist/networking/octavia_loadbalancer/#monitoring-mit-prometheus",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#monitoring-mit-prometheus"
  },"225": {
    "doc": "Octavia Loadbalancers",
    "title": "Bekannte Probleme",
    "content": "Wenn sie bei der Zuweisung der öffentliche IP Adresse zum Loadbalancer folgenden Fehler bekommen: . ResourceNotFound: 404: Client Error for url: https://network.fra.optimist.gec.io/v2.0/floatingips/46c0e8cf-783d-44a0-8256-79f8ae0be7fe, External network 54258498-a513-47da-9369-1a644e4be692 is not reachable from subnet 32259126-dd37-44d5-922c-99d68ee870cd. Therefore, cannot associate Port 37fc5b34-ee07-49c8-b054-a8d591a9679f with a Floating IP. dann fehlt eine Verbindung zwischen ihrem Beispiel-Netz (Router) und dem Provider-Netz (Schritt 10) . Die default-connect-Einstellung der haproxy-Prozesse innerhalb einer Amphore liegen bei 50 Sekunden, d.h. wenn eine Verbindung länger als 50 Sekunden anhalten soll, müssen sie am Listener diese Werte entsprechend konfigurieren. Beispiel für einen Connect mit Timeout: . $ time kubectl -n kube-system exec -ti machine-controller-5f649c5ff4-pksps /bin/sh ~ $ 50.69 real 0.08 user 0.05 sys . Um in diesem Beispiel den Timeout auf 4h zu erweitern: . openstack loadbalancer listener set --timeout_client_data 14400000 &lt;Listener ID&gt; openstack loadbalancer listener set --timeout_member_data 14400000 &lt;Listener ID&gt; . Wenn Octavia versucht, einen LB mit port_security_enabled = False in einem Netzwerk zu starten, wird der LB in den Status ERROR versetzt. ",
    "url": "/optimist/networking/octavia_loadbalancer/#bekannte-probleme",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#bekannte-probleme"
  },"226": {
    "doc": "Octavia Loadbalancers",
    "title": "Abschluss",
    "content": "Es macht durchaus Sinn immer einen Monitor für seinen Pool zu etablieren. ",
    "url": "/optimist/networking/octavia_loadbalancer/#abschluss",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#abschluss"
  },"227": {
    "doc": "Octavia Loadbalancers",
    "title": "Octavia Loadbalancers",
    "content": " ",
    "url": "/optimist/networking/octavia_loadbalancer/",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/"
  },"228": {
    "doc": "VPN as a Service",
    "title": "VPN as a Service (VPNaaS)",
    "content": "OpenStack unterstützt bei Bedarf Site-to-Site VPNs as a Service. Damit können Benutzer zwei private Netzwerke miteinander verbinden. Dazu werden von OpenStack voll funktionale IPsec VPNs innerhalb eines Projekts konfiguriert, ohne dass weitere netzwerkfähige VMs benötigt werden. ",
    "url": "/optimist/networking/vpnaas/#vpn-as-a-service-vpnaas",
    
    "relUrl": "/optimist/networking/vpnaas/#vpn-as-a-service-vpnaas"
  },"229": {
    "doc": "VPN as a Service",
    "title": "Anlegen eines Site-to-Site IPSec VPN",
    "content": "Erzeugen von “linken” und “rechten” Netzwerken und Subnetzen . Bevor wir ein VPN anlegen können, benötigen wir zwei getrennte Netzwerke, die miteinander verbunden werden sollen. In dieser Anleitung erzeugen wir diese Netzwerke in zwei verschiedenen OpenStack Projekten und nennen sie “links” (left) und “rechts” (right). Die folgenden Schritte müssen für beide Netzwerke (“links” und “rechts”) durchgeführt werden, damit die zwei verschiedenen OpenStack Cluster miteinander verbunden werden können. Der Einfachheit halber zeigen wir in dieser Anleitung nur, wie wir das linke Netzwerk erzeugen. Die Schritte für das rechte Netzwerk sind bis auf den Namen und das Subnetz-Präfix für OpenStack identisch. In diesem Beispiel verwenden wir das Subnetz-Präfix 2001:db8:1:33bc::/64 für das linke Netzwerk und 2001:db8:1:33bd::/64 für das rechte Netzwerk. Falls Sie bereits über zwei Netzwerke verfügen, die Sie über einen Site-to-Site VPN verbinden möchten, können Sie den Schritt Erzeugen von IKE und IPSec Policies auf beiden Seiten überspringen. Verwenden von Horizon (GUI) . | Erzeugen Sie das linke Netzwerk mit einem neuen Subnetz. | . Navigieren Sie innerhalb Ihres Projekts zu Network → Networks und klicken Sie auf Create Network. Geben Sie dem neuen Netzwerk einen Namen und wählen Sie Enable Admin State , um das Netzwerk zu aktivieren. Wählen Sie anschließend Create Subnet aus, um das Netzwerk und Subnetz in einem Schritt zu erzeugen. Klicken Sie danach auf Next. Geben Sie Ihrem neuen Netzwerk-Subnetz einen Namen und wählen Sie Enter Network Address manually aus. Falls Sie Ihr eigenes Subnetz verwenden möchten, geben Sie Ihr gewünschtes Subnetz in Network Address ein. Falls Sie ein Subnetz von einem vordefinierten Pool verwenden möchten, wählen Sie Allocate Network Address from a pool und wählen Sie einen Pool aus. Klicken Sie anschließend auf Next. Für Dokumentationszwecke verwenden wir die vorher genannten Präfixe. Wählen Sie Enable DHCP und unter IPv6 Address Configuration Mode DHCPV6 STATEFUL aus. Die Allokationspools werden automatisch erzeugt. Klicken Sie auf Create. Verwenden von CLI . | Erzeugen Sie das linke Netzwerk mit dem Befehl openstack network create. | . $ openstack network create vpnaas-left-network +---------------------------+--------------------------------------+ | Field | Value | +---------------------------+--------------------------------------+ | admin_state_up | UP | availability_zone_hints | | availability_zones | | created_at | 2022-09-12T12:45:42Z | description | | dns_domain | | id | ff7c61f1-4dcb-49bf-be9f-efdcaa1e0aaa | ipv4_address_scope | None | ipv6_address_scope | None | is_default | False | is_vlan_transparent | None | mtu | 1500 | name | vpnaas-left-network | port_security_enabled | True | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | provider:network_type | None | provider:physical_network | None | provider:segmentation_id | None | qos_policy_id | None | revision_number | 1 | router:external | Internal | segments | None | shared | False | status | ACTIVE | subnets | | tags | | updated_at | 2022-09-12T12:45:42Z | +---------------------------+--------------------------------------+ . | Erzeugen Sie ein neues Subnetz und weisen Sie es mit dem Befehl openstack subnet create dem neuen Netzwerk zu. | . $ openstack subnet create \\ vpnaas-left-network-subnet \\ --subnet-range 2001:db8:1:33bc::/64 --ip-version 6 \\ --network vpnaas-left-network +----------------------+--------------------------------------------------------+ | Field | Value | +----------------------+--------------------------------------------------------+ | allocation_pools | 2001:db8:1:33bc::1-2001:db8:1:33bc:ffff:ffff:ffff:ffff | cidr | 2001:db8:1:33bc::/64 | created_at | 2022-09-12T12:47:51Z | description | | dns_nameservers | | dns_publish_fixed_ip | None | enable_dhcp | True | gateway_ip | 2001:db8:1:33bc:: | host_routes | | id | e217a377-48c7-4c18-93b5-cfd805bde40a | ip_version | 6 | ipv6_address_mode | None | ipv6_ra_mode | None | name | vpnaas-left-network-subnet | network_id | ff7c61f1-4dcb-49bf-be9f-efdcaa1e0aaa | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | revision_number | 0 | segment_id | None | service_types | | subnetpool_id | None | tags | | updated_at | 2022-09-12T12:47:51Z | +----------------------+--------------------------------------------------------+ . Erzeugen des linken und rechten Routers . Verwenden von Horizon (GUI) . | Erzeugen Sie einen Router mit dem Provider-Netzwerk als externes Gateway. | . Navigieren Sie innerhalb Ihres Projekts zu Network → Routers und klicken Sie auf Create Router. Geben Sie dem neuen Router einen Namen. Wählen Sie Enable Admin State, um den Router zu aktivieren. Wählen Sie “PROVIDER” als External Network und klicken Sie auf Create Router. Verwenden von CLI . | Erzeugen Sie einen Router mit dem Befehl openstack router create. | . $ openstack router create vpnaas-left-router +-------------------------+--------------------------------------+ | Field | Value | +-------------------------+--------------------------------------+ | admin_state_up | UP | availability_zone_hints | | availability_zones | | created_at | 2022-09-12T12:48:15Z | description | | enable_ndp_proxy | None | external_gateway_info | null | flavor_id | None | id | 052e968a-a63b-4824-b904-eb70c42c53e5 | name | vpnaas-left-router | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | revision_number | 2 | routes | | status | ACTIVE | tags | | tenant_id | 281fa14f782e4d4cbfd4e34a121c2680 | updated_at | 2022-09-12T12:48:15Z | +-------------------------+--------------------------------------+ . | Verwenden Sie den Befehl openstack router set, um das Provider-Netzwerk als externes Gateway für den Router anzulegen. | . $ openstack router set vpnaas-left-router --external-gateway provider . Zuordnen des Subnetzes an den Router . Verwenden von Horizon (GUI) . Navigieren Sie innerhalb Ihres Projekts zu Network → Routers und wählen Sie den zuvor erzeugten Router aus. Wählen Sie Interfaces und klicken Sie auf Add Interface. Wählen Sie Ihr Subnetz aus und klicken Sie auf Submit. Verwenden von CLI . Verwenden Sie den Befehl openstack router add subnet, um das Subnetz mit dem Router zu verbinden. $ openstack router add subnet vpnaas-left-router vpnaas-left-network-subnet . Erzeugen von IKE und IPSec Policies auf beiden Seiten . Die IKE und IPSec Policies müsssen auf beiden Seiten identisch konfiguriert werden. In dieser Anleitung verwenden wir die folgenden Parameter: . | Parameter | IKE Policy | IPSec Policy | . | Authorization algorithm | SHA256 | SHA256 | . | Encryption algorithm | AES-256 | AES-256 | . | Encapsulation mode | N/A | TUNNEL | . | IKE Version | V2 | N/A | . | Perfect Forward Secrecy | GROUP14 | GROUP14 | . | Transform Protocol | N/A | ESP | . Verwenden von Horizon (GUI) . | Erzeugen Sie die IKE Policy. | . Navigieren Sie innerhalb Ihres Projekts zu Network → VPN, wählen Sie IKE Policies aus und klicken Sie auf Add IKE Policy. Geben Sie Ihrer IKE Policy einen Namen und geben Sie die IKE Policy Parameter ein. Klicken Sie anschließend auf Add. | Erzeugen Sie die IPSec Policy. | . Sie befinden sich noch immer innerhalb von Network → VPN. Wählen Sie IPSec Policies aus und klicken Sie auf Add IPsec Policy. Geben Sie Ihrer IPSec Policy einen Namen und geben Sie die IPSec Policy Parameter ein. Klicken Sie anschließend auf Add. Verwenden von CLI . | Erzeugen Sie die IKE Policy mit dem Befehl openstack vpn ike policy create. | . $ openstack vpn ike policy create \\ vpnaas-left-ike-policy \\ --auth-algorithm sha256 \\ --encryption-algorithm aes-256 \\ --ike-version v2 \\ --pfs group14 +-------------------------------+--------------------------------------+ | Field | Value | +-------------------------------+--------------------------------------+ | Authentication Algorithm | sha256 | Description | | Encryption Algorithm | aes-256 | ID | 561387b8-b5c1-415e-abc9-79ba93dd48ff | IKE Version | v2 | Lifetime | {'units': 'seconds', 'value': 3600} | Name | vpnaas-left-ike-policy | Perfect Forward Secrecy (PFS) | group14 | Phase1 Negotiation Mode | main | Project | 281fa14f782e4d4cbfd4e34a121c2680 | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | +-------------------------------+--------------------------------------+ . | Erzeugen Sie die IPSec Policy mit dem Befehl openstack vpn ipsec policy create. | . $ openstack vpn ipsec policy create \\ vpnaas-left-ipsec-policy \\ --auth-algorithm sha256 \\ --encryption-algorithm aes-256 \\ --pfs group14 \\ --transform-protocol esp +-------------------------------+--------------------------------------+ | Field | Value | +-------------------------------+--------------------------------------+ | Authentication Algorithm | sha256 | Description | | Encapsulation Mode | tunnel | Encryption Algorithm | aes-256 | ID | 553a600e-f39d-47a0-9550-97f2b4033685 | Lifetime | {'units': 'seconds', 'value': 3600} | Name | vpnaas-left-ipsec-policy | Perfect Forward Secrecy (PFS) | group14 | Project | 281fa14f782e4d4cbfd4e34a121c2680 | Transform Protocol | esp | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | +-------------------------------+--------------------------------------+ . Erzeugen der VPN Services auf beiden Seiten . Verwenden von Horizon (GUI) . Navigieren Sie innerhalb Ihres Projekts zu Network → VPN. Wählen Sie VPN Services aus und klicken Sie auf Add VPN Service. Geben Sie Ihrem VPN Service einen Namen. Wählen Sie Ihren Router aus und selektieren Sie Enable Admin State. Ein Subnetz wird nicht benötigt, da wir die Endpoint Gruppen verwenden. Klicken Sie anschließend auf Add. Verwenden von CLI . Verwenden Sie den Befehl openstack vpn service create, um den VPN Service zu erzeugen. $ openstack vpn service create vpnaas-left-vpn --router vpnaas-left-router +----------------+--------------------------------------+ | Field | Value | +----------------+--------------------------------------+ | Description | | Flavor | None | ID | cc258fd7-0e87-4058-ad7d-355f32c1ab5e | Name | vpnaas-left-vpn | Project | 281fa14f782e4d4cbfd4e34a121c2680 | Router | 052e968a-a63b-4824-b904-eb70c42c53e5 | State | True | Status | PENDING_CREATE | Subnet | None | external_v4_ip | 185.116.244.85 | external_v6_ip | 2a00:c320:1003::23a | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | +----------------+--------------------------------------+ . Erzeugen der Endpoint Gruppen . Bei Verwendung mehrerer Subnetze muss sichergestellt werden, dass der VPN-Endpunkt das Routing mehrerer Subnetze über dieselbe Verbindung unterstützt. Während OpenStack dies tut, müssen für Implementierungen, welche dies nicht unterstützen, mehrere Endpunktgruppen erstellt werden, eine für jedes Subnetz. Verwenden von Horizon (GUI) . | Erzeugen Sie die lokale Endpoint Gruppe für die linke Seite. | . Navigieren Sie innerhalb Ihres Projekts zu Network → VPN. Wählen Sie Endpoint Groups aus und klicken Sie auf Add Endpoint Group. Geben Sie Ihrer Endpoint Gruppe einen Namen. Wählen Sie als Typ Subnet und wählen Sie unter Local System Subnets Ihr Subnetz aus. Klicken Sie anschließend auf Add. | Erzeugen Sie die Peer Endpoint Gruppe für die linke Seite. | . Sie befinden sich noch immer innerhalb von Network → VPN und Endpoint Groups. Klicken Sie erneut auf Add Endpoint Group. Geben Sie Ihrer Endpoint Gruppe einen Namen. Wählen Sie den Typ CIDR und geben Sie das Subnetz für die rechte Seite ein. Klicken Sie anschließend auf Add. Verwenden von CLI . | Verwenden Sie den Befehl openstack vpn endpoint group create, um die lokale Endpoint Gruppe für die linke Seite zu erzeugen. | . $ openstack vpn endpoint group create \\ vpnaas-left-local \\ --type subnet \\ --value vpnaas-left-network-subnet +-------------+------------------------------------------+ | Field | Value | +-------------+------------------------------------------+ | Description | | Endpoints | ['e217a377-48c7-4c18-93b5-cfd805bde40a'] | ID | 949ccc53-5dc6-457d-95bf-278fdf9a3e5d | Name | vpnaas-left-local | Project | 281fa14f782e4d4cbfd4e34a121c2680 | Type | subnet | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | +-------------+------------------------------------------+ . | Verwenden Sie erneut den Befehl openstack vpn endpoint group create, um die Peer Endpoint Gruppe für die linke Seite zu erzeugen. | . $ openstack vpn endpoint group create \\ vpnaas-left-remote \\ --type cidr \\ --value 2001:db8:1:33bd::/64 +-------------+--------------------------------------+ | Field | Value | +-------------+--------------------------------------+ | Description | | Endpoints | ['2001:db8:1:33bd::/64'] | ID | 9146346d-1306-4b03-a3ce-04ee51832ed8 | Name | vpnaas-left-remote | Project | 281fa14f782e4d4cbfd4e34a121c2680 | Type | cidr | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | +-------------+--------------------------------------+ . Erzeugen der Site-Verbindungen . Wie auch bei den Endpunktgruppen müssen Sie, wenn Ihr VPN-Endpunkt das Routing mehrerer Subnetze über dieselbe Verbindung nicht unterstützt, mehrere Site-Verbindungen erstellen, eine für jedes Subnetz/Endpunktgruppe. Verwenden von Horizon (GUI) . Navigieren Sie innerhalb Ihres Projekts zu Network → VPN. Wählen Sie IPSec Site Connections aus und klicken Sie auf Add IPSec Site Connection. Geben Sie Ihrer Verbindung einen Namen. Wählen Sie den vorher erzeugten VPN Service, die lokale Endpoint Guppe und die IKE und IPSec Policy, den Pre-Shared Key, die Peer IP und die Router Identity. In dieser Anleitung nutzen wir 2001:db8::4:703 als IP Addresse des rechten Routers. Verwenden von CLI . Verwenden Sie den Befehl openstack vpn ipsec site connection create, um den VPN Service zu erzeugen. $ openstack vpn ipsec site connection create \\ vpnaas-left-connection \\ --vpnservice vpnaas-left-vpn \\ --ikepolicy vpnaas-left-ike-policy \\ --ipsecpolicy vpnaas-left-ipsec-policy \\ --local-endpoint-group vpnaas-left-local \\ --peer-address 2001:db8::4:703 \\ --peer-id 2001:db8::4:703 \\ --peer-endpoint-group vpnaas-left-remote \\ --psk 1gHAsAeR8lFEDDu7 +--------------------------+----------------------------------------------------+ | Field | Value | +--------------------------+----------------------------------------------------+ | Authentication Algorithm | psk | Description | | ID | d81dbe28-ccda-4ee3-ba96-145fadc74e0f | IKE Policy | 561387b8-b5c1-415e-abc9-79ba93dd48ff | IPSec Policy | 553a600e-f39d-47a0-9550-97f2b4033685 | Initiator | bi-directional | Local Endpoint Group ID | 949ccc53-5dc6-457d-95bf-278fdf9a3e5d | Local ID | | MTU | 1500 | Name | vpnaas-left-connection | Peer Address | 2001:db8::4:703 | Peer CIDRs | | Peer Endpoint Group ID | 9146346d-1306-4b03-a3ce-04ee51832ed8 | Peer ID | 2001:db8::4:703 | Pre-shared Key | 1gHAsAeR8lFEDDu7 | Project | 281fa14f782e4d4cbfd4e34a121c2680 | Route Mode | static | State | True | Status | PENDING_CREATE | VPN Service | cc258fd7-0e87-4058-ad7d-355f32c1ab5e | dpd | {'action': 'hold', 'interval': 30, 'timeout': 120} | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | +--------------------------+----------------------------------------------------+ . ",
    "url": "/optimist/networking/vpnaas/#anlegen-eines-site-to-site-ipsec-vpn",
    
    "relUrl": "/optimist/networking/vpnaas/#anlegen-eines-site-to-site-ipsec-vpn"
  },"230": {
    "doc": "VPN as a Service",
    "title": "VPN as a Service",
    "content": " ",
    "url": "/optimist/networking/vpnaas/",
    
    "relUrl": "/optimist/networking/vpnaas/"
  },"231": {
    "doc": "Storage",
    "title": "Storage",
    "content": " ",
    "url": "/optimist/storage/",
    
    "relUrl": "/optimist/storage/"
  },"232": {
    "doc": "S3 Kompatiblen Objekt Storage",
    "title": "Einführung in den S3 Kompatiblen Objekt Storage",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/#einf%C3%BChrung-in-den-s3-kompatiblen-objekt-storage",
    
    "relUrl": "/optimist/storage/s3_documentation/#einführung-in-den-s3-kompatiblen-objekt-storage"
  },"233": {
    "doc": "S3 Kompatiblen Objekt Storage",
    "title": "Was genau ist Object Storage?",
    "content": "Object Storage ist eine alternative zum klassischen Block Storage. Dabei werden die einzelnen Daten nicht einzelnen Blöcken auf einem Gerät zugeordnet, sondern als binäre Objekte innerhalb eines Storage Clusters gespeichert. Die notwendigen Metadaten werden in einer separaten DB gespeichert. Der Storage Cluster besteht dabei aus mehreren einzelnen Servern (Nodes), die wiederum mehrere Storage Devices verbaut haben. Bei den Storage Devices haben wir einen mix aus klassischen HDDs, SSDs und modernen NVMe Lösungen. Auf welchem Gerät letztendlich ein Object landet, wird durch den CRUSH Algorithmus entschieden, der beim Object Storage serverseitig implementiert ist. ",
    "url": "/optimist/storage/s3_documentation/#was-genau-ist-object-storage",
    
    "relUrl": "/optimist/storage/s3_documentation/#was-genau-ist-object-storage"
  },"234": {
    "doc": "S3 Kompatiblen Objekt Storage",
    "title": "Wie kann ich darauf zugreifen?",
    "content": "Der Zugriff auf diese Art von Storage erfolgt ausschließlich über HTTPS Zugriffe. Dafür stellen wir einen hochverfügbaren Endpunkt zur Verfügung, über den die einzelnen Operationen ausgeführt werden können. Dabei unterstützen wir zwei unabhängige Protokolle: . | S3 | Swift | . S3 ist ein von Amazon ins leben gerufene Protokoll, um mit dieser Art von Daten arbeiten zu können. Swift ist das Protokoll, welches vom gleichnamigen OpenStack Service bereitgestellt wird. Unabhängig davon, welches Protokoll man nutzt, hat man immer Zugriff auf seine gesamten Daten. Man kann also beide Protokolle in Kombination nutzen. Es gibt für alle gängigen Plattformen Tools, um mit den Daten im Object Storage arbeiten zu können: . | Windows: s3Browser, Cyberduck | MacOS: s3cmd, Cyberduck | Linux: s3cmd | . Darüberhinaus gibt es Integrationen in allen gängigen Programmiersprachen. ",
    "url": "/optimist/storage/s3_documentation/#wie-kann-ich-darauf-zugreifen",
    
    "relUrl": "/optimist/storage/s3_documentation/#wie-kann-ich-darauf-zugreifen"
  },"235": {
    "doc": "S3 Kompatiblen Objekt Storage",
    "title": "Wie sicher sind meine Daten?",
    "content": "Basis für den Object Storage ist unsere zugrunde liegende Openstack Cloud Plattform mit dem verteilten Ceph Storage Cluster. Die Objekte werden serverseitig über mehrere Storage Devices hinweg verteilt und repliziert. Dabei sorgt Ceph für die Sicherstellung von Replikation und Integrität der Datensätze. Fällt ein Server oder eine Festplatte aus, werden die betroffenen Datensätze auf verfügbare Server repliziert und das gewünschte Replikationslevel automatisch wiederhergestellt. Darüberhinaus werden die Daten in ein weiteres RZ auf einen weiteren dedizierten Storage Cluster gespiegelt und können von dort im K-Fall weitergenutzt werden. ",
    "url": "/optimist/storage/s3_documentation/#wie-sicher-sind-meine-daten",
    
    "relUrl": "/optimist/storage/s3_documentation/#wie-sicher-sind-meine-daten"
  },"236": {
    "doc": "S3 Kompatiblen Objekt Storage",
    "title": "Vorteile im Überblick",
    "content": ". | Bereitstellung per API: Die HTTPS-Schnittstelle ist sowohl kompatibel zur Amazon S3 API, also auch zur OpenStack Swift API. | unterstützt alle gängigen Betriebssysteme und Programmiersprachen. | Volle Skalierbarkeit - Der Storage kann dynamisch genutzt werden. | Höchste Ausfallsicherheit dank integrierter Replikation und Spiegelung über 2 unabhängige Rechenzentren. | Zugriff ist nahezu von jedem internetfähigen Gerät aus möglich - Somit eine super alternative zu NFS und Co. | PAYG Abrechnung nach genutztem Monatsdurchschnitt. | Transparente Abrechnung und somit gute Planbarkeit - Keine extra Traffic Kosten oder Kosten für Zugriffe auf die Daten. | Automatisierte Verwaltung der Objekte in Buckets mit s3 Lifecycle Policies möglich. | . ",
    "url": "/optimist/storage/s3_documentation/#vorteile-im-%C3%BCberblick",
    
    "relUrl": "/optimist/storage/s3_documentation/#vorteile-im-überblick"
  },"237": {
    "doc": "S3 Kompatiblen Objekt Storage",
    "title": "S3 Kompatiblen Objekt Storage",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/",
    
    "relUrl": "/optimist/storage/s3_documentation/"
  },"238": {
    "doc": "S3 Kennung erstellen und einlesen",
    "title": "S3 Kennung erstellen und einlesen",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/createanduses3credentials/",
    
    "relUrl": "/optimist/storage/s3_documentation/createanduses3credentials/"
  },"239": {
    "doc": "S3 Kennung erstellen und einlesen",
    "title": "Inhalt:",
    "content": ". | Credentials erstellen . | S3cmd | S3Browser | Cyberduck | Boto3 | . | Credentials anzeigen | Credentials löschen | . ",
    "url": "/optimist/storage/s3_documentation/createanduses3credentials/#inhalt",
    
    "relUrl": "/optimist/storage/s3_documentation/createanduses3credentials/#inhalt"
  },"240": {
    "doc": "S3 Kennung erstellen und einlesen",
    "title": "Credentials erstellen",
    "content": "Damit wir auf den Object Storage zugreifen können, benötigen wir zunächst Login Daten(Credentials). Um diese Daten per OpenStackAPI erzeugen zu können, benötigen wir den OpenStackClient und führen dort folgenden Befehl aus: . $ openstack ec2 credentials create . Wenn die Daten korrekt erstellt worden sind, sieht die Ausgabe in etwa so aus: . $ openstack ec2 credentials create +------------+-----------------------------------------------------------------+ | Field | Value | +------------+-----------------------------------------------------------------+ | access | aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa | links | {u'self': u'https://identity.optimist.gec.io/v3/users/bbb | | bbbbbbbbbbbbbbbbbbbbbbbbbbbbb/credentials/OS- | | EC2/aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'} | project_id | cccccccccccccccccccccccccccccccc | secret | dddddddddddddddddddddddddddddddd | trust_id | None | user_id | bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb | +------------+-----------------------------------------------------------------+ . Nachdem die Zugangsdaten (Credentials) vorliegen, brauchen wir eine Möglichkeit auf den S3 kompatiblen ObjectStorage zuzugreifen. Hierfür gibt es die unterschiedliche Möglichkeiten, in der Dokumentation stellen wir hierfür vier Möglichkeiten vor, genauer: S3cmd für Linux/Mac, S3Browser für Windows, Cyberduck und Boto3. ",
    "url": "/optimist/storage/s3_documentation/createanduses3credentials/#credentials-erstellen",
    
    "relUrl": "/optimist/storage/s3_documentation/createanduses3credentials/#credentials-erstellen"
  },"241": {
    "doc": "S3 Kennung erstellen und einlesen",
    "title": "Benutzerdaten in die Konfigurationsdatei eintragen",
    "content": "S3cmd . Um s3cmd zu installieren, brauchen wir einen Paketmanager wie zum Beispiel “pip”. Die Installation und Nutzung erklären wir im Schritt 4: “Der Weg vom Horizon auf die Kommandozeile” unserer Guided Tour. Der Befehl für die Installation lautet dann: . pip install s3cmd . Nach der erfolgreichen Installation von s3cmd, müssen die vorher erstellten Zugangsdaten (Credentials) in die Konfigurationsdatei von s3cmd eingetragen werden. Die dafür zuständige Datei ist die “.s3cfg”, welche sich standardgemäß im Homeverzeichnis befindet. Sollte diese noch nicht existieren, muss diese vorher erstellt werden. Folgende Daten tragen wir dann in der .s3cfg ein und speichern diese: . access_key = aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa check_ssl_certificate = True check_ssl_hostname = True host_base = s3.es1.fra.optimist.gec.io host_bucket = s3.es1.fra.optimist.gec.io secret_key = dddddddddddddddddddddddddddddddd use_https = True . S3Browser . Für den S3Browser genügt es, diese heruterzuladen und zu installieren. Nach der erfolgreichen Installation, gilt es nun die entsprechenden Daten zu hinterlegen. Hierfür öffnen wir den S3Browser und es öffnet sich beim ersten Starten automatisch folgendes Fenster: . Dort tragen wir nun folgende Werte ein und klicken auf “Add new account” . * Account Name: Frei wählbarer Name für den Account * Account Type: S3 Compatible Storage * REST Endpoint: s3.es1.fra.optimist.gec.io * Signature Version: Signature V2 * Access Key ID: Den entsprechenden Access Key (Im Beispiel: aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa) * Secret Access Key: Das entsprechende Secret (Im Beispiel: dddddddddddddddddddddddddddddddd) . Cyberduck . Um Cyberduck zu nutzen, ist es notwendig diese herunterzuladen. Nach der Installation und dem ersten öffnen, ist es notwendig auf “Neue Verbindung” zu klicken. (1) Danach öffnet sich ein neues Fenster in dem im Dropdown Menü(2) “Amazon S3” ausgewählt wird und danach werden folgende Daten benötigt: . | Server(3): s3.es1.fra.optimist.gec.io | Access Key ID(4): Den entsprechenden Access Key (Im Beispiel: aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa) | Secret Access Key(5): Das entsprechende Secret (Im Beispiel: dddddddddddddddddddddddddddddddd) | . Um nun eine Verbindung herzustellen, wird als letzter Schritt auf “Verbinden” geklickt. Boto3 . Um Boto3 nutzen zu können, wird ein Paketmanager wie zum Beispiel “pip” benötigt. Die Installation und Nutzung wird im Schritt 4: “Der Weg vom Horizon auf die Kommandozeile” unserer Guided Tour erklärt. Der Befehl für die Installation lautet dann: . pip install boto3 . Nach der erfolgreichen Installation von boto3 ist es nun nutzbar. Wichtig ist, dass bei boto3 ein Script erstellt wird, welches am Ende ausgeführt wird. Daher ist der Konfigurationsteil der im Anschluss gezeigt wird, später immer Teil der weiterführenden Scripte. Hierfür erstellen wir eine Python-Datei wie z.B. “Beispiel.py” und fügen dort folgenden Inhalt ein: . | endpoint_url: s3.es1.fra.optimist.gec.io | aws_access_key_id: Den entsprechenden Access Key (Im Beispiel: aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa) | aws_secret_access_key: Das entsprechende Secret (Im Beispiel: dddddddddddddddddddddddddddddddd) | . #!/usr/bin/env/python import boto3 from botocore.client import Config s3 = boto3.resource('s3', endpoint_url='https://s3.es1.fra.optimist.gec.io', aws_access_key_id='aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', aws_secret_access_key='dddddddddddddddddddddddddddddddd', ) . Dies dient als Startpunkt und wird in den folgenden Skripten referenziert und verwendet. ",
    "url": "/optimist/storage/s3_documentation/createanduses3credentials/#benutzerdaten-in-die-konfigurationsdatei-eintragen",
    
    "relUrl": "/optimist/storage/s3_documentation/createanduses3credentials/#benutzerdaten-in-die-konfigurationsdatei-eintragen"
  },"242": {
    "doc": "S3 Kennung erstellen und einlesen",
    "title": "Credentials anzeigen",
    "content": "Um erstellte Object Storage EC2-Credentials anzuzeigen benötigen wir den OpenstackClient und führen dort folgenden Befehl aus: . $ openstack ec2 credentials list . Der Befehl erstellt uns eine Liste mit allen EC2 Credentials, die für den aktuellen Nutzer sichtbar sind. $ openstack ec2 credentials list +----------------------------------+----------------------------------+----------------------------------+----------------------------------+ | Access | Secret | Project ID | User ID | +----------------------------------+----------------------------------+----------------------------------+----------------------------------+ | aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa | xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx | 12341234123412341234123412341234 | 32132132132132132132132132132132 | bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb | yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy | 56756756756756756756756756756756 | 65465465465465465465465465465465 | cccccccccccccccccccccccccccccccc | zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz | 89089089089089089089089089089089 | 09809809809809809809809809809809 | +----------------------------------+----------------------------------+----------------------------------+----------------------------------+ . ",
    "url": "/optimist/storage/s3_documentation/createanduses3credentials/#credentials-anzeigen",
    
    "relUrl": "/optimist/storage/s3_documentation/createanduses3credentials/#credentials-anzeigen"
  },"243": {
    "doc": "S3 Kennung erstellen und einlesen",
    "title": "Credentials löschen",
    "content": "Um vorhandene Object Storage EC2-Credentials zu löschen benötigen wir den OpenstackClient und führen dort folgenden Befehl aus: . $ openstack ec2 credentials delete &lt;access-key&gt; . ",
    "url": "/optimist/storage/s3_documentation/createanduses3credentials/#credentials-l%C3%B6schen",
    
    "relUrl": "/optimist/storage/s3_documentation/createanduses3credentials/#credentials-löschen"
  },"244": {
    "doc": "Einen Bucket erstellen und wieder löschen",
    "title": "Einen Bucket erstellen und wieder löschen",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/createanddeletebucket/",
    
    "relUrl": "/optimist/storage/s3_documentation/createanddeletebucket/"
  },"245": {
    "doc": "Einen Bucket erstellen und wieder löschen",
    "title": "Inhalt:",
    "content": ". | S3cmd | S3Browser | Cyberduck | Boto3 | . Zum Hochladen Ihrer Daten wie zum Beispiel (Dokumente, Fotos, Videos, usw.) ist es zunächst notwendig einen Bucket zu erstellen. Dieser dient als eine Art Ordner. Aufgrund der Funktionsweise unseres Objects-Storages ist es notwendig einen global eindeutigen Namen für Ihren Bucket zu verwenden. Sollte bereits ein Bucket mit dem gewählten Namen existieren, kann der Name erst verwendet werden, wenn der bereits existierende Bucket gelöscht wurde. Sollte der gewünschte Name bereits von einem weiteren Kunden in Benutzung sein müssen sie einen anderen Namen wählen. Es empfiehlt sich, Namen der Form “inhaltsbeschreibung.bucket.meine-domain.tld” oder vergleichbares zu verwenden. ",
    "url": "/optimist/storage/s3_documentation/createanddeletebucket/#inhalt",
    
    "relUrl": "/optimist/storage/s3_documentation/createanddeletebucket/#inhalt"
  },"246": {
    "doc": "Einen Bucket erstellen und wieder löschen",
    "title": "S3cmd",
    "content": "Bucket erstellen . Um einen Bucket zu erstellen, nutzt man folgenden Befehl: . s3cmd mb s3://NameDesBuckets . Die Ausgabe in der Kommandozeile kann dann so aussehen: . $ s3cmd mb s3://iNNOVO-Test Bucket 's3://iNNOVO-Test/' created . Bucket löschen . Um einen Bucket zu löschen, nutzt man folgenden Befehl: . s3cmd rb s3://NameDesBuckets . Die Ausgabe in der Kommandozeile kann dann so aussehen: . $ s3cmd rb s3://iNNOVO-Test Bucket 's3://iNNOVO-Test/' removed . ",
    "url": "/optimist/storage/s3_documentation/createanddeletebucket/#s3cmd",
    
    "relUrl": "/optimist/storage/s3_documentation/createanddeletebucket/#s3cmd"
  },"247": {
    "doc": "Einen Bucket erstellen und wieder löschen",
    "title": "S3Browser",
    "content": "Bucket erstellen . Nach dem Öffnen von S3Browser, klicken wir oben links auf “New bucket”(1), in dem sich öffnenden Fenster vergeben wir unter “Bucket name”(2) den Namen des Buckets und klicken dann auf “Create new bucket”(3). Bucket löschen . Zuerst wird der zu löschende Bucket markiert(1) und danch oben links auf “Delete bucket” geklickt. Im sich nun öffnenden Fenster, bestätigen mit dem markieren der Checkbox(1), dass die Datei gelöscht werden soll und klicken dann auf “Delete Bucket”(2). ",
    "url": "/optimist/storage/s3_documentation/createanddeletebucket/#s3browser",
    
    "relUrl": "/optimist/storage/s3_documentation/createanddeletebucket/#s3browser"
  },"248": {
    "doc": "Einen Bucket erstellen und wieder löschen",
    "title": "Cyberduck",
    "content": "Bucket erstellen . Nach dem Öffnen von Cyberduck, klicken wir oben in der Mitte auf “Aktion”(1) und auf “Neuer Ordner”(2) . Danach öffnet sich das folgende Fenster, hier können wir den Namen(1) festlegen und bestätigen dies im Anschluß mit “Anlegen”(2): . Bucket löschen . Um einen Bucket zu löschen, wird dieser mit einem linken Mausklick makiert. Gelöscht wird der Bucket dann über “Aktion”(1) und “Löschen”(2). Die Bestätigung erfolgt dann über das erneute klicken auf “Löschen”(1) . ",
    "url": "/optimist/storage/s3_documentation/createanddeletebucket/#cyberduck",
    
    "relUrl": "/optimist/storage/s3_documentation/createanddeletebucket/#cyberduck"
  },"249": {
    "doc": "Einen Bucket erstellen und wieder löschen",
    "title": "Boto3",
    "content": "Bei boto3 brauchen wir zunächst die S3 Kennung, damit ein Script nutzbar ist. Für Details: S3 Kennung erstellen und einlesen #boto3 . Bucket erstellen . Um nun einen Bucket zu erstellen, müssen wir einen Clienten nutzen und den Bucket dann erstellen. Eine Option sieht so aus: . ## Erstellung eines S3 Clienten s3 = boto3.client('s3') ## Erstellung des Buckets s3.create_bucket(Bucket='iNNOVO-Test') . Ein komplettes Script für boto 3 inkl. Authentifizierung kann so aussehen: . #!/usr/bin/env/python ## Definieren das boto3 genutzt werden soll import boto3 from botocore.client import Config ## Authentifizierung s3 = boto3.resource('s3', endpoint_url='https://s3.es1.fra.optimist.gec.io', aws_access_key_id='aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', aws_secret_access_key='bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb', ) ## Erstellung eines S3 Clienten s3 = boto3.client('s3') ## Erstellung des Buckets s3.create_bucket(Bucket='iNNOVO-Test') . Bucket löschen . Wie bei der Erstellung des Buckets, wird zunächst ein Client benötigt um dann den Bucket zu löschen. Um nun einen Bucket zu erstellen, müssen wir einen Clienten nutzen und den Bucket dann erstellen. Eine Option sieht so aus: . ## Erstellung eines S3 Clienten s3 = boto3.client('s3') ## Löschen eines Buckets s3.delete_bucket(Bucket='iNNOVO-Test') . Ein komplettes Script für boto 3 inkl. Authentifizierung kann so aussehen: . #!/usr/bin/env/python ## Definieren das boto3 genutzt werden soll import boto3 from botocore.client import Config ## Authentifizierung s3 = boto3.resource('s3', endpoint_url='https://s3.es1.fra.optimist.gec.io', aws_access_key_id='aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', aws_secret_access_key='6229490344a445f2aa59cdc0e53add88', ) ## Erstellung eines S3 Clienten s3 = boto3.client('s3') ## Erstellung des Buckets s3.delete_bucket(Bucket='iNNOVO-Test') . ",
    "url": "/optimist/storage/s3_documentation/createanddeletebucket/#boto3",
    
    "relUrl": "/optimist/storage/s3_documentation/createanddeletebucket/#boto3"
  },"250": {
    "doc": "Ein Objekt hochladen und löschen",
    "title": "Ein Objekt hochladen und löschen",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/uploadanddeleteobject/",
    
    "relUrl": "/optimist/storage/s3_documentation/uploadanddeleteobject/"
  },"251": {
    "doc": "Ein Objekt hochladen und löschen",
    "title": "Inhalt:",
    "content": ". | S3cmd | S3Browser | Cyberduck | Boto3 | . Zum Hochladen Ihrer Daten wie zum Beispiel (Dokumente, Fotos, Videos, usw.) ist es zunächst notwendig einen Bucket zu erstellen. Eine Datei kann dabei nur in einem Bucket gespeichert werden. ",
    "url": "/optimist/storage/s3_documentation/uploadanddeleteobject/#inhalt",
    
    "relUrl": "/optimist/storage/s3_documentation/uploadanddeleteobject/#inhalt"
  },"252": {
    "doc": "Ein Objekt hochladen und löschen",
    "title": "S3cmd",
    "content": "Objekt hochladen . Um eine Datei hochzuladen, nutzt man folgenden Befehl: . s3cmd put NameDerDatei s3://NameDesBuckets/NameDerDatei . Die Ausgabe in der Kommandozeile kann dann so aussehen: . $ s3cmd put innovo.txt s3://innovo-test/innovo.txt&lt;font&gt;&lt;/font&gt; upload: 'innovo.txt' -&gt; 's3://innovo-test/innovo.txt' [1 of 1]&lt;font&gt;&lt;/font&gt; 95 of 95 100% in 0s 176.63 B/s done . Objekt löschen . Um eine Datei zu löschen, nutzt man folgenden Befehl: . s3cmd del s3://NameDesBuckets/NameDerDatei . Die Ausgabe in der Kommandozeile kann dann so aussehen: . $ s3cmd del s3://innovo-test/innovo.txt&lt;font&gt;&lt;/font&gt; delete: 's3://innovo-test/innovo.txt' . ",
    "url": "/optimist/storage/s3_documentation/uploadanddeleteobject/#s3cmd",
    
    "relUrl": "/optimist/storage/s3_documentation/uploadanddeleteobject/#s3cmd"
  },"253": {
    "doc": "Ein Objekt hochladen und löschen",
    "title": "S3Browser",
    "content": "Objekt hochladen . Nach dem öffnen von S3Browser, klicken wir auf den gewünschten “Bucket”(1), wähle dann “Upload”(2) und zu letzt “Upload file(s)”(3) . Hier wählen wir nun die entsprechende Datei(1) aus und klicken auf Öffnen(2). Objekt löschen . Um eine Datei zu löschen, wird dieser mit einem linken Mausklick markiert(1). Danach wird auf “Delete”(2) geklickt. Die darauf folgende Abfrage wird mit “Ja” bestätigt. ",
    "url": "/optimist/storage/s3_documentation/uploadanddeleteobject/#s3browser",
    
    "relUrl": "/optimist/storage/s3_documentation/uploadanddeleteobject/#s3browser"
  },"254": {
    "doc": "Ein Objekt hochladen und löschen",
    "title": "Cyberduck",
    "content": "Objekt hochladen . Nach dem Öffnen von Cyberduck, klicken wir auf den gewünschten Bucket(1), klicken dann auf Aktion(2) und dort auf Upload(3). Hier wählen wir nun unsere Wunsch-Datei und klicken auf Upload. Objekt löschen . Um eine Datei zu löschen, wird dieser mit einem linken Mausklick markiert(1). Gelöscht wird sie dann über “Aktion”(2) und “Löschen”(3). Die Bestätigung erfolgt dann über das erneute klicken auf “Löschen”. ",
    "url": "/optimist/storage/s3_documentation/uploadanddeleteobject/#cyberduck",
    
    "relUrl": "/optimist/storage/s3_documentation/uploadanddeleteobject/#cyberduck"
  },"255": {
    "doc": "Ein Objekt hochladen und löschen",
    "title": "Boto3",
    "content": "Bei boto3 brauchen wir zunächst die S3 Kennung, damit ein Script nutzbar ist. Für Details: S3 Kennung erstellen und einlesen #boto3 . Objekt hochladen . Um nun eine Datei hochzuladen, müssen wir einen Clienten nutzen und den Bucket angeben in welchen die Datei hochgeladen werden soll. Eine Option sieht so aus: . ## Erstellung eines S3 Clienten s3 = boto3.client('s3') ## Hochladen einer Datei s3.upload_file(Bucket='iNNOVO-Test', Key='innovo.txt') . Ein komplettes Script für boto 3 inkl. Authentifizierung kann so aussehen: . #!/usr/bin/env/python ## Definieren das boto3 genutzt werden soll import boto3 from botocore.client import Config ## Authentifizierung s3 = boto3.resource('s3', endpoint_url='https://s3.es1.fra.optimist.gec.io',&lt;font&gt;&lt;/font&gt; aws_access_key_id='aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',&lt;font&gt;&lt;/font&gt; aws_secret_access_key='bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb',&lt;font&gt;&lt;/font&gt; ) ## Erstellung eines S3 Clienten s3 = boto3.client('s3') ## Hochladen einer Datei s3.upload_file(Bucket='iNNOVO-Test', Key='innovo.txt') . Objekt löschen . Wie beim hochladen einer Datei, wird zunächst ein Client benötigt um dann die Datei zu löschen. Dafür geben wir neben der Datei selber, auch noch den Bucket an, in dem die Datei gespeichert ist. Eine Option sieht so aus: . ## Erstellung eines S3 Clienten s3 = boto3.client('s3') ## Löschen eines Objekts s3.delete_object(Bucket='iNNOVO-Test', Key='innovo.txt') . Ein komplettes Script für boto 3 inkl. Authentifizierung kann so aussehen: . #!/usr/bin/env/python ## Definieren das boto3 genutzt werden soll import boto3 from botocore.client import Config ## Authentifizierung s3 = boto3.resource('s3', endpoint_url='https://s3.es1.fra.optimist.gec.io', aws_access_key_id='aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', aws_secret_access_key='bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb', ) ## Erstellung eines S3 Clienten s3 = boto3.client('s3') ## Löschen eines Objekts s3.delete_object(Bucket='iNNOVO-Test', Key='innovo.txt') . ",
    "url": "/optimist/storage/s3_documentation/uploadanddeleteobject/#boto3",
    
    "relUrl": "/optimist/storage/s3_documentation/uploadanddeleteobject/#boto3"
  },"256": {
    "doc": "Versionierung aktivieren/deaktivieren und wie eine versioniertes Objekt gelöscht wird",
    "title": "Versionierung aktivieren/deaktivieren und wie eine versioniertes Objekt gelöscht wird",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/versioning/",
    
    "relUrl": "/optimist/storage/s3_documentation/versioning/"
  },"257": {
    "doc": "Versionierung aktivieren/deaktivieren und wie eine versioniertes Objekt gelöscht wird",
    "title": "Inhalt:",
    "content": ". | S3cmd | S3Browser | Cyberduck | Boto3 | . Versionierung ermöglicht es, mehrere Versionen eines Objekts in einem Bucket aufzubewahren. So können Beispielsweise innovo.txt (Version 1) und innovo.txt (Version 2) in einem einzigen Bucket speichern. Die Versionierung kann Sie vor den Folgen von unbeabsichtigtem Überschreiben oder Löschen bewahren. ",
    "url": "/optimist/storage/s3_documentation/versioning/#inhalt",
    
    "relUrl": "/optimist/storage/s3_documentation/versioning/#inhalt"
  },"258": {
    "doc": "Versionierung aktivieren/deaktivieren und wie eine versioniertes Objekt gelöscht wird",
    "title": "S3cmd",
    "content": "Mit S3cmd ist es nicht möglich die Versionierung einzuschalten und/oder versionierte Dateien zu löschen. ",
    "url": "/optimist/storage/s3_documentation/versioning/#s3cmd",
    
    "relUrl": "/optimist/storage/s3_documentation/versioning/#s3cmd"
  },"259": {
    "doc": "Versionierung aktivieren/deaktivieren und wie eine versioniertes Objekt gelöscht wird",
    "title": "S3Browser",
    "content": "Versionierung einschalten . Um die Versionierung zu aktivieren, markieren wir den gewünschten Bucket(1). Machen auf den Bucket einen rechten Mausklick und klicken dann auf “Edit Versioning Settings”(2). Im sich öffnenden Fenster klicken wir in die Checkbox von “Enable versioning for bucket”(1) und bestätigen dies mit “OK”(2). Versionierung deaktivieren . Um die Versionierung zu deaktivieren, markieren wir den gewünschten Bucket(1). Klicken dann mit einem rechten Mausklick auf den Bucket und dann auf “Edit Versioning Settings”(2). Im sich öffnenden Fenster entfernen wir die Checkbox bei “Enable versioning for bucket”(1) und bestätigen dies mit “OK”(2). Versionierte Datei löschen . Dies ist in der Free-Version von S3Browser nicht möglich. ",
    "url": "/optimist/storage/s3_documentation/versioning/#s3browser",
    
    "relUrl": "/optimist/storage/s3_documentation/versioning/#s3browser"
  },"260": {
    "doc": "Versionierung aktivieren/deaktivieren und wie eine versioniertes Objekt gelöscht wird",
    "title": "Cyberduck",
    "content": "Um die verschiedenen Version einer Datei zu sehen, müssen versteckte Dateien angezeigt werden. Diese Option findet man unter Darstellung(1) → Versteckte Dateien anzeigen(2) . Versionierung einschalten . Nach dem Öffnen von Cyberduck, klicken wir auf eine Datei, wo wir die Versionierung(1) für aktivieren wollen. Danach auf Aktion(2) und auf Info(3). Danach öffnet sich das folgende Fenster, hier setzen wir den Haken bei “Bucket Versionierung”(1): . Versionierung deaktivieren . Um die Versionierung zu deaktivieren, markieren wir wieder eine Datei(1), gehen auf Aktion(2) und auf Info(3). In dem sich öffnenden Fenster wird der Haken bei “Bucket Versionierung” entfernt. Versionierte Datei löschen . Hier wird einfach die zu löschende Datei markiert(1) und über Aktion(2) → Löschen(3) entfernt. ",
    "url": "/optimist/storage/s3_documentation/versioning/#cyberduck",
    
    "relUrl": "/optimist/storage/s3_documentation/versioning/#cyberduck"
  },"261": {
    "doc": "Versionierung aktivieren/deaktivieren und wie eine versioniertes Objekt gelöscht wird",
    "title": "Boto3",
    "content": "Bei boto3 brauchen wir zunächst die S3 Kennung, damit ein Script nutzbar ist. Für Details: S3 Kennung erstellen und einlesen #boto3. Versionierung einschalten . Um nun einen Bucket zu erstellen, müssen wir einen Clienten nutzen und den Bucket dann erstellen. Eine Option sieht so aus: . ## Angabe des Buckets in dem die Versionierung aktiviert werden soll bucket = s3.Bucket('iNNOVO-Test') ## Versionierung aktivieren bucket.configure_versioning(True) . Ein komplettes Script für boto 3 inkl. Authentifizierung kann so aussehen: . #!/usr/bin/env/python ## Definieren das boto3 genutzt werden soll import boto3 from botocore.client import Config ## Authentifizierung s3 = boto3.resource('s3', endpoint_url='https://s3.es1.fra.optimist.gec.io', aws_access_key_id='aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', aws_secret_access_key='bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb', ) ## Angabe des Buckets in dem die Versionierung aktiviert werden soll bucket = s3.Bucket('iNNOVO-Test') ## Versionierung aktivieren bucket.configure_versioning(True) . Versionierung deaktivieren . Wie bei der Aktivierung der Versionierung wird zunächst der Bucket benötigt um dann die Versionierung zu deaktivieren. Eine Option sieht so aus: . ## Angabe des Buckets in dem die Versionierung deaktiviert werden soll bucket = s3.Bucket('iNNOVO-Test') ## Versionierung deaktivieren bucket.configure_versioning(False) . Ein komplettes Script für boto 3 inkl. Authentifizierung kann so aussehen: . #!/usr/bin/env/python ## Definieren das boto3 genutzt werden soll import boto3 from botocore.client import Config ## Authentifizierung s3 = boto3.resource('s3', endpoint_url='https://s3.es1.fra.optimist.gec.io', aws_access_key_id='aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', aws_secret_access_key='6229490344a445f2aa59cdc0e53add88', ) ## Angabe des Buckets in dem die Versionierung deaktiviert werden soll bucket = s3.Bucket('iNNOVO-Test') ## Versionierung deaktivieren bucket.configure_versioning(False) . Versioniertes Objekt löschen . Um ein versioniertes Objekt komplett zu löschen, ist folgender Befehl hilfreich: . ## Angabe des Buckets in dem das versionierte Objekt gelöscht werden soll bucket = s3.Bucket('iNNOVO-Test') ## Versioniertes Objekt löschen bucket.object_versions.all().delete('innovo.txt') . ",
    "url": "/optimist/storage/s3_documentation/versioning/#boto3",
    
    "relUrl": "/optimist/storage/s3_documentation/versioning/#boto3"
  },"262": {
    "doc": "S3 Security",
    "title": "S3 Security",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/security/",
    
    "relUrl": "/optimist/storage/s3_documentation/security/"
  },"263": {
    "doc": "S3 Security",
    "title": "Einführung",
    "content": "Diese Seite bietet einen Überblick über die folgenden Themen im Zusammenhang mit S3-Buckets / Swift: . | Container Access Control Lists (ACLs) | Bucket-Policies | . Operationen auf Container-ACLs müssen auf OpenStack-Ebene mit Swift-Befehlen ausgeführt werden, während Bucket-Policies für jeden Bucket innerhalb eines Projekts mit Hilfe der s3cmd-Befehlszeile festgelegt werden müssen. In diesem Dokument werden einige Beispiele für jede Art von Operation beschrieben. ",
    "url": "/optimist/storage/s3_documentation/security/#einf%C3%BChrung",
    
    "relUrl": "/optimist/storage/s3_documentation/security/#einführung"
  },"264": {
    "doc": "S3 Security",
    "title": "Container Access Control Lists (ACLs)",
    "content": "Standardmäßig haben nur Projektbesitzer die Berechtigung, Container und Objekte zu erstellen, zu lesen und zu ändern. Ein Eigentümer kann jedoch anderen Benutzern mit Hilfe einer Access Control List (ACL) Zugriff gewähren. Die ACL kann für jeden Container festgelegt werden und gilt nur für diesen Container und die Objekte in diesem Container. Einige der Hauptelemente, mit denen eine ACL für einen Container festgelegt werden kann, sind nachfolgend aufgeführt: . | Element | Beschreibung | . | .r: *. | Jeder Benutzer hat Zugriff auf Objekte. In der Anforderung ist kein Token erforderlich. | . | .r: &lt;Referrer&gt; | Der Referrer erhält Zugriff auf Objekte. Der Referrer wird durch den Referer-Anforderungsheader in der Anforderung identifiziert. Es ist kein Token erforderlich. | . | .r: - &lt;Referrer&gt; | Diese Syntax (mit “-“ vor dem Referrer) wird unterstützt. Der Zugriff wird jedoch nicht verweigert, wenn ein anderes Element (z. B.r:*) den Zugriff gewährt. | . | .rlistings | Jeder Benutzer kann HEAD- oder GET-Operationen für den Container ausführen, wenn der Benutzer auch Lesezugriff auf Objekte hat (z. B. auch .r: * oder .r: &lt;Referrer&gt;. Es ist kein Token erforderlich. | . Als Beispiel setzen wir die Policy .r:*. für einen Container mit dem Namen \"&lt;example-container&gt;\". Diese Policy ermöglicht jedem externen Benutzer den Zugriff auf die Objekte im Container. swift post example-container --read-acl \".r:*\" . Umgekehrt können wir Benutzern auch erlauben, die Liste der Objekte in einem Container aufzulisten, aber nicht darauf zuzugreifen, indem wir die Policy “.rlistings” für unseren “example-container” festlegen: . swift post example-container --read-acl \".rlistings\" . Der folgende Befehl kann verwendet werden, um die Lese-Policy zu entfernen und den Container auf den Standardstatus “privat” zu setzen: . swift post -r \"\" example-container . Verwenden Sie den folgenden Befehl, um zu überprüfen, welche ACL für einen Container festgelegt ist. swift stat example-container . Dies gibt einen Überblick über die Statistiken für den Container und zeigt die aktuelle ACL-Regel für einen Container an. Verhindern Sie das Auflisten auf Containern, wenn Sie die Policy .r: * . verwenden: . In der aktuellen Version von OpenStack empfehlen wir, ein leeres index.html-Objekt im Container zu erstellen, um zu verhindern, dass der Inhalt aufgelistet wird, während die Policy .r: * . für einen Container verwendet wird. Auf diese Weise können Benutzer Objekte herunterladen, ohne den Inhalt der Buckets aufzulisten zu koennen. Dies kann mit den folgenden Schritten erreicht werden: . Fügen Sie zunächst die leere Datei index.html zu unserem example-container hinzu: . swift post -m 'Webindex: index.html' example-container . Erstellen Sie dann die Datei index.html als Objekt im Container: . touch index.html &amp;&amp; openstack object create example-container index.html . Dadurch können externe Benutzer auf bestimmte Dateien zugreifen, ohne den Inhalt des Containers aufzulisten. ",
    "url": "/optimist/storage/s3_documentation/security/#container-access-control-lists-acls",
    
    "relUrl": "/optimist/storage/s3_documentation/security/#container-access-control-lists-acls"
  },"265": {
    "doc": "S3 Security",
    "title": "Bucket-Policy",
    "content": "Bucket-Policies werden verwendet, um den Zugriff auf jeden Bucket in einem Projekt zu steuern. Es wird empfohlen, bei der Erstellung eine Policy für alle Buckets festzulegen. Der erste Schritt besteht darin, eine Policy wie folgt zu erstellen. Für die folgende Vorlage muss nur der Bucket-Name für nachfolgende Policy geändert werden. Im folgenden Beispiel wird eine Policy für den Bucket example-bucket erstellt: . cat &gt; examplepolicy { \"Version\": \"2008-10-17\", \"Statement\": [ { \"Sid\": \"AddPerm\", \"Effect\": \"Allow\", \"Principal\": \"*\", \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::example-bucket/*\" } ] } . Aufschlüsselung jedes Elements innerhalb des obigen Policy-Beispiels: . | Version: Gibt die Sprachsyntaxregeln an, die zum Verarbeiten der Policy verwendet werden sollen. Es wird empfohlen, immer Folgendes zu verwenden: “2012-10-17”, da dies die aktuelle Version der Policy-Sprache ist. | Statement: Das Hauptelement der Policy, die anderen Elemente befinden sich in dieser Anweisung. | SID: Die Statement-ID. Dies ist eine optionale Kennung, mit der die Richtlinienanweisung beschrieben werden kann. Empfohlen, damit der Zweck jeder Policy klar ist. | Effect: Stellen Sie entweder “Allow” oder “Deny” ein. | Principal: Gibt den Principal an, dem der Zugriff auf eine Ressource gestattet oder verweigert wird. Hier wird der Platzhalter “*” verwendet, um die Regel auf alle anzuwenden. | Action: Beschreibt die spezifischen Aktionen, die zugelassen oder abgelehnt werden. | . (Weitere Informationen zu den verfügbaren Policy-Optionen und zur Anpassung an Ihre spezifischen Anforderungen finden Sie in der offiziellen AWS-Dokumentation). Wenden Sie als Nächstes die neu erstellte Policy auf example-bucket an: . s3cmd setpolicy examplepolicy s3://example-bucket . Anschließend können Sie den folgenden Befehl ausführen, um sicherzustellen, dass die Policy vorhanden ist: . s3cmd info s3://example-bucket . Sobald die Policy angewendet wurde, können Sie im Dashboard erneut festlegen: Public Access: Disabled. Sobald die oben genannten Schritte ausgeführt wurden, erhalten wir die folgenden Ergebnisse: . | Der Container ist privat und Dateien werden nicht über XML aufgelistet oder angezeigt. | Die Policy ermöglicht jetzt den Zugriff auf bestimmte Dateien mit einem direkten Link. | . ",
    "url": "/optimist/storage/s3_documentation/security/#bucket-policy",
    
    "relUrl": "/optimist/storage/s3_documentation/security/#bucket-policy"
  },"266": {
    "doc": "Swift - Serving a Static Website",
    "title": "Swift - Serving Static Websites",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#swift---serving-static-websites",
    
    "relUrl": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#swift---serving-static-websites"
  },"267": {
    "doc": "Swift - Serving a Static Website",
    "title": "Einführung",
    "content": "Mit Hilfe der Swift-CLI ist es möglich, die Daten in Containern als statische Website auszuliefern. Die folgende Anleitung beschreibt die wichtigsten Schritte, um damit zu beginnen, und enthält auch ein Beispiel. ",
    "url": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#einf%C3%BChrung",
    
    "relUrl": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#einführung"
  },"268": {
    "doc": "Swift - Serving a Static Website",
    "title": "Erste Schritte",
    "content": "Erstellen eines Containers . Zunächst erstellen wir einen Container mit dem Namen example-webpage, den wir als Basis für diese Anleitung verwenden werden: . swift post example-webpage . Den Container öffentlich lesbar machen . Als nächstes müssen wir sicherstellen, dass der Container öffentlich lesbar ist. Weitere Informationen zum Sichern von Containern und zum Festlegen von Bucket-Richtlinien finden Sie hier: . swift post -r '.r:*' example-webpage . Indexdatei der Seite setzen . Setzen Sie die Indexdatei. In diesem Fall wird index.html die Standarddatei sein, die angezeigt wird, wenn die Seite erscheint: . swift post -m 'web-index:index.html' example-webpage . Dateiliste aktivieren . Optional können wir auch die Dateiliste aktivieren. Wenn Sie mehrere Downloads bereitstellen müssen, ist es sinnvoll, die Verzeichnisliste zu aktivieren: . swift post -m 'web-listings: true' example-webpage . CSS für Dateilisten aktivieren . Aktivieren Sie ein benutzerdefiniertes Listing-Stylesheet: . swift post -m 'web-listings-css:style.css' example-webpage . Fehlerseiten einrichten . Schließlich sollten wir eine benutzerdefinierte Fehlerseite einbinden: . swift post -m 'web-error:404error.html' example-webpage . ",
    "url": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#erste-schritte",
    
    "relUrl": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#erste-schritte"
  },"269": {
    "doc": "Swift - Serving a Static Website",
    "title": "Beispiel-Webseite",
    "content": "Lassen Sie uns die Schritte rekapitulieren, die wir bis jetzt unternommen haben, um statische Webseiten zu aktivieren: . swift post example-webpage swift post -r '.r:*' example-webpage swift post -m 'web-index:index.html' example-webpage swift post -m 'web-listings: true' example-webpage swift post -m 'web-listings-css:style.css' example-webpage swift post -m 'web-error:404error.html' example-webpage . Wenn die obigen Schritte abgeschlossen sind, können wir damit beginnen, unsere statische Webseite anzupassen. Das Folgende demonstriert eine schnelle Einrichtung unter Verwendung unseres Containers example-webpage . Anpassen der Seiten index.html, page.html und 404error.html . Dies wird als Startseite dienen, die einen Link zu einer sekundären Seite erstellt. &lt;!-- index.html --&gt; &lt;html&gt; &lt;h1&gt; See the web page &lt;a href=\"mywebsite/page.html\"&gt;here&lt;/a&gt;. &lt;/h1&gt; &lt;/html&gt; . Die nächste Seite (page.html) zeigt ein Bild namens sample.png an: . &lt;!-- page.html --&gt; &lt;html&gt; &lt;img src=\"sample.png\"&gt; &lt;/html&gt; . Wir können auch benutzerdefinierte Fehlerseiten hinzufügen. Beachten Sie, dass derzeit nur die Fehler 401 (Nicht autorisiert) und 404 (Nicht gefunden) unterstützt werden. Das folgende Beispiel demonstriert die Erstellung einer 404-Fehlerseite: . &lt;!-- 404error.html --&gt; &lt;html&gt; &lt;h1&gt; 404 Not Found - We cannot find the page you are looking for! &lt;/h1&gt; &lt;/html&gt; . Hochladen der Dateien index.html und page.html . Nachdem die Inhalte der Dateien erstellt wurden, laden Sie die Dateien mit den folgenden Befehlen hoch: . swift upload beispiel-webseite index.html swift upload beispiel-webseite meinewebseite/seite.html swift upload beispiel-webseite-meine-website/beispiel.png swift upload beispiel-webseite 404error.html . Betrachten der Website . Wenn alle oben genannten Schritte abgeschlossen sind, können wir nun unsere neu erstellte Website betrachten. Den Link zur Website finden Sie im Optimist Dashboard &gt; Object Store &gt; Containers über den abgebildeten Link. Wenn Sie auf den Link klicken, wird unsere neu erstellte Website angezeigt: . Klicken Sie auf “here”, um zu der Seite zu navigieren, auf der wir unser Beispielbild hochgeladen haben: . Für den Fall, dass wir versuchen, zu einer Seite zu navigieren, die nicht existiert, wird unsere benutzerdefinierte 404-Seite angezeigt: . ",
    "url": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#beispiel-webseite",
    
    "relUrl": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#beispiel-webseite"
  },"270": {
    "doc": "Swift - Serving a Static Website",
    "title": "Swift - Serving a Static Website",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/swiftservestaticwebsite/",
    
    "relUrl": "/optimist/storage/s3_documentation/swiftservestaticwebsite/"
  },"271": {
    "doc": "Sichere Datensicherung mit Restic und Rclone",
    "title": "Problemstellung",
    "content": "Beim Durchführen von Sicherungen auf Dateiebene eines Nodes in S3 benötigt die Sicherungssoftware Schreibberechtigungen für den S3-Bucket. Verschafft sich ein Angreifer jedoch Zugriff auf die Maschine, kann er auch die Backups im Bucket zerstören, da die S3-Anmeldeinformationen auf dem kompromittierten System vorhanden sind. Die Lösung kann so einfach sein, wie die Zugriffsebene der Backup-Software auf den Bucket einzuschränken. Leider ist das bei S3 nicht trivial. ",
    "url": "/optimist/storage/s3_documentation/backupwithrestic/#problemstellung",
    
    "relUrl": "/optimist/storage/s3_documentation/backupwithrestic/#problemstellung"
  },"272": {
    "doc": "Sichere Datensicherung mit Restic und Rclone",
    "title": "Hintergrund",
    "content": "S3-Zugriffskontrolllisten (ACLs) ermöglichen Ihnen die Verwaltung des Zugriffs auf Buckets und Objekte, sind jedoch sehr eingeschränkt. Sie unterscheiden im Wesentlichen READ- und WRITE-Berechtigungen: . | READ – Ermöglicht dem Benutzer, die Objekte im Bucket aufzulisten | WRITE - Ermöglicht dem Benutzer, jedes Objekt im Bucket zu erstellen, zu überschreiben und zu löschen | . Die Einschränkungen von ACLs wurden durch die Zugriffsrichtlinienberechtigungen (ACP) behoben. Wir könnten dem Bucket eine No-Delete-Richtlinie anhängen, z.B. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"nodelete1\", \"Effect\": \"Deny\", \"Action\": [ \"s3:DeleteBucket\", \"s3:DeleteBucketPolicy\", \"s3:DeleteBucketWebsite\", \"s3:DeleteObject\", \"s3:DeleteObjectVersion\" ], \"Resource\": [ \"arn:aws:s3:::*\" ] } ] } . Leider wurde das S3-Protokoll selbst nicht mit dem Konzept von WORM-Backups (Write Once Read Many) im Hinterkopf entwickelt. Zugriffsrichtlinienberechtigungen unterscheiden nicht zwischen dem Ändern eines vorhandenen Objekts (was ein effektives Löschen ermöglichen würde) und dem Erstellen eines neuen Objekts. Das Anhängen der obigen Richtlinie an einen Bucket verhindert nicht das Überschreiben der darin enthaltenen Objekte. $ s3cmd put testfile s3://appendonly-bucket/testfile upload: 'testfile' -&gt; 's3://appendonly-bucket/testfile' [1 of 1] 1054 of 1054 100% in 0s 5.46 KB/s done # die Richtlinie erlaubt Schreiben $ s3cmd rm s3://appendonly-bucket/testfile ERROR: S3 error: 403 (AccessDenied) # die Richtlinie verweigert das Löschen $ $ s3cmd put testfile s3://appendonly-bucket/testfile upload: 'testfile' -&gt; 's3://appendonly-bucket/testfile' [1 of 1] 1054 of 1054 100% in 0s 5.50 KB/s done # :( . ",
    "url": "/optimist/storage/s3_documentation/backupwithrestic/#hintergrund",
    
    "relUrl": "/optimist/storage/s3_documentation/backupwithrestic/#hintergrund"
  },"273": {
    "doc": "Sichere Datensicherung mit Restic und Rclone",
    "title": "Vorgeschlagene Lösung",
    "content": "Da ein Angreifer auf einem kompromittierten System immer Zugriff auf die S3-Anmeldeinformationen und jeden auf dem System laufenden Dienst hat – einschließlich Restic selbst, Proxys usw. – benötigen wir eine zweite, abgesicherte VM, die Löschvorgänge einschränken kann. Restic lässt sich perfekt in rclone integrieren, daher verwenden wir es in diesem Beispiel. Our environment . | appsrv: das zu sichernde System, mit Zugriff auf die Sicherungen | rclonesrv: das System, auf dem der rclone-Proxy ausgeführt wird (und nichts anderes, um die Angriffsfläche zu minimieren) | . Den rclone-Proxy einrichten . | rclone auf dem rclonesrv installieren: . sudo apt install rclone . | User für rclone anlegen . sudo useradd -m rcloneproxy sudo su - rcloneproxy . | Die Backend-Konfiguration für rclone backend erstellen . mkdir -p .config/rclone cat &lt;&lt; EOF &gt; .config/rclone/rclone.conf [s3-resticrepo] type = s3 provider = Other env_auth = false access_key_id = 111122223333444455556666 secret_access_key = aaaabbbbccccddddeeeeffffgggghhhh region = eu-central-1 endpoint = s3.es1.fra.optimist.gec.io acl = private bucket_acl = private upload_concurrency = 8 EOF . | Den Zugriff auf das Repository testen: . rclone lsd s3-resticrepo:databucket 0 2021-11-21 20:02:10 -1 data 0 2021-11-21 20:02:10 -1 index 0 2021-11-21 20:02:10 -1 keys 0 2021-11-21 20:02:10 -1 snapshots . | . Den Appserver konfigurieren . | Ein SSH-Schlüsselpaar auf appsrv mit dem Benutzer generieren, mit dem Sie das Backup durchführen: . ssh-keygen -o -a 256 -t ed25519 -C \"$(hostname)-$(date +'%d-%m-%Y')\" . | Die Umgebungsvariablen für restic setzen: . export RESTIC_PASSWORD=\"MyV3ryS3cUr3r3571cP4ssW0rd\" export RESTIC_REPOSITORY=rclone:s3-resticrepo:databucket . | . Den SSH-Schlüssel auf restic-only Befehle beschränken . Der letzte Schritt ist nun die SSH-Datei authorized_keys auf dem rclonesrv zu bearbeiten, um den neu generierten SSH-Schlüssel auf einen einzigen Befehl zu beschränken. Auf diese Weise kann ein Angreifer das SSH-Schlüsselpaar nicht verwenden, um beliebige Befehle auf dem rclone-Proxy auszuführen und die Backups zu kompromittieren. vi ~/.ssh/authorized_keys # Fügen Sie inen Eintrag mit dem öffentlichen Schlüssel des restic-Benutzers hinzu, der in dem obigen Schritt generiert wurde: command=\"rclone serve restic --stdio --append-only s3-resticrepo:databucket\" ssh-ed25519 AAAAC3fdsC1lZddsDNTE5ADsaDgfTwNtWmwiocdT9q4hxcss6tGDfgGTdiNN0z7zN appsrv-18-11-2021 . ",
    "url": "/optimist/storage/s3_documentation/backupwithrestic/#vorgeschlagene-l%C3%B6sung",
    
    "relUrl": "/optimist/storage/s3_documentation/backupwithrestic/#vorgeschlagene-lösung"
  },"274": {
    "doc": "Sichere Datensicherung mit Restic und Rclone",
    "title": "restic mit dem rclone Proxy verwenden",
    "content": "Wenn die Umgebungsvariablen gesetzt sind, sollte restic jetzt von dem appsrv aus funktionieren. Beispiel: Sicherung von /srv/myapp: . restic -o rclone.program=\"ssh rcloneproxy@rclonesrv.mydomain.com\" backup /srv/myapp . Snapshots auflisten: . restic -o rclone.program=\"ssh rcloneproxy@rclonesrv.mydomain.com\" snapshots . Snapshots löschen: . restic -o rclone.program=\"ssh rcloneproxy@rclonesrv.mydomain.com\" forget 2738e969 repository b71c391e opened successfully, password is correct Remove(&lt;snapshot/2738e9693b&gt;) returned error, retrying after 446.577749ms: blob not removed, server response: 403 Forbidden (403) . Ah, das geht nicht. Das war ja unser Ziel! . ",
    "url": "/optimist/storage/s3_documentation/backupwithrestic/#restic-mit-dem-rclone-proxy-verwenden",
    
    "relUrl": "/optimist/storage/s3_documentation/backupwithrestic/#restic-mit-dem-rclone-proxy-verwenden"
  },"275": {
    "doc": "Sichere Datensicherung mit Restic und Rclone",
    "title": "Zusammenfassung",
    "content": "Auf dieser Art und Weise, . | der rclone-Proxy auf dem rclonerv läuft nicht einmal als Dienst. Es wird nur bei Bedarf für die Dauer der Restic-Operation gestartet. Die Kommunikation erfolgt über HTTP2 über stdin/stdout in einem verschlüsselten SSH-Tunnel. | Da rclone mit --append-only läuft, ist es nicht möglich, Snapshots im S3-Bucket zu löschen (oder zu überschreiben). | Alle Daten (außer Zugangsdaten) werden lokal verschlüsselt/entschlüsselt und dann über rclonesrv an/von S3 gesendet/empfangen. | Alle Zugangsdaten werden nur auf rclonesrv gespeichert, um mit S3 zu kommunizieren. | . Da der Befehl in der SSH-Konfiguration für den SSH-Schlüssel des Benutzers fest eingetragen ist, gibt es keine Möglichkeit, den Schlüssel zu verwenden, um Zugriff auf den rclone-Proxy zu erhalten. ",
    "url": "/optimist/storage/s3_documentation/backupwithrestic/#zusammenfassung",
    
    "relUrl": "/optimist/storage/s3_documentation/backupwithrestic/#zusammenfassung"
  },"276": {
    "doc": "Sichere Datensicherung mit Restic und Rclone",
    "title": "Noch ein paar Gedanken",
    "content": "Die Vorteile der Lösung sind wahrscheinlich schon klar. Im Weiteren, . | die Verwaltung von Snapshots (sowohl manuell als auch mit einer Aufbewahrungsrichtlinie) ist nur noch auf dem rclone-Proxy möglich. | Eine einzelne rclone-Proxy-VM (oder sogar ein Docker-Container auf einer isolierten VM) kann mehrere Backup-Clients bedienen. | Es ist sehr empfohlen, für jeden Server, der Daten sichert, einen eigenen Schlüssel zu verwenden. | Wenn Sie mehr als ein Repository aus einem Node verwenden möchten, benötigen Sie dafür neue SSH-Schlüssel. Sie können dann mit -i ~/.ssh/id_ed25519_another_repo in den rclone.program-Argumenten genau wie bei SSH angeben, welcher Schlüssel verwendet werden soll. | . ",
    "url": "/optimist/storage/s3_documentation/backupwithrestic/#noch-ein-paar-gedanken",
    
    "relUrl": "/optimist/storage/s3_documentation/backupwithrestic/#noch-ein-paar-gedanken"
  },"277": {
    "doc": "Sichere Datensicherung mit Restic und Rclone",
    "title": "Sichere Datensicherung mit Restic und Rclone",
    "content": "Restic ist eine sehr einfache und leistungsstarke file-basierte Backup-Lösung, die schnell an Popularität gewinnt. Es kann in Kombination mit S3 verwendet werden, was es zu einem großartigen Werkzeug für Optimist macht. ",
    "url": "/optimist/storage/s3_documentation/backupwithrestic/",
    
    "relUrl": "/optimist/storage/s3_documentation/backupwithrestic/"
  },"278": {
    "doc": "Localstorage",
    "title": "Compute Localstorage für Ihre Instanzen",
    "content": " ",
    "url": "/optimist/storage/localstorage/#compute-localstorage-f%C3%BCr-ihre-instanzen",
    
    "relUrl": "/optimist/storage/localstorage/#compute-localstorage-für-ihre-instanzen"
  },"279": {
    "doc": "Localstorage",
    "title": "Was genau ist Compute Localstorage?",
    "content": "Beim Localstorage befindet sich der Storage Ihrer Instanzen direkt auf dem Hypervisor (Server). Die Localstorage Funktion ist über unsere l1 Flavors verfügbar und für Anwendungen vorgesehen, die eine geringe Latenz erfordern. ",
    "url": "/optimist/storage/localstorage/#was-genau-ist-compute-localstorage",
    
    "relUrl": "/optimist/storage/localstorage/#was-genau-ist-compute-localstorage"
  },"280": {
    "doc": "Localstorage",
    "title": "Datensicherheit und Verfügbarkeit",
    "content": "Da Ihre Daten direkt durch Ihre Instanz auf dem Storage des lokalen Hypervisors gebunden sind, empfiehlt es sich, diese Daten mithilfe eines HA Konzepts über die gegebenen Availability Zonen zu verteilen. Das Storage Backend der Localstorage Instanzen ist gegen einen Ausfall einzelner Speichermedien des Arrays geschützt, die dadurch hergestellte Redundanz besteht jedoch gegenüber der Ceph basierten Instanzen nur innerhalb des Hypervisor Nodes welcher die Instanz bereitstellt. Beim Ersetzen von Einzelkomponenten auf Grund eines Hardwaredefekts, kann es bis zur Wiederherstellung kurzfristig zu einer eingeschränkten Verfügbarkeit und Performance kommen. Die Hypervisor unterliegen einem definierten Patch Zyklus, bei dem die Hypervisoren nacheinander gebootet werden müssen. Durch den Localstorage der Instanzen können die Wartungsarbeiten nicht wie bei den auf Ceph Storage basierten Flavors unterbrechungsfrei durchgeführt werden. Aus diesem Grund existiert für l1 Flavors ein regelmäßiges Wartungsfenster. Dabei wird innerhalb einer Availability Zone und innerhalb des festgelegten Wartungsfensters ein Server nach dem anderen aktualisiert und rebootet. Innerhalb des Wartungsfensters werden laufende Instanzen von unserem System heruntergefahren und nach 10 Minuten gestoppt. ",
    "url": "/optimist/storage/localstorage/#datensicherheit-und-verf%C3%BCgbarkeit",
    
    "relUrl": "/optimist/storage/localstorage/#datensicherheit-und-verfügbarkeit"
  },"281": {
    "doc": "Localstorage",
    "title": "Standard Wartungsfenster",
    "content": "| Intervall | Tag | Zeit (in UTC) | . | wöchentlich | Mittwoch | 9:00 Uhr - 16:00 Uhr | . ",
    "url": "/optimist/storage/localstorage/#standard-wartungsfenster",
    
    "relUrl": "/optimist/storage/localstorage/#standard-wartungsfenster"
  },"282": {
    "doc": "Localstorage",
    "title": "Openstack Features",
    "content": "OpenStack bietet Ihnen viele Funktionen für Ihre Instanzen, wie z.B. resize, shelving oder snapshot. Wenn Sie für Ihre Instanzen l1 Flavors verwenden möchten, beachten Sie bitte folgendes: . Resize: Die Option Resize wird Ihnen angezeigt, aber technisch ist es nicht möglich, eine auf einem l1 Flavor basierende Instanz zu resizen. Sie können das umgehen, in dem Sie einen Cluster (Applikationsbezogen) mit l1 Flavors aufsetzen, größere l1 Flavors parallel starten und Ihre Daten von den alten l1 Flavors auf die neuen l1 Flavors rollen. Dies gilt auch bei einem wechsel von einem andren Flavor zu l1 Flavors. Shelving/Snapshotting: Beide Features sind möglich, aber aufgrund der Disk Size innerhalb der l1 Flavors raten wir wegen der langen Uploadzeiten davon ab. Hier empfiehlt es sich, die für die Applikation vorgesehene externe Backup-Lösung zu nutzen. ",
    "url": "/optimist/storage/localstorage/#openstack-features",
    
    "relUrl": "/optimist/storage/localstorage/#openstack-features"
  },"283": {
    "doc": "Localstorage",
    "title": "Localstorage",
    "content": " ",
    "url": "/optimist/storage/localstorage/",
    
    "relUrl": "/optimist/storage/localstorage/"
  },"284": {
    "doc": "FAQ",
    "title": "FAQ",
    "content": " ",
    "url": "/optimist/faq/",
    
    "relUrl": "/optimist/faq/"
  },"285": {
    "doc": "FAQ",
    "title": "Der Befehl openstack --help zeigt bei vielen Punkten “Could not load EntryPoint.parse” an.",
    "content": "In diesem Fall ist eine der mit dem OpenStack Client installierten Komponenten nicht aktuell. Um zu sehen, welche der Komponenten aktualisiert werden muss, rufen Sie den folgenden Befehl auf: . openstack --debug --help . Hier wird nun vor jedem Punkt angezeigt, was genau der Fehler ist und man kann einfach die jeweilige Komponente mit dem folgenden Befehl aktualisieren (&lt;PROJECT&gt; muss durch die richtige Komponente ersetzt werden): . pip install python-&lt;PROJECT&gt;client -U . ",
    "url": "/optimist/faq/#der-befehl-openstack---help-zeigt-bei-vielen-punkten-could-not-load-entrypointparse-an",
    
    "relUrl": "/optimist/faq/#der-befehl-openstack---help-zeigt-bei-vielen-punkten-could-not-load-entrypointparse-an"
  },"286": {
    "doc": "FAQ",
    "title": "Wie kann ich VRRP nutzen?",
    "content": "Um VRRP nutzen zu können, muss dies in einer Security-Group aktiviert und dann den jeweiligen Instanzen zugeordnet werden. Aktuell ist dies nur mit dem OpenStack Client möglich, zum Beispiel: . openstack security group rule create --remote-ip 10.0.0.0/24 --protocol vrrp --ethertype IPv4 --ingress default . ",
    "url": "/optimist/faq/#wie-kann-ich-vrrp-nutzen",
    
    "relUrl": "/optimist/faq/#wie-kann-ich-vrrp-nutzen"
  },"287": {
    "doc": "FAQ",
    "title": "Warum werden mir Floating IPs berechnet, die ich gar nicht benutze?",
    "content": "Der Grund dafür ist mit hoher Wahrscheinlichkeit, dass Floating IPs erstellt wurden, aber nach der Benutzung nicht korrekt gelöscht wurden. Um eine Übersicht über die aktuell verwendeten Floating IPs zu erhalten, können Sie das Horizon Dashboard nutzen. Dort befindet sich der entsprechende Punkt unter Project → Network → Floating-IPs. Alternativ können Sie den Weg über den OpenStack Client nutzen: . $ openstack floating ip list +--------------------------------------+---------------------+------------------+--------------------------------------+--------------------------------------+----------------------------------+ | ID | Floating IP Address | Fixed IP Address | Port | Floating Network | Project | +--------------------------------------+---------------------+------------------+--------------------------------------+--------------------------------------+----------------------------------+ | 84eca713-9ac1-42c3-baf6-860ba920a23c | 185.116.245.222 | 192.0.2.7 | a3097883-21cc-49fa-a060-bccc1678ece7 | 54258498-a513-47da-9369-1a644e4be692 | b15cde70d85749689e6568f973bb002 | +--------------------------------------+---------------------+------------------+--------------------------------------+--------------------------------------+----------------------------------+ . ",
    "url": "/optimist/faq/#warum-werden-mir-floating-ips-berechnet-die-ich-gar-nicht-benutze",
    
    "relUrl": "/optimist/faq/#warum-werden-mir-floating-ips-berechnet-die-ich-gar-nicht-benutze"
  },"288": {
    "doc": "FAQ",
    "title": "Wie kann ich den Flavor einer Instanz ändern (instance resize)?",
    "content": "Resizing über die Command Line . Geben Sie den Namen oder die UUID des Servers an, dessen Größe Sie ändern möchten, und ändern Sie die Größe mit dem Befehl openstack server resize. Geben Sie das gewünschte neue Flavor und dann den Instanznamen oder die UUID an: . openstack server resize --flavor FLAVOR SERVER . Die Größenänderung kann einige Zeit in Anspruch nehmen. Während dieser Zeit wird der Instanzstatus als RESIZE angezeigt. Wenn die Resizing abgeschlossen ist, wird der Instanzstatus VERIFY_RESIZE angezeigt. Sie können nun die Größenänderung bestätigen, um den Status auf ACTIVE zu ändern: . openstack server resize --confirm SERVER . Warnung . Die Größenänderung wird nach einer Stunde automatisch bestätigt, falls sie vorher nicht manuell bestätigt oder rückgängig gemacht wurde. Resizing über das Optimist-Dashboard . Gehen Sie zu Optimist Dashboard → Instances und navigieren Sie zu der Instanz, deren Größe geändert werden soll. Wählen Sie anschließend Actions → Resize Flavor. Der aktuelle Flavor wird angezeigt. Verwenden Sie die Dropdown-Liste “Select a new flavor”, wählen Sie den neuen Flavor aus und bestätigen Sie mit “Resize”. Warnung . Dies gilt nicht für l1 (localstorage) Flavors. Weitere Informationen finden Sie unter Storage → Localstorage. ",
    "url": "/optimist/faq/#wie-kann-ich-den-flavor-einer-instanz-%C3%A4ndern-instance-resize",
    
    "relUrl": "/optimist/faq/#wie-kann-ich-den-flavor-einer-instanz-ändern-instance-resize"
  },"289": {
    "doc": "FAQ",
    "title": "Warum ist das Logfile der Compute Instanz im Optimist Dashboard leer?",
    "content": "Bedingt durch Wartungsarbeiten oder einem Umverteilen der Last im OpenStack wurde die Instanz verschoben. In diesem Fall wird das Logfile neu angelegt und neue Meldungen werden wie gewohnt protokolliert. ",
    "url": "/optimist/faq/#warum-ist-das-logfile-der-compute-instanz-im-optimist-dashboard-leer",
    
    "relUrl": "/optimist/faq/#warum-ist-das-logfile-der-compute-instanz-im-optimist-dashboard-leer"
  },"290": {
    "doc": "FAQ",
    "title": "Warum erhalte ich den Fehler “Conflict (HTTP 409)” beim Erstellen eines Swift Containers?",
    "content": "Swift verwendet einzigartige Namen über die gesamte OpenStack Umgebung hinweg. Die Fehlermeldung besagt, dass der gewählte Name bereits in Verwendung ist. ",
    "url": "/optimist/faq/#warum-erhalte-ich-den-fehler-conflict-http-409-beim-erstellen-eines-swift-containers",
    
    "relUrl": "/optimist/faq/#warum-erhalte-ich-den-fehler-conflict-http-409-beim-erstellen-eines-swift-containers"
  },"291": {
    "doc": "FAQ",
    "title": "Anbringen von Cinder-Volumes an Instanzen per UUID",
    "content": "Wenn Sie mehrere Cinder-Volumes an eine Instanz anhängen, werden die Mount-Punkte möglicherweise bei jedem Neustart neu gemischt. Durch das Mounten der Volumes mittels der UUID wird sichergestellt, dass die richtigen Volumes wieder an die richtigen Mount-Punkte angehängt werden, falls für die Instanz ein Aus- und Wiedereinschalten erforderlich ist. Nachdem Sie die UUID des Volumes mit blkid in der Instanz abgerufen haben, ändern Sie den Mountpunkt in /etc/fstab wie folgt, um die UUID zu verwenden. Beispiel: . # /boot war auf /dev/sda2 während der Installation /dev/disk/by-uuid/f6a0d6f3-b66c-bbe3-47ba-d264464cb5a2 /boot ext4 defaults 0 2 . ",
    "url": "/optimist/faq/#anbringen-von-cinder-volumes-an-instanzen-per-uuid",
    
    "relUrl": "/optimist/faq/#anbringen-von-cinder-volumes-an-instanzen-per-uuid"
  },"292": {
    "doc": "FAQ",
    "title": "Ist die Nutzung von Cinder multi-attached Volumes möglich?",
    "content": "Wir unterstützen keine multi-attached Volumes in der Optimist Platform, da für die Nutzung von multi-attached Volumes clusterfähige Dateisysteme erforderlich sind, um den gleichzeitigen Zugriff auf das Dateisystem zu koordinieren. Versuche, multi-attached Volumes ohne clusterfähige Dateisysteme zu verwenden, bergen ein hohes Risiko für Datenkorruption, daher ist diese Funktion auf der Optimist Plattform nicht aktiviert. ",
    "url": "/optimist/faq/#ist-die-nutzung-von-cinder-multi-attached-volumes-m%C3%B6glich",
    
    "relUrl": "/optimist/faq/#ist-die-nutzung-von-cinder-multi-attached-volumes-möglich"
  },"293": {
    "doc": "FAQ",
    "title": "Warum kann ich keinen Snapshot einer laufenden Instance erstellen?",
    "content": "Um konsistente Snapshots zu erstellen, verwendet die Optimist-Plattform das Property os_require_quiesce=yes. Diese Eigenschaft ermöglicht die Nutzung von fsfreeze, um den Zugriff auf laufende Instanzen auszusetzen, um einen konsistenten Snapshot der Instanz zu erstellen. Zur Erstellung von Snapshots von Instanzen stehen in der Optimist Platform die folgenden Optionen zur Verfügung: . Die erste Option besteht darin, einen Snapshot der laufenden Instanz zu erstellen, indem der qemu-guest-agent installiert und ausgeführt wird. Dieser kann wie folgt installiert und ausgeführt werden: . apt install qemu-guest-agent systemctl start qemu-guest-agent systemctl enable qemu-guest-agent . Außerdem empfehlen wir beim Hochladen Ihrer eigenen Images, dass Sie --property hw_qemu_guest_agent=True als Property an Ihren Images hinzufügen. Sobald der qemu-guest-agent läuft, kann der Snapshot erstellt werden. Die zweite Möglichkeit besteht darin, die laufende Instanz zu stoppen, den Snapshot zu erstellen und die Instanz schließlich erneut zu starten. Dies kann über das Horizon Dashboard oder auf der CLI wie folgt erfolgen: . openstack server stop ExampleInstance openstack server image create --name ExampleInstanceSnapshot ExampleInstance openstack server start ExampleInstance . ",
    "url": "/optimist/faq/#warum-kann-ich-keinen-snapshot-einer-laufenden-instance-erstellen",
    
    "relUrl": "/optimist/faq/#warum-kann-ich-keinen-snapshot-einer-laufenden-instance-erstellen"
  },"294": {
    "doc": "Spezifikationen",
    "title": "Spezifikationen",
    "content": " ",
    "url": "/optimist/specs/",
    
    "relUrl": "/optimist/specs/"
  },"295": {
    "doc": "Flavor Spezifikationen",
    "title": "Flavor Spezifikationen",
    "content": "“Flavor” bezeichnet im OpenStack-Kontext ein Hardware-Profil, das eine virtuelle Maschine nutzt bzw. nutzen kann. Im Optimist sind diverse Standard-Hardwareprofile (Flavors) eingerichtet. Diese haben unterschiedliche Limits und Begrenzungen, welche hier für alle verfügbaren Flavors aufgelistet sind. ",
    "url": "/optimist/specs/flavor_specification/",
    
    "relUrl": "/optimist/specs/flavor_specification/"
  },"296": {
    "doc": "Flavor Spezifikationen",
    "title": "Migration zwischen Flavor-Typen",
    "content": "Um die Flavors bestehender Instanzen zu ändern, kann die OpenStack-Option „Resize Instance“ entweder über das Dashboard oder die CLI verwendet werden. Dies führt zu einem Neustart der Instanz, aber der Inhalt der Instanz bleibt erhalten. Bitte beachten Sie, dass ein Wechsel der Flavors von den großen Root-Disk-Typen zu einem Flavor mit einer kleineren Root-Disk nicht möglich ist. Warnung . Dies gilt nicht für l1 (localstorage) Flavors. Weitere Informationen finden Sie unter Storage → Localstorage. ",
    "url": "/optimist/specs/flavor_specification/#migration-zwischen-flavor-typen",
    
    "relUrl": "/optimist/specs/flavor_specification/#migration-zwischen-flavor-typen"
  },"297": {
    "doc": "Flavor Spezifikationen",
    "title": "Deprecated Flavor-Typen",
    "content": "Die folgenden Flavor-Typen gelten derzeit als veraltet und für die nahe Zukunft ist geplant, diese Flavor-Familien zu entfernen. Wir werden regelmäßig überprüfen, ob diese Flavors noch verwendet werden. Falls dies nicht der Fall ist, werden wir sie auf privat setzen, um zu vermeiden, dass neue Instanzen damit erstellt werden: . | m1-Familie (deprecated) | e1-Familie (e = equal) (deprecated) | r1-Familie (r = ram) (deprecated) | Memory-Flavors (deprecated) | Windows-Flavors (deprecated) | . ",
    "url": "/optimist/specs/flavor_specification/#deprecated-flavor-typen",
    
    "relUrl": "/optimist/specs/flavor_specification/#deprecated-flavor-typen"
  },"298": {
    "doc": "Flavor Spezifikationen",
    "title": "Flavor-Typen",
    "content": "Standard-Flavors . | Bezeichnung | Kerne | RAM | Disk | IOPS Limits (read/write) | IO throughput rate (read/write) | Network Bandwidth | . | s1.micro | 1 | 2 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 1 Gbit/s | . | s1.small | 2 | 4 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 2 Gbit/s | . | s1.medium | 4 | 8 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 3 Gbit/s | . | s1.large | 8 | 16 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . | s1.xlarge | 16 | 32 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . | s1.2xlarge | 30 | 64 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . Standard CPU Large Disk Flavors . | Bezeichnung | Kerne | RAM | Disk | IOPS Limits (read/write) | IO throughput rate (read/write) | Network Bandwidth | . | s1.micro.d | 1 | 2 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 1 Gbit/s | . | s1.small.d | 2 | 4 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 2 Gbit/s | . | s1.medium.d | 4 | 8 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 3 Gbit/s | . | s1.large.d | 8 | 16 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 4 Gbit/s | . | s1.xlarge.d | 16 | 32 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 4 Gbit/s | . | s1.2xlarge.d | 30 | 64 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 4 Gbit/s | . Dedicated CPU Flavors . | Bezeichnung | Kerne | RAM | Disk | IOPS Limits (read/write) | IO throughput rate (read/write) | Network Bandwidth | . | d1.micro | 1 | 8 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 1 Gbit/s | . | d1.small | 2 | 16 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 2 Gbit/s | . | d1.medium | 4 | 32 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 3 Gbit/s | . | d1.large | 8 | 64 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . | d1.xlarge | 16 | 128 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . | d1.2xlarge | 30 | 256 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . Dedicated CPU Large Disk Flavors . | Bezeichnung | Kerne | RAM | Disk | IOPS Limits (read/write) | IO throughput rate (read/write) | Network Bandwidth | . | d1.micro.d | 1 | 8 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 1 Gbit/s | . | d1.small.d | 2 | 16 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 2 Gbit/s | . | d1.medium.d | 4 | 32 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 3 Gbit/s | . | d1.large.d | 8 | 64 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 4 Gbit/s | . | d1.xlarge.d | 16 | 128 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 4 Gbit/s | . | d1.2xlarge.d | 30 | 256 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 4 Gbit/s | . Localstorage Flavors . | Bezeichnung | Kerne | RAM | Disk | IOPS Limits (read/write) | IO throughput rate (read/write) | Network Bandwidth | . | l1.micro | 1 | 8 GB | 300 GB | 25000 / 10000 | 125 MB/s / 60 MB/s | 1 Gbit/s | . | l1.small | 2 | 16 GB | 600 GB | 50000 / 25000 | 250 MB/s / 125 MB/s | 2 Gbit/s | . | l1.medium | 4 | 32 GB | 1200 GB | 100000 / 50000 | 500 MB/s / 250 MB/s | 3 Gbit/s | . | l1.large | 8 | 64 GB | 2500 GB | 100000 / 100000 | 1000 MB/s / 500 MB/s | 4 Gbit/s | . | l1.xlarge | 16 | 128 GB | 5000 GB | 100000 / 100000 | 2000 MB/s / 1125 MB/s | 4 Gbit/s | . | l1.2xlarge | 30 | 256 GB | 10000 GB | 100000 / 100000 | 2000 MB/s / 2000 MB/s | 4 Gbit/s | . m1-Familie (deprecated) . | Bezeichnung | Kerne | RAM | Disk | IOPS Limits (read/write) | IO throughput rate (read/write) | Network Bandwidth | . | m1.micro | 1 | 1 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 1 Gbit/s | . | m1.small | 2 | 4 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 2 Gbit/s | . | m1.medium | 4 | 8 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 3 Gbit/s | . | m1.large | 8 | 16 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . | m1.xlarge | 16 | 32 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . | m1.xxlarge | 30 | 64 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . e1-Familie (e = equal) (deprecated) . | Bezeichnung | Kerne | RAM | Disk | IOPS Limits (read/write) | IO throughput rate (read/write) | Network Bandwidth | . | e1.small | 2 | 2 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 1 Gbit/s | . | e1.medium | 4 | 4 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 2 Gbit/s | . | e1.large | 8 | 8 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 3 Gbit/s | . | e1.xlarge | 16 | 16 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . | e1.xxlarge | 30 | 32 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . r1-Familie (r = ram) (deprecated) . | Bezeichnung | Kerne | RAM | Disk | IOPS Limits (read/write) | IO throughput rate (read/write) | Network Bandwidth | . | r1.small | 2 | 6 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 1 Gbit/s | . | r1.medium | 4 | 12 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 2 Gbit/s | . | r1.large | 8 | 32 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 3 Gbit/s | . | r1.xlarge | 16 | 48 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . Memory-flavors (deprecated) . | Bezeichnung | Kerne | RAM | Disk | IOPS Limits (read/write) | IO throughput rate (read/write) | Network Bandwidth | . | mem.micro | 4 | 16 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 1 Gbit/s | . | mem.small | 8 | 32 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 2 Gbit/s | . | mem.medium | 8 | 64 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 3 Gbit/s | . Windows-flavors (deprecated) . | Bezeichnung | Kerne | RAM | Disk | IOPS Limits (read/write) | IO throughput rate (read/write) | Network Bandwidth | . | win.micro | 1 | 2 GB | 80 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 1 Gbit/s | . | win.small | 2 | 8 GB | 80 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 2 Gbit/s | . | win.medium | 4 | 16 GB | 80 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 3 Gbit/s | . | win.large | 8 | 32 GB | 80 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . | win.xlarge | 16 | 64 GB | 80 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . ",
    "url": "/optimist/specs/flavor_specification/#flavor-typen",
    
    "relUrl": "/optimist/specs/flavor_specification/#flavor-typen"
  },"299": {
    "doc": "Default Quotas",
    "title": "OpenStack Default Quotas",
    "content": "Im Optimist haben wir Standardwerte für den OpenStack Compute-Dienst, den OpenStack Block Storage-Dienst und den OpenStack Networking-Dienst definiert. Wir haben auch separate Quotas für den Octavia Loadbalancer-Dienst und die zugehörigen Komponenten. Diese Standardwerte sind unten aufgeführt. ",
    "url": "/optimist/specs/default_quota/#openstack-default-quotas",
    
    "relUrl": "/optimist/specs/default_quota/#openstack-default-quotas"
  },"300": {
    "doc": "Default Quotas",
    "title": "Compute Standardwerte",
    "content": "| Ressource | Wert | . | Cores | 256 | . | Fixed IPs | Unlimited | . | Floating IPs | 15 | . | Injected File Size | 10240 (MB) | . | Injected Files | 100 | . | Instances | 100 | . | Key Pairs | 100 | . | Properties | 128 | . | Ram | 524288 (MB) | . | Server Groups | 10 | . | Server Group Members | 10 | . ",
    "url": "/optimist/specs/default_quota/#compute-standardwerte",
    
    "relUrl": "/optimist/specs/default_quota/#compute-standardwerte"
  },"301": {
    "doc": "Default Quotas",
    "title": "Block Storage Standardwerte",
    "content": "| Ressource | Wert | . | Backups | 100 | . | Backup Gigabytes | 10000 (MB) | . | Gigabytes | 10000 (MB) | . | Per-volume-gigabytes | Unlimited | . | Snapshots | 100 | . | Volumes | 100 | . ",
    "url": "/optimist/specs/default_quota/#block-storage-standardwerte",
    
    "relUrl": "/optimist/specs/default_quota/#block-storage-standardwerte"
  },"302": {
    "doc": "Default Quotas",
    "title": "Network Standardwerte",
    "content": "| Ressource | Wert | . | Floating IPs | 15 | . | Secgroup Rules | 1000 | . | Secgroups | 100 | . | Networks | 100 | . | Subnets | 200 | . | Ports | 500 | . | Routers | 50 | . | RBAC Policies | 100 | . | Subnetpools | Unlimited | . ",
    "url": "/optimist/specs/default_quota/#network-standardwerte",
    
    "relUrl": "/optimist/specs/default_quota/#network-standardwerte"
  },"303": {
    "doc": "Default Quotas",
    "title": "Octavia Loadbalancers",
    "content": "| Ressource | Wert | . | Load Balancers | 100 | . | Listeners | 100 | . | Pools | 100 | . | Health Monitors | 100 | . | Members | 100 | . ",
    "url": "/optimist/specs/default_quota/#octavia-loadbalancers",
    
    "relUrl": "/optimist/specs/default_quota/#octavia-loadbalancers"
  },"304": {
    "doc": "Default Quotas",
    "title": "Default Quotas",
    "content": " ",
    "url": "/optimist/specs/default_quota/",
    
    "relUrl": "/optimist/specs/default_quota/"
  },"305": {
    "doc": "Application Credentials",
    "title": "Einführung",
    "content": "Benutzer können Application Credentials erstellen, damit sich ihre Anwendungen bei der OpenStack-Komponente Keystone authentifizieren können, ohne ihre eigenen Anmeldeinformationen des Benutzers verwenden zu müssen. Mit Application Credentials können sich Anwendungen mit der Application Credential-ID und einer geheimen Zeichenfolge authentifizieren, die nicht das Kennwort des Benutzers ist. Auf diese Weise wird das Passwort des Benutzers nicht in die Konfiguration der Anwendung eingebettet. Benutzer können eine Teilmenge ihrer Rollenzuweisungen für ein Projekt an Application Credentials delegieren und der Anwendung dieselben oder eingeschränkte Berechtigungen innerhalb eines Projekts erteilen. ",
    "url": "/optimist/specs/application_credentials/#einf%C3%BChrung",
    
    "relUrl": "/optimist/specs/application_credentials/#einführung"
  },"306": {
    "doc": "Application Credentials",
    "title": "Anforderungen für Application Credentials",
    "content": "Name / Secrets . Application Credentials für Ihr Projekt können über die Befehlszeile oder über das Dashboard generiert werden. Diese werden dem Projekt zugeordnet, in dem sie erstellt werden. Der einzige erforderliche Parameter zum Erstellen der Anmeldeinformationen ist ein Name, jedoch kann mit dem Parameter —-secret ein bestimmtes Secret festgelegt werden. Ohne Parameter wird stattdessen automatisch ein Secret in der Ausgabe generiert. Es ist in jedem Fall wichtig das Secret zu notieren, da dieses vor dem Speichern gehasht wird und nach dem Festlegen nicht wiederhergestellt werden kann. Wenn das Secret verloren geht, müssen neue Application Credential für die Anwendung erstellt werden. Roles . Wir empfehlen außerdem, die Roles festzulegen, die die Application Credentials der Anwendung im Projekt haben sollen, da standardmäßig ein neu erstellter Satz von Anmeldeinformationen alle verfügbaren Roles erbt. Im Folgenden sind die verfügbaren Roles aufgeführt, die einem Satz von Application Credentials zugewiesen werden können. Wenn Sie diese Roles mithilfe des Parameters --role auf einen Satz von Application Credentials anwenden, beachten Sie bitte, dass bei allen Role-Namen die Groß-/Kleinschreibung beachtet wird: . | Member: Die Rolle “Member” hat nur administrativen Zugriff auf das zugewiesene Projekt. | heat_stack_owner: Als “heat_stack_owner” können Sie vorhandene HEAT-Templates verwenden und ausführen. | load-balancer_member: Als „load-balancer_member“ können Sie die Octavia LoadBalancer-Ressourcen nutzen. | . Expiration . Standardmäßig laufen erstellte Application Credentials nicht ab, jedoch können feste Ablaufdaten/-zeiten für Application Credentials bei der Erstellung festgelegt werden, indem der Parameter --expires im Befehl verwendet wird (zum Beispiel: --expires '2021-07-15T21: 00:00'). ",
    "url": "/optimist/specs/application_credentials/#anforderungen-f%C3%BCr-application-credentials",
    
    "relUrl": "/optimist/specs/application_credentials/#anforderungen-für-application-credentials"
  },"307": {
    "doc": "Application Credentials",
    "title": "Erstellen von Application Credentials über die CLI",
    "content": "Ein Set von Application Credentials kann im gewünschten Projekt über die CLI erstellt werden. Das folgende Beispiel zeigt, wie ein Set von Credentials mit den folgenden Parametern erstellt wird: . | Name: test-credentials | Secret: ZYQZm2k6pk | Roles: Member, heat_stack_owner, load-balancer_member | Expiration Date/Time: 2021-07-12 at 21:00:00 | . Die neuen Zugangsdaten sollten wie folgt aussehen: . $ openstack application credential create test-credentials --secret ZYQZm2k6pk --role Member --role heat_stack_owner --role load-balancer_member --expires '2021-07-15T21:00:00' +--------------+----------------------------------------------+ | Field | Value | +--------------+----------------------------------------------+ | description | None | expires_at | 2021-07-15T21:00:00.000000 | id | 707d14e835124b4f957938bb5a57d1be | name | test-credentials | project_id | c704ac5a32b84b54a0407d28ad448399 | roles | Member heat_stack_owner load-balancer_member | secret | ZYQZm2k6pk | system | None | unrestricted | False | user_id | 1d9f1ecb5de3607e8982695f72036fa5 | +--------------+----------------------------------------------+ . Hinweis: Das Secret (ob vom Benutzer festgelegt oder automatisch generiert) wird nur beim Erstellen der Application Credentials angezeigt. Bitte notieren Sie sich das Secret zu diesem Zeitpunkt. ",
    "url": "/optimist/specs/application_credentials/#erstellen-von-application-credentials-%C3%BCber-die-cli",
    
    "relUrl": "/optimist/specs/application_credentials/#erstellen-von-application-credentials-über-die-cli"
  },"308": {
    "doc": "Application Credentials",
    "title": "Anzeigen von Application Credentials über die CLI",
    "content": "Die Liste der zu einem Projekt gehörenden Application-Credentials kann mit dem folgenden Befehl aufgelistet werden. $ openstack application credential list +----------------------------------+-------------------+----------------------------------+-------------+------------+ | ID | Name | Project ID | Description | Expires At | +----------------------------------+-------------------+----------------------------------+-------------+------------+ | 707d14e835124b4f957938bb5a57d1be | test-credentials | c704ac5a32b84b54a0407d28ad448399 | None | None | +----------------------------------+-------------------+----------------------------------+-------------+------------+ . Einzelne Credentials können mit dem $ openstack application credential show &lt;name&gt; Befehl angezeigt werden. ",
    "url": "/optimist/specs/application_credentials/#anzeigen-von-application-credentials-%C3%BCber-die-cli",
    
    "relUrl": "/optimist/specs/application_credentials/#anzeigen-von-application-credentials-über-die-cli"
  },"309": {
    "doc": "Application Credentials",
    "title": "Löschen von Application Credentials über die CLI",
    "content": "Application Credentials können über die CLI mit dem folgenden Befehl mit dem Namen oder der ID des spezifischen Satzes von Anmeldeinformationen gelöscht werden: . openstack application credential delete test-credentials . ",
    "url": "/optimist/specs/application_credentials/#l%C3%B6schen-von-application-credentials-%C3%BCber-die-cli",
    
    "relUrl": "/optimist/specs/application_credentials/#löschen-von-application-credentials-über-die-cli"
  },"310": {
    "doc": "Application Credentials",
    "title": "Erstellen und Löschen von Application Credentials für Anwendungen über das Optimist Dashboard",
    "content": "Alternativ können Application Credentials auch über das Optimist Dashboard unter Identität &gt; Application Credentials generiert werden: . Hinweis: Hier können mehrere Rollen ausgewählt werden, indem Sie die Umschalttaste gedrückt halten und durch die Optionen navigieren. Nach der Erstellung wird ein Dialogfeld angezeigt, in dem Sie aufgefordert werden, die ID und das Secret zu notieren. Wenn Sie fertig sind, klicken Sie auf “Close”. Die Zugangsdaten hier können jederzeit gelöscht werden, indem Sie den zu löschenden Zugangsdatensatz zu markieren und dann auf “DELETE APPLICATION CREDENTIAL” klicken. ",
    "url": "/optimist/specs/application_credentials/#erstellen-und-l%C3%B6schen-von-application-credentials-f%C3%BCr-anwendungen-%C3%BCber-das-optimist-dashboard",
    
    "relUrl": "/optimist/specs/application_credentials/#erstellen-und-löschen-von-application-credentials-für-anwendungen-über-das-optimist-dashboard"
  },"311": {
    "doc": "Application Credentials",
    "title": "Application Credentials testen",
    "content": "Sobald wir über die CLI oder das Dashboard einen Satz von Application Credentials erstellt haben, können wir sie mit dem folgenden curl-Befehl testen, um zu überprüfen, ob sie funktionieren. Wir müssen unsere &lt;name&gt; und &lt;secret&gt; im curl-Befehl verwenden: . curl -i -H \"Content-Type: application/json\" -d ' { \"auth\": { \"identity\": { \"methods\": [\"application_credential\"], \"application_credential\": { \"id\": “&lt;id&gt;\", \"secret\": “&lt;secret&gt;\"}}}}' https://identity.optimist.gec.io/v3/auth/tokens . Ein erfolgreicher curl-Versuch gibt ein x-subject-token aus, erfolglose Versuche mit falschen Anmeldeinformationen führen zu einem 401-Fehler. ",
    "url": "/optimist/specs/application_credentials/#application-credentials-testen",
    
    "relUrl": "/optimist/specs/application_credentials/#application-credentials-testen"
  },"312": {
    "doc": "Application Credentials",
    "title": "Application Credentials",
    "content": " ",
    "url": "/optimist/specs/application_credentials/",
    
    "relUrl": "/optimist/specs/application_credentials/"
  },"313": {
    "doc": "Volumenspezifikationen",
    "title": "Volumenspezifikationen",
    "content": "In OpenStack sind “Volumes” persistente Speicher, die Sie an Ihre laufenden OpenStack Compute-Instanzen anhängen können. In Optimist haben wir drei Serviceklassen für Volumes eingerichtet. Diese haben unterschiedliche Limits, die unten aufgeführt sind. ",
    "url": "/optimist/specs/volume_specification/",
    
    "relUrl": "/optimist/specs/volume_specification/"
  },"314": {
    "doc": "Volumenspezifikationen",
    "title": "Volume-Typen",
    "content": "Wir haben drei Hauptvolumentypen: . | high-iops | default | low-iops | . ",
    "url": "/optimist/specs/volume_specification/#volume-typen",
    
    "relUrl": "/optimist/specs/volume_specification/#volume-typen"
  },"315": {
    "doc": "Volumenspezifikationen",
    "title": "Volumen-Typen-Liste",
    "content": "Nachfolgend eine Übersicht der drei Volume-Typen: . | Name | Read Bytes Sec | Read IOPS Sec | Write Bytes Sec | Write IOPS Sec | . | high-iops | 524288000 | 10000 | 524288000 | 10000 | . | default | 209715200 | 2500 | 209715200 | 2500 | . | low-iops | 52428800 | 300 | 52428800 | 300 | . ",
    "url": "/optimist/specs/volume_specification/#volumen-typen-liste",
    
    "relUrl": "/optimist/specs/volume_specification/#volumen-typen-liste"
  },"316": {
    "doc": "Volumenspezifikationen",
    "title": "Auswählen eines Volume-Typs",
    "content": "Sie können beim Erstellen eines Volumes mit dem folgenden Befehl einen der drei Volume-Typen auswählen (Wenn nicht anders angegeben, wird immer der Typ „Standard“ verwendet): $ openstack volume create &lt;volume-name&gt; --size 10 --type high-iops . ",
    "url": "/optimist/specs/volume_specification/#ausw%C3%A4hlen-eines-volume-typs",
    
    "relUrl": "/optimist/specs/volume_specification/#auswählen-eines-volume-typs"
  },"317": {
    "doc": "Images",
    "title": "Images",
    "content": "Es gibt 4 Arten von Images in OpenStack: . | Public Images: Diese Images werden von uns gepflegt, sind für alle Benutzer verfügbar, werden regelmäßig aktualisiert und zur Verwendung empfohlen. | Community Images: Ehemals öffentliche Images, die durch neuere Versionen ersetzt wurden. Wir behalten diese Images, bis sie nicht mehr verwendet werden, um Ihre Deployments nicht zu gefährden. | Private Images: Von Ihnen hochgeladene Images, die nur für Ihr Projekt verfügbar sind. | Shared Images: Private Images, die entweder durch Sie oder oder mit Ihnen in mehreren Projekten gemeinsam genutzt werden. | . Nur die ersten beiden Typen werden von uns verwaltet. ",
    "url": "/optimist/specs/images/",
    
    "relUrl": "/optimist/specs/images/"
  },"318": {
    "doc": "Images",
    "title": "Public und community images",
    "content": "Um Ihren Aufwand so gering wie möglich zu halten, stellen wir Ihnen eine Reihe ausgewählter Images zur Verfügung. Aktuell enthält diese Liste: . | Ubuntu 24.04 LTS (Noble Numbat) | Ubuntu 22.04 LTS (Jammy Jellyfish) | Ubuntu 20.04 LTS (Focal Fossa) | Debian 12 (Bookworm) | Debian 11 (Bullseye) | Debian 10 (Buster) | CentOS 8 | CentOS 7 | CoreOS (stable) | Flatcar Linux | Windows Server 2019 (GUI/Core) | . Diese Images werden täglich auf neue Versionen überprüft. Die neueste verfügbare Version ist immer ein “public image” und endet auf Latest. Alle vorherigen Versionen eines Images werden durch unseren Automatismus in “community images” umgewandelt, umbenannt (latest wird durch das Datum des ersten Uploads ersetzt), und bei ausbleibender Verwendung (keinerlei Nutzung) schlussendlich gelöscht. OpenStack und viele Deployment-Tools unterstützen die Verwendung dieser Images entweder über den Namen oder über ihre UUID. Durch die Verwendung eines Namens, z.B. Ubuntu 22.04 Jammy Jellyfish - Latest, Erhalten sie jeweils die aktuellste Version des jeweiligen Images, indem Sie Ihre Instanzen neu bereitstellen oder neu aufbauen, selbst wenn wir das Image zwischendurch ersetzen. Sie können dieses Verhalten vermeiden, indem Sie stattdessen die UUID verwenden. Dies kann für Cluster-Einsätze nützlich sein, bei denen Sie sicherstellen wollen, dass auf allen Instanzen die gleiche Version des Images läuft. ",
    "url": "/optimist/specs/images/#public-und-community-images",
    
    "relUrl": "/optimist/specs/images/#public-und-community-images"
  },"319": {
    "doc": "Images",
    "title": "Linux Images",
    "content": "Alle von uns zur Verfügung gestellten Linux-Images sind unmodifiziert und kommen direkt von ihren offiziellen Maintainern. Wir testen sie während des Upload-Prozesses auf Kompatibilität. ",
    "url": "/optimist/specs/images/#linux-images",
    
    "relUrl": "/optimist/specs/images/#linux-images"
  },"320": {
    "doc": "Images",
    "title": "Windows Images",
    "content": "Was ist drin? . Leider gibt es keine vorgefertigten Images für Windows-Deployments, haben wir eigene gebaut. Unsere Anpassungen sind minimal, gerade genug, um eine einfache Nutzung innerhalb unserer Instanzen zu ermöglichen. Unsere Images basieren auf einer regulären Installation von Windows Server 2019 Standard Edition, Version 1809 (LTSC). Unsere Image Builds enthalten die aktuellsten Treiber für unsere Virtualisierungsinfrastruktur, für die Netzwerkkarte und Festplatten hinzugefügt. Außerdem haben wir die neueste OpenSSH-Version für Windows und die neueste Version der PowerShell installiert. Beide sind für die folgenden Schritte erforderlich und ermöglichen Ihnen die erste Verbindung mit Ihrer Instanz. Des weiteren ist der RDP-Dienst aktiviert, der für eine Remote-Desktop-Verbindung erforderlich ist. Vergessen Sie nicht, die dafür erforderlichen Sicherheitsgruppen hinzuzufügen, und achten Sie darauf, den Zugriff so weit wie möglich einzuschränken. Außerdem haben wir aus Sicherheitsgründen AutoLogon deaktiviert. Unsere Images sind außerdem mit aktivierten Spectre- und Meltdown-Mitigations ausgestattet. Außerdem mussten wir die Nutzung von zufälligen MAC-Adressen deaktivieren, da unsere virtuellen Netzwerke feste MAC-Adressen voraussetzen. Für einen schnellen Start und Ihre Sicherheit stellen wir diese Windows-Images mit den neuesten kumulativen Updates für Windows und das .NET Framework bereit. Nach dem Hochfahren einer Instanz müssen Sie wahrscheinlich nur die Windows-Defender-Definitionen aktualisieren. Schließlich haben wir die verfügbaren DotNetAssemblies optimiert, Firewall-Regeln hinzugefügt, um ICMP-Echoantworten zuzulassen, und cloud-init installiert. Letzteres ist für das Hinzufügen Ihrer ssh-Schlüssel zu den neuen Instanzen verantwortlich. Und wie? . Fast genauso einfach wie mit unseren Linux-Instanzen. Importieren Sie Ihren SSH-Schlüssel in OpenStack (CLI oder Dashboard) und starten Sie Ihre Instanzen. Danach können Sie sich mit folgendem Befehl anmelden: . ssh -i ~/.ssh/id_rsa $instanceIP -l Administrator . Einmal eingeloggt, können Sie nun ein neues Administrator-Passwort vergeben, mit dem Sie sich am Remote-Desktop einloggen können: . net user Administrator $password . Wir raten dringend davon ab, veraltete Verfahren wie z.B. ein admin_pass über die Instanz-Metadaten zu setzen. Hierbei wird nichts verschlüsselt oder anderweitig gesichert, und wird außerdem nicht funktionieren, sollte Ihr Passwort nicht den nötigen Sicherheitsrichtlinien entsprechen. Achtung: Unsere Windows-Images enthalten weder Produkt-Schlüssel, noch Lizenzen. Sie werden Ihre eigenen verwenden müssen. ",
    "url": "/optimist/specs/images/#windows-images",
    
    "relUrl": "/optimist/specs/images/#windows-images"
  },"321": {
    "doc": "Images",
    "title": "Upload von eigenen Images",
    "content": "Sie können jederzeit Ihre eigenen Images hochladen, anstatt die von uns bereit gestellten zu nutzen. Am einfachsten funktioniert das über die OpenStack-CLI. openstack image create \\ --property hw_disk_bus=scsi \\ --property hw_qemu_guest_agent=True \\ --property hw_scsi_model=virtio-scsi \\ --property os_require_quiesce=True \\ --private \\ --disk-format qcow2 \\ --container-format bare \\ --file ~/my-image.qcow2 \\ my-image . Dabei müssen mindestens folgende Parameter spezifiziert werden: . | --disk-format: Das Format Ihres Quell-Images, z.B. qcow2 | --file: Das Quell-Image auf Ihrem System | Name des Abbilds: my-image als Beispiel. | . Um die Erstellung von Snapshots für laufende Instanzen zu ermöglichen ist es notwendig, dass Sie das Property --property hw_qemu_guest_agent=True an den von Ihnen genutzten Images setzen und qemu-guest-agent auf dem System installieren. Weitere Details finden Sie in unseren FAQ. Das gleiche funktioniert auch über das Dashboard. Achten Sie hier darauf, alle der obigen Parameter anzugeben. ",
    "url": "/optimist/specs/images/#upload-von-eigenen-images",
    
    "relUrl": "/optimist/specs/images/#upload-von-eigenen-images"
  },"322": {
    "doc": "Shelving-Instanzen",
    "title": "Shelving-Instanzen",
    "content": " ",
    "url": "/optimist/specs/shelving_instances/",
    
    "relUrl": "/optimist/specs/shelving_instances/"
  },"323": {
    "doc": "Shelving-Instanzen",
    "title": "Einführung",
    "content": "Auf der OpenStack-Plattform haben Sie die Möglichkeit, eine Instanz zurückzustellen. Shelving-Instanzen ermöglichen es Ihnen, eine Instanz zu stoppen, ohne dass sie Ressourcen verbraucht. Eine zurückgestellte Instanz sowie die ihr zugewiesenen Ressourcen (z.B. IP-Adresse, usw.) werden als bootfähige Instanz beibehalten. Diese Funktion kann als Teil eines Lifecycle-Prozesses einer Instanz oder zum Einsparen von Ressourcen verwendet werden. Warnung . Dies gilt nicht für l1 (localstorage) Flavors. Weitere Informationen finden Sie unter Storage → Localstorage. ",
    "url": "/optimist/specs/shelving_instances/#einf%C3%BChrung",
    
    "relUrl": "/optimist/specs/shelving_instances/#einführung"
  },"324": {
    "doc": "Shelving-Instanzen",
    "title": "Shelving einer Instanz",
    "content": "Instanzen auf Openstack können wie folgt abgelegt werden: $ openstack server shelve &lt;server-id&gt; . ",
    "url": "/optimist/specs/shelving_instances/#shelving-einer-instanz",
    
    "relUrl": "/optimist/specs/shelving_instances/#shelving-einer-instanz"
  },"325": {
    "doc": "Shelving-Instanzen",
    "title": "Unshelving einer Instanz",
    "content": "Instanzen können mit dem folgenden Befehl Unshelved werden: $ openstack server unshelve &lt;server-id&gt; . ",
    "url": "/optimist/specs/shelving_instances/#unshelving-einer-instanz",
    
    "relUrl": "/optimist/specs/shelving_instances/#unshelving-einer-instanz"
  },"326": {
    "doc": "Shelving-Instanzen",
    "title": "Eventliste für eine Instanz anzeigen",
    "content": "Sie können den Shelving/Unshelving-Verlauf jedes Servers anzeigen, indem Sie sich die Ereignisliste anzeigen lassen: . $ openstack server event list &lt;server-id&gt; +------------------------------------------+--------------------------------------+--------+----------------------------+ | Request ID | Server ID | Action | Start Time | +------------------------------------------+--------------------------------------+--------+----------------------------+ | req-8d593999-a09b-41a7-8916-1d7c28cd4dc0 | 846112be-d107-4c75-db75-a32eb47a78c5 | shelve | 2022-07-17T15:28:08.000000 | req-076969ee-15a4-470e-8913-051c6f9d4bd3 | 846112be-d107-4c75-db75-a32eb47a78c5 | create | 2022-07-19T16:15:22.000000 | +------------------------------------------+--------------------------------------+--------+----------------------------+ . ",
    "url": "/optimist/specs/shelving_instances/#eventliste-f%C3%BCr-eine-instanz-anzeigen",
    
    "relUrl": "/optimist/specs/shelving_instances/#eventliste-für-eine-instanz-anzeigen"
  },"327": {
    "doc": "Shelving-Instanzen",
    "title": "Warum Shelving verwenden?",
    "content": "Diese Funktion ist nützlich, um Instanzen zu archivieren, die Sie derzeit nicht verwenden, aber nicht löschen möchten. Das Shelving einer Instanz ermöglicht es Ihnen, die Instanzdaten und Ressourcenzuordnungen beizubehalten, gibt aber CPU und Arbeitsspeicher Ressourcen der Instanz frei. Wenn Sie eine Instanz zurückstellen, generiert der Compute-Dienst ein Snapshot-Image, das den Status der Instanz erfasst, und lädt es in die Glance-Library hoch. Wenn die Instanz unshelved wird, wird sie mithilfe des Snapshots neu erstellt. Das Snapshot-Image wird gelöscht, wenn die Instanz unshelved oder gelöscht wird. ",
    "url": "/optimist/specs/shelving_instances/#warum-shelving-verwenden",
    
    "relUrl": "/optimist/specs/shelving_instances/#warum-shelving-verwenden"
  },"328": {
    "doc": "Shelving-Instanzen",
    "title": "Abrechnung von Shelved Instances",
    "content": "Bei einer Shelved Instance wird nur die Root Disk der Instanz weiterhin abgerechnet. CPU- und Arbeitsspeicher- Ressourcen aus dem Flavor der Instanz werden ab dem Zeitpunkt des Shelvings nicht mehr in Rechnung gestellt und nach dem unshelving automatisch wieder berechnet. Shelving hat keine Auswirkungen auf die Auslastung der Quotas im Projekt. Shelved Ressourcen werden nicht in der Quota freigegeben, um jederzeit ausreichend Ressourcen für das unshelving der Instanz im Projekt zu gewährleisten. ",
    "url": "/optimist/specs/shelving_instances/#abrechnung-von-shelved-instances",
    
    "relUrl": "/optimist/specs/shelving_instances/#abrechnung-von-shelved-instances"
  },"329": {
    "doc": "Changelog",
    "title": "Changelog Optimist",
    "content": "All notable changes to the Optimist Platform are documented on this page. ",
    "url": "/optimist/changelog/#changelog-optimist",
    
    "relUrl": "/optimist/changelog/#changelog-optimist"
  },"330": {
    "doc": "Changelog",
    "title": "Upcoming",
    "content": "Upcoming changes to the Optimist platform are listed here . ",
    "url": "/optimist/changelog/#upcoming",
    
    "relUrl": "/optimist/changelog/#upcoming"
  },"331": {
    "doc": "Changelog",
    "title": "Completed",
    "content": "2022-04-28 . | Optimist Horizon Upgrade (Train) | . 2022-04-27 . | Optimist Heat Upgrade (Train) | . 2022-04-21 . | Optimist Neutron Upgrade (Train) | . 2022-04-05 . | Optimist Nova Upgrade (Train) | . 2022-03-01 . | Optimist Cinder Upgrade (Train) | . 2022-02-23 . | Optimist Designate Upgrade (Train) | . 2022-02-22 . | Optimist Glance Upgrade (Train) | . 2022-02-10 . | Neutron LBaaS removed from Optimist | . 2022-01-25 . | Optimist Keystone Upgrade (Train) | . 2021-08-24 . | Optimist Cinder Upgrade (Stein) | . 2021-08-18 . | Optimist Neutron Feature: . We activated the internal DNS feature. This allows customers to assign dns names to neutron ports. Nova will automatically add the instance name as dns name to the neutron port. | . 2021-07-20 . | Optimist Neutron Upgrade (Stein) | . 2021-06-23 . | Optimist Nova Upgrade (Stein) | . 2021-06-02 . | Optimist Glance upgrade (Stein) | . 2021-06-01 . | Optimist Keystone upgrade (Stein) | . ",
    "url": "/optimist/changelog/#completed",
    
    "relUrl": "/optimist/changelog/#completed"
  },"332": {
    "doc": "Changelog",
    "title": "Changelog",
    "content": " ",
    "url": "/optimist/changelog/",
    
    "relUrl": "/optimist/changelog/"
  }
}
